\documentclass[a4paper,12pt,leqno, titlepage]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{a4}
\usepackage{graphicx}
\usepackage{flafter}
\usepackage{bm}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{natbib}
\bibliographystyle{mystyle2}
\newcommand{\LF}{\ensuremath{\lambda(F)}}
\newcommand{\LFC}{\ensuremath{\lambda^2(F)}}
\newcommand{\EX}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\VAR}{\mathbb{V}}
\newcommand{\COV}{\mathbb{COV}}
\newcommand{\MAV}{\mathbb{MAV}}
\newcommand{\MRAV}{\mathbb{MRAV}}
\newcommand{\POP}{\mathcal{P}}
\newcommand{\SAMP}{\mathcal{S}}
\newcommand{\RE}{\mathbb{RE}}
\newcommand{\PLAN}{\Re^2}
\newcommand{\SUR}{\mathbb{S}}
\newcommand{\ING}{\mathbb{I}}
\newcommand{\DEP}{\mathbb{D}}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{3mm}
\setlength{\headsep}{1cm}
\setlength{\topskip}{0cm}
\setlength{\textwidth}{16cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\makeatletter
\def\tagform@#1{\maketag@@@{[\ignorespaces#1\unskip\@@italiccorr]}}
\makeatother




\begin{document}
\doublespacing
\pagestyle{plain}
\pagenumbering{arabic}

\title{Design-based properties of some small-area estimators in forest inventory with two-phase sampling}
\author{Daniel Mandallaz \thanks{Tel. ++41(0)44 6323186 e-mail
    daniel.mandallaz@env.ethz.ch} \\Chair of Land Use Engineering\\ETH Zurich\\CH 8092 Zurich, Switzerland}
\date{}

\maketitle
\newpage
\nolinenumbers
\begin{abstract}
We consider the small-area estimation problem in forest inventories with two-phase sampling schemes. We propose an improvement of the synthetic estimator, when the true mean of the auxiliary variables over the small-area is unknown and must be estimated, likewise for the residual corrected small-area estimator. We derive the asymptotic design-based variances of these new estimators, \textbf{the pseudo-synthetic and pseudo small-area estimators}, by incorporating also the design-based variance of the regression coefficients. We then propose a very simple mathematical device that transforms pseudo small-area estimators into pseudo-synthetic estimators, which is very convenient to derive asymptotic variances. The results are extended to cluster and two-stage sampling at the plot level. To illustrate the theory we consider the case of post-stratification and a case study.
\begin{center}
\textbf{R\'{e}sum\'{e}}
\end{center}
\begin{sloppypar}
Nous consid\`{e}rons le probl\`{e}me de l'estimation pour petits domaines dans le contexte d'inventaires forestiers en deux phases. Nous proposons une am\'{e}lioration simple de l'estimateur synth\'{e}tique quand la moyenne des variables auxiliaires dans le petit domaine doit \^{e}tre estim\'{e}e en premier lieu     , de m\^{e}me pour l'estimateur pour petit domain bas\'{e} sur les r\'{e}sidus. Nous calculons la variance sous le plan de sondage de ces nouveaux estimateurs  en tenant compte de la variance des coefficients de r\'{e}gression.  De plus, nous proposons un artifice math\'{e}matique qui permet de transformer un estimateur pour petit domaine en un estimateur synth\'{e}tique, ce qui simplifie le calcul de la variance asymptotique. L'extension aux sondages par satellites et à deux degr\'{e}s au niveau de la placette est aussi trait\'{e}e. La th\'{e}orie est illustr\'{e}e par la post-stratification et par une \'{e}tude de cas.
\end{sloppypar}
\end{abstract}
\clearpage
\section{Introduction}\label{introduction}
\pagenumbering{arabic} \setcounter{page}{1}
There is an extensive literature on the problem of small area estimation (or small domain estimation in general sampling). In this paper we shall investigate the properties of some estimators in the \textbf{model-assisted framework}, in which prediction models are used to improve the efficiency but are not assumed to be correct as in the \textbf{model-dependent approach}. The validity of the statistical procedures is ensured by the randomization principle: i.e. we are in the \textbf{design-based} inference framework, which has a definite advantage in official statistics. The reader is referred to (\cite{koehl}, section 3.8) for a good review of small-area estimation in forest inventory that presents alternative techniques, in particular Bayesian. Let us now define the sampling scheme.\par
The \textbf{first phase} draws a large sample $s_1$ of $n_1$ points that are independently and uniformly distributed within the forest area $F$. At each point $x\in{s}_1$ auxiliary
information is collected, very often coding information of qualitative nature
(e.g. following the  interpretation of aerial photographs) or quantitative (e.g. timber volume estimates  based on LIDAR measurements). We shall assume that the auxiliary information at point $x$ is described by the column vector $\mathbf{Z}(x)\in{\Re^{p}}$. \par
 The \textbf{second phase} draws a small sample $s_2\subset{s_1}$ of
$n_2$ points from $s_1$ according to \textbf{equal probability
sampling without replacement}. In the forested area $F$ we consider a well-defined population $ \mathcal{P}$ of $N$ trees with response variable
 $Y_i,\;i=1,2 \ldots$, e.g. the timber volume.  \textbf{The objective is to estimate the overall spatial mean}  $\bar{Y}=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i$, where $\lambda(\cdot)$ denotes the surface area (usually in ha) and \textbf{the mean over a small area} $G\subset F$, defined as
 \begin{equation}\label{small1}
 \bar{Y}_G=\frac{1}{\lambda(G)}\sum_{i=1}^N I_G(i)Y_i=:\frac{1}{\lambda(G)}\sum_{i\in{G}}Y_i
 \end{equation}
 where the indicator variable $I_G(i)$ is $1$ if the $i$-th tree lies in $G$, and $0$ otherwise.\\
 For each point $x\in{s_2}$ trees are drawn from the population $\mathcal{P}$ with probabilities $\pi_i$, for instance with concentric circles or angle count techniques. The
set of trees selected at point $x$ is denoted by $s_{2}(x)$. From each of the
selected trees $i\in{s_{2}(x)}$ one determines $Y_i$. The indicator variable $I_i$ is defined as
\begin{equation}\label{1stage}
 I_i(x)=\begin{cases}&1 \text{ if $i\in s_{2}(x)$}\\
                      &0 \text{ if $i\not\in s_{2}(x)$}
         \end{cases}
\end{equation}
At each point $x\in{s_2}$ the
terrestrial inventory provides the \textbf{local density} $Y(x)$
\begin{equation}\label{truelocaldensity}
 Y(x) =\frac{1}{\lambda(F)}\sum_{i=1}^N \frac{I_i(x)Y_i}{\pi_i}=\frac{1}{\lambda(F)}\sum_{i\in{s}_2(x)} \frac{Y_i}{\pi_i}
 \end{equation}
 The term $\frac{1}{\lambda(F)\pi_i}$ is the tree extrapolation factor $f_i$ with dimension $ha^{-1}$. One must include possible boundary adjustments, $\lambda(F)\pi_i=\lambda(F \cap K_i)$, where $K_i$ is the inclusion circle of the $i$-th tree. In the infinite population or Monte Carlo approach one samples the function $Y(x)$ (\cite{mandallaz}) for which the following important relation holds:
 \begin{equation}\label{montecarlo}
 \EX_{x} (Y(x))=\frac{1}{\lambda(F)}\int_{F} Y(x)dx=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i=\bar{Y}
 \end{equation}
 Where $\EX_x$ denotes the expectation with respect to a random point $x$ uniformly distributed in $F$. This establishes the link between the infinite population (continuum) $\{x\in{F} \mid Y(x)\}$ and the finite population of trees $\{i=1,2 \ldots N \mid Y_i\}$.\par
 Usually boundary adjustments are performed only with respect to $F$ and not with respect to the small area $G$. However, we shall assume that we also have
 \begin{equation}\label{small2}
 \bar{Y}_G=\frac{1}{\lambda(G)}\int_{G} Y(x)dx
 \end{equation}
 The afore mentioned randomization principle assume that we have uniformly independently distributed points or clusters in the forested area $F$, whereas in practice systematic grids are used. There is reasonable theoretical and empirical evidence that treating systematic grids as simple random samples is acceptable for point estimation and also for variance estimation (which will be in most instances slightly overestimated) for extensive forest inventories. From a mathematical point of view the only correct, and also most efficient, procedure, is the geostatistical Kriging technique (see \cite{mandallaz}, chapter 7 for a brief introduction and further references), which, however, is difficult to use and not uncontroversial in some aspects (e.g. choice of spatial correlation models and stationarity assumptions).
 \section{The model}
 We consider the linear model (the upper script on vector or matrices denotes thereafter the transposition operator)
 \begin{equation}\label{linearmodel1}
 Y(x)=\pmb{Z}^t(x)\pmb{\beta}+ R(x)
 \end{equation}
 In the \textbf{model-dependent approach} the point $x$ is fixed and $R(x)$ is a random variable with zero mean and a given covariance structure. In the \textbf{design-based approach}
  $Y(x), \pmb{Z}(x), R(x)$ are random variables  because $x$ is random. The true regression coefficient $\pmb{\beta}$ is by definition the least squares estimate minimizing
 $$\int_F R^2(x)dx=\int_F(Y(x)-\pmb{Z}^t(x)\pmb{\beta})^2 dx$$
 It satisfies the normal equation
 \begin{equation}\label{normaleq1}
 \Big(\int_F\pmb{Z}(x)\pmb{Z}^t(x)dx\Big)\pmb{\beta}=\int_F Y(x)\pmb{Z}(x)dx
 \end{equation}
 and the orthogonality relationship
 \begin{equation}\label{linearmodel2}
 \int_F R(x)\pmb{Z}(x)dx=\pmb{0}
 \end{equation}
  We shall assume that $\pmb{Z}(x)$ contains the intercept term $1$, or, more generally, that the intercept can be expressed as a linear combination of the component of $\pmb{Z}(x)$, which then insures that the mean residual is zero, i.e.
 $$\int_F R(x)dx=0$$
   The important case of stratification amounts to taking $\pmb{Z}^t(x)=(I_{F_1}(x),I_{F_2}(x),\ldots I_{F_L}(x))$, where $F=\cup_{k=1}^L F_k$ and $I_{F_k}(x)$ is the zero-one indicator variable of the $k$-th stratum $F_k$.\\
    We emphasize the fact that in the design-based model-assisted approach the model [\ref{linearmodel1}] is not viewed as an adequate description of the complex stochastic process generating the $Y(x)$, but, more pragmatically, simply as a tool to reduce the variance of estimators of $\bar{Y}, \bar{Y}_G$. Of course, ideally, the model should capture qualitatively the main features of the underlying natural phenomenon. \\To simplify the notation let us
 set $\pmb{A}=\EX_x\pmb{Z}(x)\pmb{Z}^t(x)$, $\pmb{U}(x)=Y(x)\pmb{Z}(x)$.
 The normal equation then reads
 $$\pmb{A}\pmb{\beta}=\EX_x\pmb{U}(x):=\pmb{U}$$
 Of course, only a sample-based normal equation is available, i.e.
 $$\pmb{A}_{s_2}\hat{\pmb{\beta}}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{U}(x)=\pmb{U}_{s_2}$$
 where we have set
 $$\pmb{A}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x)$$ and
 $$\pmb{U}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)$$
 The theoretical and empirical regression vector parameters are
 \begin{eqnarray}\label{linearmodel3}
 \pmb{\beta}&=&\pmb{A}^{-1}\pmb{U} \nonumber \\
 \hat{\pmb{\beta}}_{s_2}&=&\pmb{A}_{s_2}^{-1}\pmb{U}_{s_2}
 \end{eqnarray}
$\hat{\pmb{\beta}}_{s_2}$ is asymptotically design-unbiased for
$\pmb{\beta}$.
 To calculate the design-based variance-covariance matrix of the regression coefficients we need
 $$\EX(\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})(\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})^t$$
 we shall use the Taylor linearization technique. Let us consider the function
 $f(\cdot,\cdot)$ of an arbitrary $(p,p)$ matrix $\pmb{A}$ and an arbitrary $(p,1)$
vector $\pmb{U}$ defined by
 $f(\pmb{A},\pmb{U})=\pmb{A}^{-1}\pmb{U}$. We can write
 $$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}=f(\pmb{A}_{s_2},\pmb{U}_{s_2})-f(\pmb{A},\pmb{U})$$
 which can be viewed as the differential of the function $f()$ at
 the point $\pmb{P}_0=(\pmb{A},\pmb{U})$, which is the expected value of
 the random point $\pmb{P}_{s_2}=(\pmb{A}_{s_2},\pmb{U}_{s_2})$. The distances between the fixed and the random point are of the order
 $n_2^{-\frac{1}{2}}$ in design-probability (by the law of large numbers for $\pmb{U}_{s_2}$ and $\pmb{A}_{s_2}$ and the continuity of the inverse operation). The differential of $f(\cdot,\cdot)$ at $\pmb{P}_0$ is, by the derivation
 rule for product
 $$df=d(\pmb{A}^{-1})\pmb{U}+\pmb{A}^{-1}d\pmb{U}$$
 Differentiating the identity $\pmb{A}^{-1}\pmb{A}=\pmb{I}$ one gets
 $$d(\pmb{A}^{-1})=-\pmb{A}^{-1}(d\pmb{A})\pmb{A}^{-1}$$
 and the following first-order Taylor expansion:
 $$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}\approx
 -\pmb{A}^{-1}(\pmb{A}_{s_2}-\pmb{A})\pmb{A}^{-1}\pmb{U}+\pmb{A}^{-1}(\pmb{U}_{s_2}-\pmb{U})$$
 Expanding this expression and substituting
 $\pmb{A}^{-1}\pmb{U}=\pmb{\beta}$ we obtain the Taylor
 linearization \index{Linearization!Taylor}
 $$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}\approx \pmb{A}^{-1}\Big(-\pmb{A}_{s_2}\pmb{\beta}+\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{U}(x)\Big)$$
 which is, by definition, equal to
 $$\pmb{A}^{-1}\Big(-\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}(x)^t\pmb{\beta}+\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)\Big)$$
 and consequently also to
 $$\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\big(Y(x)-\pmb{Z}(x)^t\pmb{\beta}\big)\pmb{Z}(x)\Big)
 =\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\Big)$$
 Thus, we finally arrive at
 \begin{equation}\label{Taylor}
 \hat{\pmb{\beta}}_{s_2}-\pmb{\beta}\approx \pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\Big)
 \end{equation}
 Using [\ref{linearmodel2}] and the independence of the $R(x)\pmb{Z}(x)$ one obtains the design-based variance-covariance matrix of
$\hat{\pmb{\beta}}_{s_2}$
\begin{equation}\label{varmatrix}
\pmb{\Sigma}_{\hat{\pmb{\beta}}_{s_2}}\approx
\pmb{A}^{-1}\Big(\frac{1}{n_2}\EX_xR^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}^{-1}
\end{equation}
which can be estimated by replacing the theoretical residual $R(x)$
with their empirical counterparts
$\hat{R}(x)=Y(x)-\hat{Y}(x)$, with $\hat{Y}(x)=\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}$, and $\pmb{A}$
with $\pmb{A}_{s_2}$. We then get the \textbf{estimated design-based
variance-covariance matrix} as
\begin{equation}\label{estvarmatrix}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}:=\pmb{A}_{s_2}^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}_{s_2}^{-1}
\end{equation}
Interestingly this is precisely the \textbf{robust estimate of the model-dependent covariance matrix} given in \cite{gregoire2} (see also \cite{mandallaz} p. 107). Setting
$\hat{\sigma}^2=\frac{\sum_{x\in{s_2}}\hat{R}^2(x)}{n_2}$ we get $\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\approx \frac{\hat{\sigma^2}}{n_2}\pmb{A}_{s_2}^{-1}$ whereas the model-dependent ordinary least squares theory gives the unbiased estimate of the covariance matrix as $(\frac{n_2}{n_2-p}\hat{\sigma}^2)\frac{1}{n_2}\pmb{A}_{s_2}^{-1}$.\\
The empirical residuals satisfy the sample orthogonality relation
\begin{equation}\label{sampleortho}
\frac{1}{n_2}\sum_{x\in{s_2}}\hat{R}(x)\pmb{Z}(x)=\pmb{0}
\end{equation}
Theoretically one may use the exact matrix $\pmb{A}$ if it is available or its estimate $\pmb{A}_{s_1}=\frac{1}{n_1}\sum_{x\in{s_1}}\pmb{Z}(x)\pmb{Z}^t(x)$ based on the large sample. However, the resulting point estimates are not always intuitively convincing and not optimal in the model-dependent framework. Beside, they are not available from the usual statistical software packages. For these reasons we shall only work with $\pmb{A}_{s_2}$.
\section{The estimators}
\subsection{External models}
If the prediction model is \textbf{external}, i.e. not fitted with the inventory data at hand, the regression estimate is defined as
\begin{equation}\label{external1}
\hat{Y}_{reg}=\frac{1}{n_1}\sum_{x\in{s_1}}\hat{Y}_0(x)+ \frac{1}{n_2}\sum_{x\in{s_2}}R_0(x)
\end{equation}
with the predictions $\hat{Y}_0(x)=\pmb{Z}^t(x)\pmb{\beta_0}$ and the residuals $R_0(x)=Y(x)-\hat{Y}_0(x)$, where $\pmb{\beta_0}$ is the given external regression coefficient, ideally obtained from another similar inventory. Note that in this case the mean residual will not necessarily be zero.
To calculate the variance one uses the decomposition
\begin{equation}\label{external2}
\VAR_{1,2}(\hat{Y}_{reg})=\VAR_1\EX_{2\mid 1}(\hat{Y}_{reg})+\EX_1\VAR_{2 \mid 1}(\hat{Y}_{reg})
\end{equation} to obtain
\begin{equation}\label{external3}
\VAR(\hat{Y}_{reg})=\frac{1}{n_1}\VAR(Y(x))+(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR(R_0(x))
 \end{equation}
 which can be unbiasedly estimated with
\begin{equation}\label{varexternalsimple}
\hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s_2}}(Y(x)-\bar{Y}_2)^2+
(1-\frac{n_2}{n_1})\frac{1}{n_2}\frac{1}{n_2-1}\sum_{x\in{s_2}}(R_0(x)-\bar{R}_{0,2})^2
\end{equation}
where $\bar{Y}_2=\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)$ and $\bar{R}_{0,2}=\frac{1}{n_2}\sum_{x\in{s_2}}R_0(x)$.\\
The estimation for any small area $G\subset F$ is straightforward, indeed one simply restricts the samples of $n_1$ and $n_2$ points in $F$ to the $n_{1,G}$ and $n_{2,G}$ points in $G$ and apply the above formulae to obtain an unbiased estimate of the conditional variance (i.e. given $n_{1,G}$ and $n_{2,G}$, which are realizations random variables because in our set-up only $n_1$ and $n_2$ are fixed).
\subsection{Internal models}
In most applications the model has to be fitted with the data provided by the current inventory. In this case, the model is said to be \textbf{internal}. In very large samples one can treat an internal model as external and apply again the formulae given above, which obviously neglects the error in the regression coefficients. This is essentially the framework presented in (\cite{mandallaz}, chapter 5 and section 6.3). We shall show in the present paper how one can take the design-based variance of the regression coefficients into account, albeit still in large samples, and incorporate the mean residual directly in the model.\\
 The model-dependent estimator for the small area $G$ is called the
\textbf{synthetic estimator} and is given by
\begin{eqnarray}\label{synthetic1}
\hat{Y}_{G,synth}&=&\frac{1}{\lambda(G)}\int_G\hat{Y}_{s_2}(x)dx \\ \nonumber
&=& \frac{1}{\lambda(G)}\int_G \pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}dx=\bar{\pmb{Z}}_G^t\hat{\pmb{\beta}}_{s_2}
\end{eqnarray}
where $\bar{\pmb{Z}}_G=\frac{1}{\lambda(G)}\int_G\pmb{Z}(x)dx$ is the true mean of the auxiliary vector over the small area $G$, which is available only if the first phase is exhaustive. $\hat{Y}_{G,synt}$ is unbiased under the model, but not optimal as it does not the take the model-dependent spatial correlation of the $Y(x)$ into account. Let us emphasize the fact that the model, i.e. $\hat{\pmb{\beta}}_{s_2}$, is fitted with the full data set and not only with $\{Y(x), \pmb{Z}(x)\mid x\in G\}$. \\
\textbf{In this paper we shall investigate the properties of $\hat{Y}_{G,synt}$ in the design-based inference framework}. \\
First, let us note that $\hat{Y}_{G,synt}$ is a design-based consistent sample copy of
 $$\frac{1}{\lambda(G)}\int_G \hat{Y}(x)dx=\frac{1}{\lambda(G)}\int_G(Y(x)-R(x))dx=\bar{Y}_G-\frac{1}{\lambda(G)}\int_G R(x)dx$$
 Consequently, the synthetic estimator $\hat{Y}_{G,synt}$ has a design-based asymptotic bias equal to $-\frac{1}{\lambda(G)}\int_GR(x)$, which is not zero unless $G=F$ (we have zero mean residual over the entire domain, see [\ref{linearmodel2}]) or, which is unlikely, zero mean residual over the small area of interest. Using [\ref{synthetic1}] and [\ref{estvarmatrix}] \textbf{the estimated design-based variance of the synthetic estimator} is
\begin{equation}\label{synthetic2}
\hat{\VAR}(\hat{Y}_{G,synth})=\bar{\pmb{Z}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G
\end{equation}
We define the g-weights as
\begin{equation}\label{gweights1}
g_G(x)=\bar{\pmb{Z}}_G^t\pmb{A}^{-1}_{s_2}\pmb{Z}(x)
\end{equation}
It is easily checked that one can rewrite the point estimate and its estimated variance as
\begin{eqnarray}\label{gweights2}
\hat{Y}_{G,synth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}g_G(x)Y(x) \nonumber\\
\hat{\VAR}(\hat{Y}_{G,synth})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_G(x)\hat{R}^2(x)
\end{eqnarray}
where the $\hat{R}(x)=Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}$ are the empirical residuals.
In the above the special case $G=F$ is possible. The g-weights enjoy  several attractive statistical properties (see \cite{sarndal} for the aspects in general sampling theory and \cite{mandallaz} for their Monte-Carlo counterparts in forest inventory).\\
To compensate for the bias due to the non vanishing mean residual over $G$ one considers the \textbf{small-area estimator} (\cite{mandallaz} p.120)
\begin{equation}\label{smallareaest}
\hat{Y}_{G,small}=\hat{Y}_{G,synt}+
\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)
\end{equation}
\noindent where $s_{2,G}=s_2\cap G$ and $n_{2,G}=\sum_{x\in{s_2}}I_G(x)$ is the number of
points of $s_2$ falling within $G$. It can be shown (\cite{mandallaz}) that $\hat{Y}_{G,small}$ is asymptotically design-unbiased with estimated design-based variance given by
\begin{equation}\label{smallareadesignvariance1}
\hat{\VAR}(\hat{Y}_{G,small})=
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{equation}
\noindent where
$$\bar{\hat{R}}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)$$
is the estimated mean residual over a small area. The above variance estimate neglects the variance of $\hat{\pmb{\beta}}_{s_2}$ and is therefore valid only if $n_2$ is very large and $n_2>> n_{2,G}$. To have better insight we use the expansion [\ref{Taylor}] to obtain
\begin{equation}\label{smallexp1}
\hat{Y}_{G,small}-\bar{Y}_G=
\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\Big)
+ \frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R(x)-\bar{R}_G
\end{equation}
which leads to the variance
\begin{equation}\label{smallvar1}
\EX(\hat{Y}_{G,small}-\bar{Y}_G)^2=
\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\Big(\frac{1}{n_2}\EX R^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}^{-1}\bar{\pmb{Z}}_G + \VAR \big(\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R(x)\big )+ C
\end{equation}
\noindent where the cross-product term $C$ is given by
$$2\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\EX\big( (\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x))
(\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R(x)-\bar{R}_G)\big)$$
Both terms of the product tend to zero at rate $(n_2^{-\frac{1}{2}})$, which unfortunately is of the same order as the first two terms. However, using the fact that the $R(x)$, $\pmb{Z}(x)$ are independent of $R(y)$, $\pmb{Z}(y)$ for $x\ne y$ we obtain after tedious but simple calculations
$$C=\frac{1}{n_2}\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\big(\EX_{x\in{G}}R^2(x)\pmb{Z}(x)-\bar{R}_G\EX_{x\in{G}}R(x)\pmb{Z}(x)\Big)$$
which we can reasonably assume to be negligible. The above arguments suggest therefore the following estimate of the design-based variance of the small-area estimator with exhaustive first phase
\begin{eqnarray}\label{smallareadesignvariance2}
\hat{\VAR}(\hat{Y}_{G,small})&=&
\bar{\pmb{Z}}^t_G\pmb{A}_{s_2}^{-1}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}} \hat{R}^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}_{s_2}^{-1}\bar{\pmb{Z}}_G \\ \nonumber &+&
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
Comparing with [\ref{varexternalsimple}] (after restricting the samples to $G$) we see that \textbf{treating an internal model as an external model} (i.e. ignoring the variability of $\hat{\pmb{\beta}}_{s_2}$) \textbf{ will underestimate the variance of the small area estimate}. The first term in [\ref{smallareadesignvariance2}] reflects the uncertainty in the regression coefficients.\\
If the first-phase is non-exhaustive, i.e. $n_1 \ne \infty$, then one can replace the true mean $\bar{\pmb{Z}}_G$ by its estimate in the large sample
$$\hat{\bar{\pmb{Z}}}_{1,G}=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\pmb{Z}(x)$$ where $s_{1,G}$ is the set $s_1\cap G$ of the $n_{1,G}=\sum_{x\in{s_1}}I_g(x)$ points of the large sample falling into the small area $G$. This gives the \textbf{pseudo-synthetic estimator}
\begin{equation}\label{pseudosynth1}
\hat{Y}_{G,psynth}=\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}
=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\hat{Y}(x)
\end{equation}
which is clearly asymptotically equivalent to $\hat{Y}_{G,synt}$ as $n_1 \to \infty$ and its design-based expected value tends to  $\bar{\pmb{Z}}^t_G\pmb{\beta}$. To calculate the asymptotic variance we use the decomposition (actually the first order Taylor expansion)
$$\Delta=\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}-\bar{\pmb{Z}}_G^t\pmb{\beta}=
\hat{\bar{\pmb{Z}}}_{1,G}^t(\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})+ (\hat{\bar{\pmb{Z}}}_{1,G}^t-\bar{\pmb{Z}}_G)^t\pmb{\beta}$$
Asymptotically we get
$$\EX \Delta^2=\bar{\pmb{Z}}_G ^t\pmb{\Sigma}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G+
\pmb{\beta}^t\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\pmb{\beta}+
2\EX\Big((\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})^t\hat{\bar{\pmb{Z}}}_{1,G}(\hat{\bar{\pmb{Z}}}_{1,G}
-\bar{\pmb{Z}}_G)^t\pmb{\beta}\Big)$$
\noindent where $\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}$ is the covariance matrix of
$\hat{\bar{\pmb{Z}}}_{1,G}  $. The first two terms are of order $n_2^{-1}$ and $n_1^{-1}$ respectively. For the third term we note that
$\EX (\hat{\bar{\pmb{Z}}}_{1,G}(\hat{\bar{\pmb{Z}}}_{1,G}-\bar{\pmb{Z}}_G)^t)$ is equal to the covariance matrix $\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}$ and therefore of order $n_1^{-1}$ and that
$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}$ is of order $n_2^{-\frac{1}{2}}$. The last term is therefore of smaller order than the first two which leads to the following asymptotic design-based estimate of variance
\begin{equation}\label{estvarpseudosynth1}
\hat{\VAR}(\hat{Y}_{G,psynth}): =
\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_{1,G}
+ \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\hat{\pmb{\beta}}_{s_2}
\end{equation}
The variance-covariance matrix of the auxiliary vector $\hat{\bar{\pmb{Z}}}_G$ is estimated by
\begin{equation}\label{estvarcovaux}
\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}=
\frac{1}{n_{1,G}(n_{1,G}-1)}\sum_{x\in{s_{1,G}}}
(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{1,G})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{1,G})^t
\end{equation}
Usually $\hat{Y}_{G,psynt}$ will have a small variance but at the cost of a potential bias.
We can rewrite [\ref{pseudosynth1}] and [\ref{estvarpseudosynth1}] with the g-weights
\begin{eqnarray}\label{gweightpseudo}
g_{G,1}(x)&=&\hat{\bar{\pmb{Z}}}^t_{1,G}\pmb{A}^{-1}_{s_2}\pmb{Z}(x) \nonumber\\
\hat{Y}_{G,psynth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}g_{G,1}(x)Y(x)
\end{eqnarray}
and after some algebra we get
\begin{eqnarray}\label{gweightpseudo2}
\hat{\VAR}(\hat{Y}_{G,psynth})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{G,1}(x)\hat{R}^2(x) + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\hat{\pmb{\beta}}_{s_2}  \nonumber \\
&=& \frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{G,1}(x)\hat{R}^2(x)+ \frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}(\hat{Y}(x)-\bar{\hat{Y}}_{1,G})^2
\end{eqnarray}
where $\bar{\hat{Y}}_{1,G}=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\hat{Y}(x)$. The second term in the last equation is the variance of the predictions over $G$.\\
The \textbf{pseudo small-area estimator}
\begin{equation}\label{pseudosmall}
\hat{Y}_{G,psmall}=\hat{Y}_{G,psynth}+\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)
\end{equation}
is asymptotically design-unbiased and intuitively its variance can be expected to be well approximated by
\begin{equation}\label{pseudosmallvar}
\hat{\VAR}(\hat{Y}_{G,psmall})=
\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_{1,G}
+ \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\hat{\pmb{\beta}}_{s_2}
+\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{equation}
A tedious formal proof can be given by using [\ref{external2}], [\ref{smallvar1}] and
[\ref{estvarpseudosynth1}].\\
Using the same arguments as in [\ref{gweightpseudo2}] we also have
\begin{eqnarray}\label{pseudosmallvar2}
\hat{\VAR}(\hat{Y}_{G,psmall})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{G,1}(x)\hat{R}^2(x)
+ \frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}(\hat{Y}(x)-\bar{\hat{Y}}_{1,G})^2 \nonumber \\
&+& \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
This should be compared with the external version [\ref{varexternalsimple}]
\begin{eqnarray}\label{varexternalsimpleG}
\hat{\VAR}(\hat{Y}_{G,psmall})&=&\frac{1}{n_{1,G}}\frac{1}{n_{2,G}-1}
\sum_{x\in{s_{2,G}}}(Y(x)-\bar{Y}_{2,G})^2 \nonumber \\
&+& (1-\frac{n_{2,G}}{n_{1,G}})
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
For very large $n_{1,G}$ is is clear that the external version will underestimate the variance
as it neglects the first term in [\ref{pseudosmallvar2}], which albeit is also small for large $n_2$. \\
The special case $G=F$ deserves special attention: because of the zero mean residual we have $\hat{Y}_{F,psmall}=\hat{Y}_{F,psynt}=\hat{Y}_{reg}$ and [\ref{estvarpseudosynth1},\ref{pseudosmallvar2}] lead to the estimated variance
\begin{equation}\label{varyregsynt1}
\hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{F,1}(x)\hat{R}^2(x)
+ \frac{1}{n_1(n_1-1)}
\sum_{x\in{s_1}}(\hat{Y}(x)-\bar{\hat{Y}}_1)^2
\end{equation}
with $\bar{\hat{Y}}_1=\frac{1}{n_1}\sum_{x\in{s_1}}\hat{Y}(x)$. The external version
[\ref{varexternalsimpleG}] gives
\begin{equation}\label{varexternalsimpleG2}
\hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s_2}}(Y(x)-\bar{Y}_2)^2+
(1-\frac{n_{2}}{n_{1}})
\frac{1}{n_{2}}\frac{1}{n_{2}-1}\sum_{x\in{s_{2}}}\hat{R}^2(x)
 \end{equation}
 Writing $Y(x)=\hat{Y}(x)+\hat{R}(x)$ an using [\ref{sampleortho}] we can rewrite [\ref{varexternalsimpleG2}] as
 \begin{equation}\label{varexternalsimpleG3}
 \hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s_2}}(\hat{Y}(x)-\bar{\hat{Y}}_2)^2+
\frac{1}{n_{2}}\frac{1}{n_{2}-1}\sum_{x\in{s_{2}}}\hat{R}^2(x)
\end{equation}
For $G=F$ the $g^2_{F,s_1}(x)$ are asymptotically equal to $1$ (see \cite{mandallaz} p. 113 and the properties of the g-weights discussed below) so that both versions [\ref{varyregsynt1}] and [\ref{varexternalsimpleG3}] are asymptotically equivalent. However, version [\ref{varyregsynt1}] estimates the variance of the predictions in the large sample, which is better, and it rests upon the g-weights for the residual part, which it is known to have better conditional properties (see \cite{mandallaz} p. 84 and section \ref{poststrat} for the important special case of stratification).\\
In the next section we present a simple reformulation of the problem that allows one to transform small-area estimators into synthetic estimators, which offers a great mathematical advantage.
\subsection{Alternative estimators in extended model}
 The main difficulty stems from the fact that
$\int_{G}R(x)dx \ne 0$. If we now extend the auxiliary information vector  $\pmb{Z}(x)$ to $\pmb{\mathcal{Z}}^t(x)=(\pmb{Z}^t(x),I_G(x))\in{\mathcal{R}}^{(p+1)}$, the corresponding model reads
\begin{equation}\label{extendemodel1}
Y(x)=\pmb{\mathcal{Z}}^t(x)\pmb{\theta} +\mathcal{R}(x)
 \end{equation}
 which leads to the normal equation for the extended parameter vector $\pmb{\theta}\in{\mathcal{R}}^{(p+1)}$
$$\Big(\int_F\pmb{\mathcal{Z}}(x)\pmb{\mathcal{Z}}^t(x)dx\Big)\pmb{\theta}=:
\pmb{\mathcal{A}}\pmb{\theta}=\int_{F}Y(x)\pmb{\mathcal{Z}}(x)dx$$ and the orthogonality relationship
$$\int_{F}\mathcal{R}(x)\pmb{\mathcal{Z}}(x)dx=\pmb{0}$$
 Since $I_F(x)\equiv 1$ is the intercept term (or linear combination of the components of $\pmb{Z}(x)$) and $\pmb{\mathcal{Z}}(x)$ contains $I_G(x)$ \textbf{we have the two zero mean residual properties}
$$\int_F \mathcal{R}(x)dx=\int_G \mathcal{R}(x)dx=0$$
Hence, by including the $0,1$ indicator variable of the small area $G$ into the model, we enforce zero mean residual over $F$ and $G$. Note also that $G$ must be a proper subset of $F$, otherwise $\pmb{\mathcal{A}}$ and $\pmb{\mathcal{A}}_{s_2}$ are singular. In practice near-singularity could cause numerical problems, so that the small area $G$ must indeed be small with respect to $F$. Simple calculations yield the following block structure for $\pmb{\mathcal{A}}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{\mathcal{Z}}(x)\pmb{\mathcal{Z}}^t(x)$
\begin{equation}\label{tildeAs2}
 \pmb{\mathcal{A}}_{s_2}=
\left[ \begin {array}{ll}
\pmb{A}_{s_2}& \hat{p}_{2,G}\hat{\bar{\pmb{Z}}}_{2,G} \\
 \hat{p}_{2,G}\hat{\bar{\pmb{Z}}}_{2,G}^t & \hat{p}_{2,G} \\ \end {array} \right]
\end{equation}
where we have set $\hat{p}_{2,G}=\frac{n_{2,G}}{n_2}$, $\hat{\bar{\pmb{Z}}}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\pmb{Z}(x)$. Using formulae for the inversion of partitioned matrices (see e.g. \cite{searle} p. 27 and \cite{tian} for useful generalizations) one obtains
\begin{equation}\label{inversetildeAs2}
 \pmb{\mathcal{A}}^{-1}_{s_2}=
\left[ \begin {array}{ll}
\pmb{A}^{-1}_{s_2}& \pmb{0} \\
 \pmb{0} & \pmb{0} \\ \end {array} \right]
 + \frac{1}{\gamma}\left[ \begin {array}{ll}
\hat{p}_{2,G}^2\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}\hat{\bar{\pmb{Z}}}^t_{2,G}\pmb{A}^{-1}_{s_2}
& - \hat{p}_{2,G}\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}\\
 -\hat{p}_{2,G}\hat{\bar{\pmb{Z}}}^t_{2,G}\pmb{A}^{-1}_{s_2} & 1 \\ \end {array} \right]
\end{equation}
with $\gamma=\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}^t_{2,G}\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}$.\\
We need
$$\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{\mathcal{Z}}(x)=
(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}^t(x),\hat{p}_{2,G}\hat{\bar{Y}}_G)^t
=((\pmb{A}^{-1}_{s_2}\hat{\pmb{\beta}}_{s_2})^t,\hat{p}_{2,G}\hat{\bar{Y}}_G)^t$$
where $\hat{\bar{Y}}_G=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}Y(x)$. This leads after some algebra to the following relationship between the regressions coefficients
\begin{equation}\label{relationship}
 \hat{\pmb{\theta}}_{s_2}=
\left[ \begin {array}{l}
\hat{\pmb{\beta}}_{s_2} \\
 0 \\ \end {array} \right]
 + \frac{1}{\gamma}\left[ \begin {array}{l}
-\hat{p}_{2,G}^2(\hat{\bar{Y}}_G-\hat{\bar{\pmb{Z}}}^t_{2,G}\hat{\pmb{\beta}}_{s_2})\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}
\\
 \hat{p}_{2,G}(\hat{\bar{Y}}_G-\hat{\bar{\pmb{Z}}}^t_{2,G}\hat{\pmb{\beta}}_{s_2})\end {array} \right]
\end{equation}
Note that the term
$$\hat{\bar{Y}}_G-\hat{\bar{\pmb{Z}}}^t_{2,G}\hat{\pmb{\beta}}_{s_2}=\frac{1}{n_{2,G}}
\sum_{x\in{s_{2,G}}}(Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2})$$
is precisely the mean residual over the small area. Hence, the last component of $\hat{\pmb{\theta}}_{s_2}$ is essentially the residual term. We see that the original regression coefficient $\hat{\pmb{\beta}}_{s_2}$ is corrected in the extended model by the residual term and that the impact of this correction tends to zero as the small area gets smaller with respect to $F$, a very intuitive result indeed. One obtains a very similar but not identical result by least squares minimization under the constraint of zero mean residual over the small area (see \cite{searle}, pp 112-113).\\
In perfect analogy with [\ref{estvarmatrix}] the estimated covariance matrix is given by
\begin{equation}\label{estvarmatrixext}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}=\pmb{\mathcal{A}}_{s_2}^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{\mathcal{R}}^2(x)\pmb{\mathcal{Z}}(x)
\pmb{\mathcal{Z}}(x)^t\Big)\pmb{\mathcal{A}}_{s_2}^{-1}
\end{equation}
where we have set $\hat{\mathcal{R}}(x)=Y(x)-\pmb{\mathcal{Z}}^t(x)\hat{\pmb{\theta}}_{s_2}$.
If the first phase is exhaustive we calculate the synthetic estimator in the extended model
\begin{equation}\label{extsynthethic1}
\hat{\tilde{Y}}_{G,synth}=
\frac{1}{\lambda(G)}\int_G\pmb{\mathcal{Z}}^t(x)\hat{\pmb{\theta}}_{s_2}dx
=\bar{\pmb{\mathcal{Z}}}^t_{G}\hat{\pmb{\theta}}_{s_2}
\end{equation}
With $\bar{\pmb{\mathcal{Z}}}^t_{G}=(\bar{\pmb{Z}}^t_{G},1)$ and some algebra one finally obtains
\begin{equation}\label{extsynthethic2}
\hat{\tilde{Y}}_{G,synth}=\bar{\pmb{Z}}_{G}^t\hat{\pmb{\beta}}_{s_2}+
\frac{\alpha}{n_{2,G}}\sum_{x\in{s_{2,G}}}\big(Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}\big)
\end{equation}
where we have set
$$\alpha=\frac{\hat{p}_{2,G}-\hat{p}^2_{2,G}\bar{\pmb{Z}}_{G}^t\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}
{\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}_{2,G}^t\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}$$
Clearly $\hat{\tilde{Y}}_{G,synth}$ and
$\hat{{Y}}_{G,small}$ are asymptotically equivalent because $\alpha$ tends to $1$ in large samples. Note that $\alpha=1$ if the sample is exactly balanced, i.e. if $\hat{\bar{\pmb{Z}}}_{2,G}= \bar{\pmb{Z}}_{G}$.\\
By using [\ref{synthetic2}] and replacing $\pmb{Z}(x)$ with $\pmb{\mathcal{Z}}(x)$ we obtain at once the asymptotic variance
\begin{equation}\label{extsynthetic1var}
\hat{\VAR}(\hat{\tilde{Y}}_{G,synth})=
\bar{\pmb{\mathcal{Z}}}^t_{G}\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}\bar{\pmb{\mathcal{Z}}}_{G}
\end{equation}
One can rewrite $\hat{\tilde{Y}}_{G,synth}$ in terms of g-weights in the extended model as in [\ref{gweights1}] and [\ref{gweights2}] with
$$\tilde{g}_{G}(x)=\bar{\pmb{\mathcal{Z}}}_G^t\pmb{\mathcal{A}}^{-1}_{s_2}\pmb{\mathcal{Z}}(x)$$
 \begin{eqnarray}\label{gweightsynthextended}
 \hat{\tilde{Y}}_{G,synth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G}(x)Y(x) \nonumber \\
 \hat{\VAR}(\hat{\tilde{Y}}_{G,synth})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}\tilde{g}^2_{G}(x)\hat{\mathcal{R}}^2(x)
 \end{eqnarray}
If the first phase is not exhaustive we estimate the true mean of the extended auxiliary variables
\begin{equation}
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}
\pmb{\mathcal{Z}}(x)
\end{equation}
to get the pseudo-synthetic estimate in the extended model
\begin{equation}\label{pseudosyntheticextended1}
\hat{\tilde{Y}}_{G,psynth}=\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^t\hat{\pmb{\theta}}_{s_2}
\end{equation}
As in [\ref{extsynthethic2}] we have
\begin{equation}\label{extsynthethic3}
\hat{\tilde{Y}}_{G,psynth}=\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}+
\frac{\alpha_1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\big(Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}\big)
\end{equation}
where we have set
$\alpha_1=\frac{\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}_{1,G}^t
\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}
{\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}_{2,G}^t
pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}$.\\
By [\ref{estvarpseudosynth1}] we get immediately the following consistent estimate of the design-based variance
\begin{equation}\label{estvarpsynth1extended}
\hat{\VAR}(\hat{\tilde{Y}}_{G,psynth})=
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}
+ \hat{\pmb{\theta}}^t_{s_2}\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}}\hat{\pmb{\theta}}_{s_2}
\end{equation}
The variance-covariance matrix of $\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}$ can be estimated as usual by
\begin{equation}\label{estvarcovaux}
\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}}=
\frac{1}{n_{1,G} (n_{1,G}-1)}\sum_{x\in{s_{1,G}}}
(\pmb{\mathcal{Z}}(x)-\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G})(\pmb{\mathcal{Z}}(x)-
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G})^t
\end{equation}
Again, one can rewrite the above expression with the g-weights
$\tilde{g}_{G,1}(x)=\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^t
\pmb{\mathcal{A}}^{-1}_{s_2}\pmb{\mathcal{Z}}(x)$
namely
\begin{eqnarray}\label{gweightextended}
\hat{\tilde{Y}}_{G,psynth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,1}(x)Y(x)\\
\hat{\VAR}(\hat{\tilde{Y}}_{G,psynth})&=&\frac{1}{n^2_2}
\sum_{x\in{s_2}}\tilde{g}^2_{G,1}(x)\hat{\mathcal{R}}^2(x)+
\hat{\pmb{\theta}}^t_{s_2}\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}}\hat{\pmb{\theta}}_{s_2}
\end{eqnarray}
\textbf{Properties of the g-weights:}\label{gweights}
 \begin{enumerate}
 \item
 The g-weights enjoy the calibration properties
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{G,1}(x)\pmb{Z}(x)=
 \hat{\bar{\pmb{Z}}}_{1,G}$  and  $\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,1}(x)\pmb{\mathcal{Z}}(x)=
 \hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}$. The proof is immediate by transposing the equalities and by the very definition of the g-weights.
 \item
  The fact that one can assume the g-weights depend only on the point $x$ and not on the whole sample $s_2$ when calculating variances is fully justified by the Taylor expansion leading to the robust design-based covariances.
 \item
 By considering formally the trivial constant local density $Y(x)\equiv 1$ and solving the normal equations one sees that
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{G,1}(x)=
  \frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,1}(x)=1$, i.e. the g-weights have means equal to 1.
 \item
  When $G=F$ the estimator $\hat{Y}_{reg}$ is asymptotically equivalent to the sample mean over $F$, i.e. to $\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)$. This must hold for an arbitrary density $Y(x)$ and therefore one gets $\lim_{n_2\to\infty}g_{F,1}(x)=1>0$. This is not true for a proper subset $G\subset F$. In this case, $\hat{\tilde{Y}}_{G,synt}$ is asymptotically equivalent to the sample mean over the small domain, i.e. to $\hat{\bar{Y}}_G$. Hence, $\tilde{g}_{G,1}(x)$ will tend to $0$  for $x\not\in{G}$ (negative values are possible) and to $\frac{n_2}{n_{2,G}} $ for $x\in{G}$.
 \end{enumerate}
In the next section we generalize the previous results to cluster sampling. The main ideas remain the same but the formulae are slightly more cumbersome due to the random cluster size.
\section{Generalization to cluster sampling}
We follow the description of cluster sampling as define in \cite{mandallaz} (especially section 5.5). A cluster is identified by its origin $x$, uniformly distributed in $\tilde{F}\supset F$. The geometry of the cluster is given by $M$ vectors $e_1,\ldots e_M$ defining the random cluster $x_l=x+e_l$. $M(x)=\sum_{l=1}^MI_{F}(x_l)$ is the random number of points of the cluster falling into the forest area $F$. We define the local density at the cluster level by $Y_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)Y(x_l)}{M(x)}$, likewise we set $\pmb{Z}_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)\pmb{Z}(x_l)}{M(x)}$. The set $\tilde{F}$ above can be mathematically defined as the smallest set $\{x\in{\mathcal{R}}^2 \mid M(x) \ne 0 \}$. In the first phase we have $n_1$ clusters identified by $x\in{s_1}$ and in the second phase $n_2$ clusters with $x\in{s_2}$, obtained by simple random sampling from $s_1$.\\
We shall use the model-based approach, in which the regression coefficient $\pmb{\beta}_c$ at the cluster level minimizes
$$\EX_{x\in{\tilde{F}}}M(x)(Y_c(x)-\pmb{\beta}^{t}\pmb{Z}_c(x))^2$$
In the pure design-based approach the weights will be $M^2(x)$ but this leads to non-zero mean residual (thought close zero in practice), and the definitions of the regression estimator and of the normal equation are slightly different (see \cite{mandallaz}, section 5.5 for details). The choice of $M(x)$ rather than $M^2(x)$ as weights is suggested by the model-dependent approach. When $Y_c(x)$ is the mean of the
$M(x)$ observations, its variance can be expected to be inversely
proportional to $M(x)$.  This procedure  leads to the normal equation
$$\Big(\EX_{x\in{\tilde{F}}}M(x)\pmb{Z_c}(x)\pmb{Z_c}(x)^{t}\Big)\pmb{\beta}_c=\EX_{x\in{\tilde{F}}}M(x)Y_c(x)\pmb{Z_c}(x)$$
and to $\EX_{x\in{\tilde{F}}}M(x)R_c(x)=0$. An asymptotically design-unbiased estimate $\hat{\pmb{\beta}}_{c,s_2}$  for $\pmb{\beta}_c$ can be obtained  by taking a sample copy of the above equation, i.e.
 \begin{eqnarray}\label{estcluster1}
 \hat{\pmb{\beta}}_{c,s_2}&=&
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{Z}_c(x)\pmb{Z}_c^t(x)\Big)^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \nonumber\\
 &:=&\pmb{A}^{-1}_{c,s_2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \end{eqnarray}
 The empirical residuals at the cluster level are
 $$\hat{R}_{c}(x)=Y_c(x)-\pmb{Z}^t_c(x)\hat{\pmb{\beta}}_{c,s_2}$$
 which satisfy the orthogonality relation
 $$\sum_{x\in{s_2}}M(x)\hat{R}_c(x)\pmb{Z}_c(x)=0$$
 and in particular the zero mean residual property
$$\frac{\sum_{x\in{s_2}}M(x)\hat{R}_c(x)}{\sum_{x\in{s_2}}M(x)}=0$$
Using mutatis mutandis exactly the same arguments as in simple random sampling we get the asymptotic robust design-based estimated variance-covariance matrix
\begin{equation}\label{estvarcluster1}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,s_2}}=\pmb{A}^{-1}_{c,s_2}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}}M^2(x)\hat{R}^2_c(x)
\pmb{Z}_c(x)\pmb{Z}^t_c(x)\Big)\pmb{A}^{-1}_{c,s_2}
\end{equation}
In two-phase sampling we estimate the mean of the auxiliary information over the small area $G$ by
$$\hat{\bar{\pmb{Z}}}_{c,1,G}=\frac{\sum_{x\in{s_{1,G}}}M(x)\pmb{Z}_c(x)}{\sum_{x\in{S_{1,G}}}M(x)}$$
with estimated covariance matrix
\begin{equation}\label{estcovauxiliarycluster}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}_{c,1,G}}=\frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}\big(\frac{M(x)}{\bar{M}_{1,G}}\big)^2(\pmb{Z}_c(x)-\hat{\bar{\pmb{Z}}}_{c,1,G})
(\pmb{Z}_c(x)-\hat{\bar{\pmb{Z}}}_{c,1,G})^t
\end{equation}
according to (\cite{mandallaz}, section 4.3).
The \textbf{pseudo-synthetic estimate} is then
\begin{eqnarray}\label{psynthclusterest}
\hat{Y}_{c,G,psynth}&=&\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\beta}}_{c,s_2}\nonumber \\
&=& \frac{1}{n_2}\sum_{x\in{s_2}}g_{c,1,G}(x)Y_c(x)
\end{eqnarray}
with the g-weights $g_{c,1,G}(x)=\hat{\bar{\pmb{Z}}}_{c,1,G}^t\pmb{A}^{-1}_{c,s_2}M(x)\pmb{Z}_c(x)$. The estimated variance is as in  [\ref{estvarpseudosynth1}]

\begin{equation}\label{estvarpseudosynth1cluster}
\hat{\VAR}(\hat{Y}_{c,G,psynth})=
\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,s_2}}\hat{\bar{\pmb{Z}}}_{c,1,G}
+ \hat{\pmb{\beta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{c,1,G}}\hat{\pmb{\beta}}_{c,s_2}
\end{equation}
The pseudo-synthetic estimate is generally design-biased. Adjusting for the residuals we get the  small-area estimator
\begin{equation}\label{psmallclusterest}
\hat{Y}_{c,G,psmall}=\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,s_2}}
+ \frac{\sum_{x\in{s_{2,G}}}M(x)\hat{R}_c(x)}{\sum_{x\in{s_{2,G}}}M(x)}
\end{equation}
It is asymptotically design-unbiased and intuitively its variance can be expected to be approximated by

\begin{eqnarray}\label{estvarpsmallcluster}
\hat{\VAR}(\hat{Y}_{c,G,psmall})&=&
\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,s_2}}\hat{\bar{\pmb{Z}}}_{c,1,G}
+ \hat{\pmb{\beta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{c,1,G}}\hat{\pmb{\beta}}_{c,s_2}
\nonumber \\ &+& \frac{1}{n_{2,g}(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}\big(\frac{M(x)}{\bar{M}_{2,G}}\big)^2
(\hat{R}_c(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
where $\bar{M}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}M(x)$ and
$\bar{\hat{R}}_{2,G}=\frac{\sum_{x\in{s_{2,G}}}M(x)\hat{R}_c(x)}{\sum_{x\in{s_{2,G}}}M(x)}$.
One can rewrite [\ref{estvarpsmallcluster}] with g-weights and predictions as in [\ref{pseudosmallvar2}].
\par
As in simple two-phase sampling we can transform the above estimator into a synthetic estimator by considering the extended model $\pmb{\mathcal{Z}}_c^t(x)=(\pmb{Z}_c^t(x),I_{c,G}(x))\in{\mathcal{R}}^{(p+1)}$ with
$I_{c,G}(x)=\frac{\sum_{l=1}^M I_G(x_l)}{M(x)}$. In extensive inventories we can reasonably assume that all the points of a cluster lying in the forest area $F$ will belong to the same small area $G$ so that in fact $I_{c,G}(x) \equiv 1$ for all $x\in{\tilde{G}}=\{x \mid \sum_{l=1}^M I_G(x_l)>0\}$. The theoretical normal equation
reads
\begin{equation}
\Big(\int_{\tilde{F}}M(x)\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}_c^t(x)dx\Big)\pmb{\theta}_c=:
\pmb{\mathcal{A}}_c\pmb{\theta}_c=\int_{\tilde{F}}M(x)Y_c(x)\pmb{\mathcal{Z}}_c(x)dx
\end{equation}
 which satisfy by construction the two  zero mean residuals properties
$\int_{\tilde{F}} M(x)\mathcal{R}_c(x)dx=\int_{\tilde{G}} M(x)\mathcal{R}_c(x)dx=0$.
The second equality will only hold approximately if $I_{c,G}(x)<1$ for some $x$ in $\tilde{G}$.\\
With
$\pmb{\mathcal{A}}_{c,s_2}
=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}_c^t(x)$
we obtain the estimate of the regression coefficients at the cluster level
\begin{equation}\label{regcoeffclusterext}
\hat{\pmb{\theta}}_{c,s_2}=\pmb{\mathcal{A}}^{-1}_{c,s_2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}_c(x)Y(x)\Big)
\end{equation}
with estimated design-based covariance matrix
\begin{equation}\label{regcoeffclusterextestvar}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,s_2}}=
\pmb{\mathcal{A}}^{-1}_{c,s_2}\Big(\frac{1}{n^2_2}\sum_{\in{s_2}}M^2(x)\hat{\mathcal{R}}_c^2(x)
\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}^t_c(x)\Big)\pmb{\mathcal{A}}^{-1}_{c,s_2}
\end{equation}
where the $\hat{\mathcal{R}}_c(x)=Y_c(x)-\pmb{\mathcal{Z}}^t_c(x)\hat{\pmb{\theta}}_{c,s_2}$ are the empirical residuals at the cluster level with respect to the extended model. We define the pseudo-synthetic estimator in the extended model according to
\begin{equation}\label{estimatorclusterextended}
\hat{\tilde{Y}}_{c,psynth}=\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^t\hat{\pmb{\theta}}_{c,s_2}
\end{equation}
where
$\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}=\frac{\sum_{x\in{s_{1,G}}}M(x)\pmb{\mathcal{Z}}_c(x)}
{\sum_{x\in{s_{1,G}}}M(x)}$
is the mean of the extended auxiliary vector over the small area.\\
Obviously a decomposition similar to [\ref{extsynthethic3}] will hold so that $\hat{\tilde{Y}}_{c,psynth}$ and $\hat{Y}_{c,G,small}$ in [\ref{psmallclusterest},\ref{estimatorclusterextended}]
are asymptotically equivalent.\\
As in [\ref{estvarpseudosynth1cluster}] the estimated variance is given by
\begin{equation}\label{estvarpsynth1clusterextended}
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,psynth})=
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,s_2}}\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}
+ \hat{\pmb{\theta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}}\hat{\pmb{\theta}}_{c,s_2}
\end{equation}
\noindent where
\begin{equation}\label{estcovauxiliaryclusterextended}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}}=\frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}\big(\frac{M(x)}{\bar{M}_{1,G}}\big)^2(\pmb{\mathcal{Z}}_c(x)-
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G})
(\pmb{\mathcal{Z}}_c(x)-\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G})^t
\end{equation}
Defining the g-weights at the cluster level as
\begin{equation}
\tilde{g}_{G,c,1}(x)=\hat{\bar{\pmb{\mathcal{Z}}}}^t_{c,1,G}\pmb{\mathcal{A}}^{-1}_{c,s_2}M(x)
\pmb{\mathcal{Z}}_c(x)
\end{equation}
we obtain as usual
\begin{equation}\label{gweightclusterpointestextended}
\hat{\tilde{Y}}_{c,G,psynth}=\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,c,1}(x)Y_c(x)
\end{equation}
and
\begin{equation}\label{gweightclustervarestextended}
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,psynth})=\frac{1}{n_2^2}\sum_{x\in{s_2}}
\tilde{g}^2_{G,c,1}(x)\hat{\mathcal{R}}^2(x)+
\hat{\pmb{\theta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}}\hat{\pmb{\theta}}_{c,s_2}
\end{equation}

The synthetic estimator in the extended model corresponds formally to $n_1=\infty$, i.e.
\begin{equation}
\hat{\tilde{Y}}_{c,G,synth}=\bar{\pmb{\mathcal{Z}}}_G^t\hat{\pmb{\theta}}_{c,s_2}
 \end{equation}
 with estimated variance
 \begin{equation}
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,synth})=
\bar{\pmb{\mathcal{Z}}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,s_2}}\bar{\pmb{\mathcal{Z}}}_{G}
\end{equation}
with obvious modification for the g-weights
\begin{eqnarray}
\tilde{g}_{G,c}(x)&=&\bar{\pmb{\mathcal{Z}}}_G^t\pmb{\mathcal{A}}^{-1}_{c,s_2}M(x)
\pmb{\mathcal{Z}}_c(x)\nonumber \\
\hat{\tilde{Y}}_{c,G,synth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,c}(x)Y_c(x)\nonumber \\
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,synth})&=&\frac{1}{n_2^2}\sum_{x\in{s_2}}
\tilde{g}^2_{G,c}(x)\hat{\mathcal{R}}^2(x)
\end{eqnarray}
\noindent
\textbf{Properties of the g-weights}:
\begin{enumerate}
\item
 We have
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{c,G,1}(x)\pmb{Z}_c(x)=
 \hat{\bar{\pmb{Z}}}_{c,1,G}$  and  $\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{c,G,1}(x)\pmb{\mathcal{Z}}_c(x)=
 \hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}$.
 \item
 By considering $Y_c(x)\equiv 1$ one gets
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{c,G,1}(x)=
  \frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{c,G,1}(x)=1$, i.e. the g-weights have means equal to 1.
 \item
  When $G=F$ the estimator $\hat{Y}_{c,reg}$ is asymptotically equivalent to the sample mean over $F$, i.e. to $\frac{\sum_{x\in{s_2}}M(x)_cY(x)}{\sum_{x\in{s_2}}M(x)}$. Thus, for large $n_2$, $g_{c,F,1}(x)\approx \frac{M(x)}{\bar{M}_2}$ with $\bar{M}_2=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)$. Likewise, $\hat{\tilde{Y}}_{G,synt}$ is asymptotically equivalent to the sample mean over the small domain, i.e. to
  $\bar{Y}_{c,G,2}=\frac{\sum_{x\in{s_{2,G}}}M(x)_cY(x)}{\sum_{x\in{s_{2,G}}}M(x)}$. Hence, for large $n_2$ we get $\tilde{g}_{c,G,1}(x)\approx 0$ for $x\not\in{\tilde{G}}$ (negative values are possible) and to $\tilde{g}_{c,G,1}(x)\approx \frac{M(x)}{\bar{M}_{2,G}}$ for $x\in{\tilde{G}}$, where $\bar{M}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}M(x)$.
 \end{enumerate}

The construction of design-unbiased small-area estimators as synthetic or pseudo-synthetic estimators in the extended model containing the indicator variable of the small area of interest is mathematically more convenient than the classical approach. The mathematical approximation of the variances is also more satisfactory than simply treating the internal model as an external one and it can be formulated within the g-weight technique, which is known to offer several theoretical advantages.\\
It is mathematically clear that one can generalize all the previous results to the simultaneous estimation of $q \ge 2$ small areas by extending the model with $q$ indicator variables (combined extended model). One can conjecture that the combined model will be less efficient, for any given small area, than the individual estimation and, on the other hand, that it will smooth out the residual pattern.\\

\section{Generalization to two-stage sampling}
In many applications costs to measure the response variable $Y_i$ are high. For instance,
a good determination of the volume may require that one records
$DBH$, as well as the diameter at $7m$ above ground and total height
in order to utilize a three-way volume function. However, one could rely
on a coarser, but cheaper, approximation of the volume based only on
$DBH$. Nonetheless, it may be most sensible to assess those three
parameters only on a sub-sample of trees. We now briefly formalize this
simple idea, which is used in the Swiss National Forest Inventory. The reader is referred to (\cite{mandallaz}, section 4.4, 4.5, 5.4 and 9.5) for details. For each point $x\in{s_2}$ trees are drawn with
probabilities $\pi_i$. The set of selected trees is denoted by
$s_{2}(x)$. From each of the selected trees $i\in{s_{2}(x)}$ one
gets an approximation $Y_i^*$ of the exact value $Y_i$. From the
finite set $s_{2}(x)$ one draws a sub-sample
$s_{3}(x)\subset{s_{2}(x)}$ of trees by Poisson sampling. For each tree $i\in{s_{3}(x)}$
one then measures the exact variable $Y_i$. Let us now define the
second stage indicator variable
\begin{equation}
 J_i(x)=\begin{cases}&1 \text{ if $i\in s_{3}(x)$}\\
                      &0 \text{ if $i\not\in s_{3}(x)$}
         \end{cases}
\end{equation}

 \par To construct a good point estimate, we must have
 \textbf{the residual} $R_i=Y_i-Y_i^*$ which is known only for trees
 $i\in{s_{3}(x)}$. The \textbf{generalized local density} $Y^*(x)$ is defined
 according to    \index{Generalized local density}
 \begin{eqnarray}\label{gdens}
 Y^*(x)&=&\frac{1}{\LF}\left( \sum_{i=1}^N \frac{I_{i}(x)Y_i^*}{\pi_i} +
 \sum_{i=1}^N \frac{I_{i}(x)J_{i}(x)R_i}{\pi_{i}p_i}\right)\nonumber \\
 &=&\frac{1}{\LF}\left( \sum_{i\in{s_{2}(x)}} \frac{Y_i^*}{\pi_i}
 +\sum_{i\in{s_{3}(x)}} \frac{R_i}{\pi_{i}p_i}\right)
 \end{eqnarray}
 where the $p_i$ are the conditional inclusion probabilities for the the second stage sampling, i.e. $p_i=\PR(J_i(x)=1 \mid I_i(x)=1)$.
 It follows from general principles presented in (\cite{mandallaz}, sections 4.4 and 4.5) that one can use all the previous results by replacing everywhere the exact local densities $Y(x)$, or $Y(x_l)$ in cluster sampling, by the corresponding generalized local densities $Y^*(x)$ or $Y^*(x_l)$. The second-stage variance is automatically taken into account.
\section{Examples}
\subsection{Post-stratification}\label{poststrat}
We consider the important special case of post-stratification, which illustrates the main issues. We consider a forested area $F$ partitioned in $L$ strata $F_k$, i.e. $F=\cup_{k=1}^L F_k$ and a small area $G\subset F$, we set $G_k=G \cap F_k$. Note some $G_k$ might be the empty set. The auxiliary vector is defined by the indicator variables of the $L$ strata, i.e.
$$\pmb{Z}^t(x)=(I_{F_1}(x),I_{F_2}(x),\ldots I_{F_L}(x))$$
 where $I_{F_k}(x)=1$ if $x\in{F}_k$ otherwise $I_{F_k}(x)=0$. Note that condition [\ref{linearmodel2}] is fulfilled. Straightforward calculations lead to the $(L,L)$ diagonal matrix $\pmb{A}_{s_2}=\frac{1}{n_2}diag(n_{2,k})$ where $n_{2,k}=\sum_{x\in{s_2}}I_{F_k}(x)$. This leads to the obvious regression estimate
 $$\hat{\pmb{\beta}}_{s_2}=(\hat{\beta}_1,\hat{\beta}_2,\ldots \hat{\beta}_L)^t$$
 with the empirical strata means $\hat{\beta}_k=\frac{1}{n_{2,k}}\sum_{x\in{s_2}\cap F_k}Y(x)=\hat{\bar{Y}}_k$.
 After some elementary algebra the estimated variance-covariance matrix is found to be the diagonal $(L,L)$ matrix
 $$\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}=diag(\frac{s_k^2}{n_{2,k}})$$
 where
 $s_k^2=\frac{1}{n_{2,k}}\sum_{x\in{F_k}}\hat{R}^2(x)$ with $\hat{R}(x)=Y(x)-\hat{\beta}_k$ for $x\in{F}_k$.\\
 One obtains for the empirical mean of the auxiliary vector over the small area
 $$\hat{\bar{\pmb{Z}}}_{1,G}=(\hat{p}_{1,G},\hat{p}_{2,G},\ldots, \hat{p}_{L,G})^t=\hat{\pmb{p}}_{1,G}$$
 where
 $$\hat{p}_{k,G}=\frac{\sum_{x\in{s_1}}I_{G_k}(x)}{\sum_{x\in{s_1}}I_G(x)}:=\frac{n_{1k,G}}{n_{1,G}}$$
 \noindent are the proportions of the strata surfaces areas within the small area as estimated from the large sample. Conditionally on $n_{1,G}$ the $n_{1k,G}$ follow the multinomial distribution with cell probabilities given by the vector
 $\pmb{p}^t_{G}=(\frac{\lambda(G_1)}{\lambda(G)},\frac{\lambda(G_2)}{\lambda(G)},\ldots \frac{\lambda(G_L)}{\lambda(G)})$. In this case the estimated variance-covariance matrix is known to be given by
 \begin{equation}\label{covmatrixcellfreq}
 \hat{\pmb{\Sigma}}_{\hat{\pmb{p}}_{1,G}}=
\frac{1}{n_{1,G}}\left[ \begin {array}{cccc}
\hat{p}_{1,G}(1-\hat{p}_{1,G})& \hat{p}_{1,G}\hat{p}_{2,G} &\ldots& \hat{p}_{1,G}\hat{p}_{1,L}\\
\hat{p}_{1,G}\hat{p}_{2,G}& \hat{p}_{2,G}(1-\hat{p}_{2,G}) &\ldots& \hat{p}_{2,G}\hat{p}_{1,L}\\
\ldots & \ldots & \ldots & \ldots \\
\hat{p}_{1,G}\hat{p}_{1,L} & \hat{p}_{2,G}\hat{p}_{1,L}& \ldots & \hat{p}_{1,L}(1-\hat{p}_{1,L}) \end {array} \right]
\end{equation}
Note that the same is obtained by using [\ref{estvarcovaux}] after replacing $n_{1,G}-1$ by $n_{1,G}$. Simple algebra leads then to the pseudo-synthetic estimate
\begin{equation}\label{postsynth1}
\hat{Y}_{G,psynth}=\hat{\pmb{p}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}=\sum_{k=1}^L\hat{p}_{k,G}\hat{\beta}_k
\end{equation}
with estimated asymptotic design-based variance
\begin{equation}\label{postsynth1}
\hat{\VAR}(\hat{Y}_{G,psynth})=\sum_{k=1}^L\hat{p}^2_{k,G}\frac{s_k^2}{n_{2,k}} +
\frac{1}{n_{1,G}}\sum_{k=1}^L \hat{p}_{k,G}\big(\hat{\beta}_k-\hat{Y}_{G,psynt}\big)^2
\end{equation}
When $n_1=\infty$ and $G=F$ this is precisely the exact conditional variance estimate, i.e. given the $n_k$. The g-weights are $g_{s_2}(x)=p_k\frac{n_2}{n_{2,k}}$ for $x\in{F}_k$, where $p_k=\frac{\lambda(F_k)}{\lambda(F)}$. Thus, for $n_1 < \infty$ the overall variance will depend on the variances within strata and on the variance between strata, which is given by the second term. Note also that for $G=F$ in [\ref{postsynth1}] the strata weights are estimated from the large sample whereas this is not the case for the external model approach [\ref{varexternalsimple}], which illustrates perfectly the better conditional properties of the g-weights technique (see \cite{mandallaz}, p.84).\\
\noindent\textbf{Remarks}\\
\noindent If we assume the $\lambda(F_k)$ and therefore $\pmb{A}$ to be known, the estimator $$\hat{\pmb{\beta}}_0=\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)\Big)$$ is easily found to be $(\frac{n_{21}}{n_2p_1}\hat{\bar{Y}}_1, \frac{n_{22}}{n_2p_2}\hat{\bar{Y}}_2,\ldots \frac{n_{2L}}{n_2p_L}\hat{\bar{Y}}_L)^t$ and yields $\hat{Y}_{synth}=\frac{1}{n_2}\sum_{\in{s_2}}Y(x)$, which is unbiased but useless. If we use $\pmb{A}_{s_2} $ to estimate $\pmb{\beta}$ we get as shown above $\hat{\beta}_k=\hat{\bar{Y}}_k$, which is very intuitive, and if we use $\pmb{A}^{-1}$ instead of $\pmb{A}^{-1}_{s_2}$ in the Taylor approximation for the variance, we obtain $\hat{\VAR}(\hat{Y}_{synth})=\sum_{k=1}^L\frac{n_{2k}s_k^2}{n^2_2}$ instead of $\sum_{k=1}^L p_k^2 \frac{s_k^2}{n_{2k}}$, the later is of course much better from a conditional point of view (even if both estimates are asymptotically equivalent). This examples illustrates why it is better to work with $\pmb{A}^{-1}_{s_2}$ throughout.
\\
It can be easily checked that the original small-area estimator is given by
$$\hat{Y}_{G,psmall}=\sum_{k=1}^L \hat{p}_{k,G}\hat{\bar{Y}}_k+\sum_{k=1}^L\frac{n_{2k,G}}{n_{2,G}}
(\hat{\bar{Y}}_{k,G}-\hat{\bar{Y}}_k)$$
where $n_{2k,G}=\sum_{x\in{s_2}}I_{G_k}(x)$ and $\hat{\bar{Y}}_{k,G}=\frac{1}{n_{2k,G}}\sum_{x\in{s_2 \cap G_k}}Y(x)$. Thus, the residual term will have an impact if the strata means within the small area differ from the strata means within the entire domain, which is intuitively clear.
\\ The formulae for the variances are very cumbersome and not really informative, likewise for
$\hat{\tilde{Y}}_{G,psynth}$ in the extended model.
\subsection{Case study}
We reanalyze the case study described with full details in Chapter 9 of \cite{mandallaz}. The inventoried area covered $218ha$.  The auxiliary information is based on 16 stands defined by the following qualitative variables:
\begin{enumerate}
\item
\textbf{Developmental stage} \newline This entails four categories
``pole stage=3," ``young timber tree=4," ``middle age timber
tree=5," and ``old timber tree=6." These were assigned according to
the dominant diameter.
\item
\textbf{Degree of mixture} \newline This variable was simplified to
the categories of ``predominantly conifers=1" and ``predominantly
broadleaves=2."
\item \textbf{Crown closure} \newline
This variable was based on canopy density, defined as the proportion
of the entire ground surface within the stand that was covered by
the tree crowns. It was simplified to the categories of ``dense=1"
and ``close=2."
\end{enumerate}
These factors produced $4 \times 2 \times 2=16$ possible stands, all
of which were found on the study site. \\
The inventory utilized systematic cluster sampling. The cluster
comprises five points: central point, two points each established
$30\;m$ east or west of the central point; two other points each
established $40\;m$ either north or south of the central point.\\
The first phase sets the
central cluster point on a $120\;m$ W-E by $75\;m$ N-S rectangular
grid (note that the clusters partially overlapped in the N-S
direction). The second, terrestrial phase, place the central point on a 1:4
sub-grid of the first phase, i.e. on a $240\;m$ W-E by $150\;m$ N-S
systematic rectangular grid. The terrestrial inventory was purely one-stage
with simple circular plots of $300m^2$ horizontal surface area, and
an inventory threshold set at 12cm DBH.\\
We use the following linear model with the
vector $\pmb{Z}(x)$:

\begin{itemize}
\item
$Z_1(x) \equiv 1$ intercept term
\item
$Z_2(x)=1$ if $x$ lies in Development Stage 3 and $Z_2(x)=0$
otherwise
\newline $Z_3(x)=1$ if $x$ lies in Development Stage 4 and $Z_3(x)=0$
otherwise
\newline $Z_4(x)=1$ if $x$ lies in Development Stage 5 and $Z_4(x)=0$
otherwise
\newline $Z_2(x)=Z_3(x)=Z_4(x)=-1$ if $x$ lies in Development Stage
6
\item
$Z_5(x)=1$ if $x$ lies in a coniferous stand and $Z_5(x)=-1$
otherwise
\item
$Z_6(x)=1$ if $x$ lies in a dense stand and $Z_6(x)=-1$ otherwise
\end{itemize}
Hence, we have an additive ANOVA model with 7 parameters, as compared with 16 parameters for the full stratification model.\\
We shall consider 5 small areas:
\begin{itemize}
\item
Small area $G_1$ ($\approx 17ha$) was used for a full census. The condition that a cluster hitting the small area has all its points in $F$ within the small area is occasionally  violated (i.e. $I_{c,G}(x)=\frac{\sum_{l=1}^M I_G(x_l)}{M(x)} <1$ for some $x$), so that the extended model for $G_1$ is only approximately correct. The mean residual over the small area is not exactly zero. The true values for basal area and stem densities are known.
\item
Small area $G_2$ ($\approx 33ha$) is the most eastern part of the forest.
\item
Small area $G_{21}\subset G_2$ ($\approx 7ha$) is a small subset in the central part of $G_2$ chosen to have a small number ($3$) of complete clusters ($I_{c,G}(x)\equiv 1$) spread over many different stands.
\item
Small area $G_3$ ($\approx 46ha$) is the most southern part of the forest.
\item
Small area $G_4$ ($\approx 55ha$) is the central part north of $G_3$.
\item
Small area $G_5$ ($\approx 84ha$) is the most western part north of of $G_3$.
\end{itemize}
We have $F=G_2\cup G_3\cup G_4\cup G_5$ and $I_{c,G_k} \equiv 1$ for $k=2,3,4,5$.
Fig. \ref{zbergterrestrial} displays the terrestrial plots according to the domains $G_2-G_5$. Stand map of $F$ and detailed maps of $G_1$ are given in \cite{mandallaz} Chapter 8.
\begin{figure}[h]
\begin{center}
\caption{\label{zbergterrestrial}\textbf{Location of terrestrial plots in $G_2-G_5$}}
\end{center}
\begin{center}
\includegraphics[scale=0.50]{domain2.jpg}
\end{center}
\end{figure}

Tables \ref{newresult1} and \ref{newresult2} displays the result for the basal area and the stem density for the small areas $G_1$, $G_2$, $G_{21}$ and $G_3$.\\
The standard error for $\hat{Y}_{c,G,small}$ are given within $(-)$ when considering the internal model as an external one, i.e. by using the formulae (see \cite{mandallaz} p. 87):
\begin{equation*}
\hat{Y}_{c,G,psmall}=\frac{\sum_{x\in{s_{1,G}}}M(x)\hat{Y}_{c}(x)}{\sum_{x\in{s_{1,G}}}M(x)}
+\frac{\sum_{x\in{s_{2,G}}}M(x)\hat{R}_{c}(x)}{\sum_{x\in{s_{2,G}}}M(x)}
\end{equation*}
\begin{eqnarray*}
\widehat{\VAR}(\widehat{Y}_{c,G,psmall})&=&\left(1-\frac{n_{2,G}}{n_{1,G}}\right)
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}
\sum_{x\in{s_{2,G}}}\left(\frac{M(x)}{\bar{M}_{2,G}}\right)^{2}(\hat{R}_c(x)-\hat{\bar{R}}_{2,G})^2\\
&+&
\frac{1}{n_{1,G}}\frac{1}{(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}\left(\frac{M(x)}{\bar{M}_{2,G}}
\right)^{2}(Y_c(x)-\hat{Y}_{2,G})^2
\end{eqnarray*}
The standard errors given in $[-]$ refer to the various Taylor expansions given in [\ref{estvarpseudosynth1cluster}], [\ref{estvarpsmallcluster}], [\ref{estvarpsynth1clusterextended}] or their equivalent g-weights versions.\\

The extended model for the
$\hat{\tilde{Y}}_{c,G_k,psynt}$ contain only the indicator variable of the corresponding small area $G_k$. For this reason the corresponding estimates for the entire domain $F$ are given for $G_1$ $G_2$ and $G_3$ separately (in this order).\\
 We also consider the joint estimation of $F$ and the small areas $G_2$, $G_3$, $G_4$ and $G_5$, which form a partition of $F$. The corresponding model $\pmb{\mathcal{Z}}(x)$ contains the $4$ indicator variable $I_{G_k}(x)$ ($k=2,3,4,5$), the previous components $Z_l(x), l=2,3,4,5,6$ but no longer the intercept term $Z_1(x) \equiv 1$ (otherwise $\pmb{\mathcal{A}}_{s_2}$ would be singular because $Z_1(x)$ is a linear combination of the $I_{G_k}(x)$). The results for this estimator, denoted by $ \hat{\tilde{Y}}_{c,G,cpsynt}$, are displayed in Table  \ref{newresult3}.\\
 All the calculations were performed with the linear algebra procedure \textbf{proc iml} of the statistical software package SAS.
\section{Discussion and conclusions}
In the case study all point estimates were close to each other and do not differ significantly from each other. In the small area with full census the synthetic estimator was closer to the true values. As confirmed by simulations this was due to the fact that the plots within this small area were in the lower tail of the distribution for basal area and stem density. Of course, the synthetic estimators always had the smallest standard errors but at the potential cost of a local bias.\\
For the classical small-area estimator the standard errors based on the external model assumption were usually, but not always, smaller than their counterparts based on the g-weights (equivalent to the Taylor asymptotic expansions), but the differences were small, a reassuring result. The g-weights based standard errors of the synthetic estimators in the extended model for one single small area were usually smaller than their g-weights counterparts of the classical small-area estimator but generally still larger than under the external model assumption. The g-weights based standard errors in the extended model with several small areas were comparable to those derived specifically for one single small area. In this case study  the various methods can be regarded as practically almost equivalent, which should be confirmed or eventually invalidated by further examples.\\
From a mathematical point of view the Taylor based g-weights technique in the models extended by the indicator variables of the small areas is without any doubts the most elegant approach: it bypasses the residuals terms and allows for a straightforward calculation of the asymptotic variances that takes into account the errors of the regression coefficients.

\newpage

\bibliography{biblio1}
\newpage
\begin{table}[h]
\begin{center}
\caption{ \label{newresult1}\textbf{Two-phase estimates for basal area}}
\end{center}
\begin{center}
 \begin{tabular}{|l|c|c|c|c|}\hline
                      & \textbf{sample sizes} & $\hat{Y}_{c,G,psynth}$   &$ \hat{Y}_{c,G,psmall}$ & $\hat{\tilde{Y}}_{c,G,psynth}$\\[1.0ex]
                & $n_1:n_2$  &   &  &  \\[1.0ex]
 \textbf{Domain}& $n_1\bar{M}_1:n_2\bar{M}_2$ &   &  &  \\ \hline \hline
 $F$            &  $ 298:73$ &  $31.34$    &  $31.34$    & $31.30[0.92]$   \\ [1.0ex]
                & $1203:298$ &  [0.94]     &  $[0.94]$   & $31.35[0.93]$ \\ [1.0ex]
                &            &             &  $(0.91)$   & $31.35[0.94]$  \\ \hline \hline
 $G_1$          &  $29:8$    &  30.28    &  $23.99$              & $25.55$     \\ [1.0ex]
  true=$29.60$  &  $92:19$   &  [1.34]   &  $[3.90]$             & $[3.79]$   \\ [1.0ex]
                &            &           &  $(3.68)$             &             \\ \hline\hline
 $G_2$          &  $49:9$    &  28.27    &   $29.32$             & $29.31$     \\ [1.0ex]
                &  $185:41$  &  [1.40]   &  $[2.52]$             & $[2.23]$  \\ [1.0ex]
                &            &           &  $(2.08)$             &            \\ \hline
 $G_{21}\subset G_2$ &$17:3$ & $25.55 $  &   $29.52 $            & $ 29.61$     \\ [1.0ex]
                 &  $39:15$   &  $[2.16]$     &  $[ 4.13]$        & $[2.95]$  \\ [1.0ex]
                &            &               &  $(3.53)$         &            \\ \hline \hline
  $G_3$         &  $73:18$   &  31.62    &  $31.46$              & $31.46$  \\ [1.0ex]
                & $250:66$   &  [1.47]   &   $[2.33]$            & $[2.16] $ \\[1.0ex]
                &             &            &  $(1.87)$            & \\ \hline \hline
\end{tabular}
\end{center}
\textbf{Standard errors:} $[-]$ (Taylor, g-weights) and $(-)$ (external model)
\end{table}
\noindent \textbf{Remark:}\\
The mean residual for small area $G_1$ was $-1.59$ for the extended model instead of $0$ because $I_{c,G}(x) \not\equiv 1$.

\newpage
\begin{table}[h]
\begin{center}
\caption{ \label{newresult2}\textbf{Two-phase estimates for stem density}}
\end{center}
\begin{center}
 \begin{tabular}{|l|c|c|c|c|}\hline
        & \textbf{sample sizes} & $\hat{Y}_{c,G,psynth}$  & $\hat{Y}_{c,G,psmall}$ & $ \hat{\tilde{Y}}_{c,G,psynt}$ \\[1.0ex]
        & $n_1:n_2$   & &   &  \\[1.0ex]
    \textbf{Domain} & $n_1\bar{M}_1:n_2\bar{M}_2$ &   &  &  \\ \hline\hline
 $F$            & $ 298:73$  &  $325.79$    &  $325.79$       & $325.62[12.81]$     \\ [1.0ex]
                & $1203:298$ &  $[12.80]$   &  $[12.80]$      & $325.88[12.84]$     \\ [1.0ex]
                &            &              &  $(12.39)$      & $325.72[12.85]$ \\ \hline\hline
 $G_1$          &  $29:8$    &  $279.54$    &  $257.34$              & $258.20$     \\ [1.0ex]
 true=$280.23$  &  $92:19$   &  $[22.65]$   &  $[45.81]$             & $[54.07]$   \\ [1.0ex]
                &            &              &  $(48.29)$             &    \\ \hline\hline
 $G_2$          &  $49:9$    &  $400.49$    &  $406.47$              & $406.41 $     \\ [1.0ex]
                & $185:41$   &  $[23.36]$   &  $[41.83]$             & $[36.22 ]$  \\ [1.0ex]
                &            &              &  $(43.49)$             &            \\ \hline
  $G_{21}\subset G_2$ &$17:3$    &  $578.90$    &  $589.51$           & $589.74 $ \\ [1.0ex]
                    & $39:15$   &  $[35.48]$   &  $[85.68]$          & $[67.16]$  \\ [1.0ex]
                &            &                 &  $(94.31)$          &            \\ \hline\hline
 $G_3$          &  $73:18$   &  $279.75$    &  $282.46$              & $282.40$  \\ [1.0ex]
                & $250:66$   &  $[15.41]$   &  $[21.38]$             & $[20.14 ] $ \\[1.0ex]
                &            &              &  $(16.56)$             & \\ \hline\hline
\end{tabular}
\end{center}
\textbf{Standard errors:} $[-]$ (Taylor, g-weights) and $(-)$ (external model)
\end{table}
\noindent \textbf{Remark:}\\
\noindent The mean residual for small area $G_1$ was $-1.00$ for the extended model instead of $0$ because $I_{c,G}(x) \not\equiv 1$.

\newpage
\begin{table}[h]
\begin{center}
\caption{ \label{newresult3}\textbf{Two-phase combined estimates}}
\end{center}
\begin{center}
 \begin{tabular}{|l|c|c|c|}
   \textbf{Domain} & \textbf{sample sizes} & \textbf{basal area}  & \textbf{stem density}\\[1.0ex]
 $ $   & $n_1:n_2$    &  & \ \\[1.0ex]
 $ $   & $n_1\bar{M}_1:n_2\bar{M}_2$  & $ \hat{\tilde{Y}}_{c,G,cpsynth}$ &$\hat{\tilde{Y}}_{c,G,cpsynth}$ \\ \hline
 $F$            & $ 298:73$   &  $31.32 $    &  $325.17$       \\ [1.0ex]
                & $1203:298$  &  $[0.93]$    &  $[12.62]$       \\ \hline \hline
 $G_2$          &  $49:9$     &  $29.39 $    &  $407.84$        \\ [1.0ex]
                &  $185:41$   &  $[2.23]$    &  $[39.04]$       \\ \hline \hline
 $G_3$          &  $73:18$     &  $31.57 $    &  $284.17$         \\ [1.0ex]
                & $250:66$    &  $[2.09]$    &  $[18.09]$       \\ \hline \hline
 $G_4$          &  $81:17$     &  $27.77 $    &  $274.59$        \\ [1.0ex]
                & $306:69$    &  $[1.99]$   &  $[23.49]$        \\ \hline \hline
 $G_5$          &  $125:29$    &  $34.31$    &  $347.76$          \\ [1.0ex]
                & $462:122$   &  $[1.24]$   &  $[16.61]$         \\ \hline \hline
\end{tabular}
\end{center}
\textbf{Standard errors:} $[-]$ (Taylor, g-weights).
\end{table}
\noindent \textbf{Remarks}:\\
The classical estimates $\hat{Y}_{c,G,psmall}$ for $G_4$ were: $27.78(2.00)$ for basal area and $274.79(24.12)$ for stem density. For $G_5$  the corresponding results were $34.33(1.35)$ and $347.89(17.49)$.\\
As expected on mathematical grounds all extended models yielded empirical means of the residuals over the entire domain and over one or many small areas which were equal to zero ($<10^{-12})$.



\end{document}
