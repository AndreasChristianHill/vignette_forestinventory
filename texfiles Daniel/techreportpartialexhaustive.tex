\documentclass[a4paper,12pt,leqno, titlepage]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{a4}
\usepackage{graphicx}
\usepackage{flafter}
\usepackage{bm}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{natbib}
\bibliographystyle{mystyle2}
\newcommand{\LF}{\ensuremath{\lambda(F)}}
\newcommand{\LFC}{\ensuremath{\lambda^2(F)}}
\newcommand{\EX}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\VAR}{\mathbb{V}}
\newcommand{\COV}{\mathbb{COV}}
\newcommand{\MAV}{\mathbb{MAV}}
\newcommand{\MRAV}{\mathbb{MRAV}}
\newcommand{\POP}{\mathcal{P}}
\newcommand{\SAMP}{\mathcal{S}}
\newcommand{\RE}{\mathbb{RE}}
\newcommand{\PLAN}{\Re^2}
\newcommand{\SUR}{\mathbb{S}}
\newcommand{\ING}{\mathbb{I}}
\newcommand{\DEP}{\mathbb{D}}
\newcommand{\R}{\mathbb{R}}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{3mm}
\setlength{\headsep}{1cm}
\setlength{\topskip}{0cm}
\setlength{\textwidth}{16cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\makeatletter
\def\tagform@#1{\maketag@@@{[\ignorespaces#1\unskip\@@italiccorr]}}
\makeatother

\begin{document}
\doublespacing
\pagestyle{plain}
\pagenumbering{arabic}

\title{Regression estimators in forest inventories with two-phase sampling and partially exhaustive information with applications to small-area estimation}
\author{Daniel Mandallaz \thanks{Tel. ++41(0)44 6323186 e-mail
    daniel.mandallaz@env.ethz.ch} \\Chair of Land Use Engineering\\ETH Zurich\\CH 8092 Zurich, Switzerland}
\date{}

\maketitle
\newpage
\nolinenumbers
\begin{abstract}
We consider two-phase sampling schemes where one component of the auxiliary information is known in every point ("wall-to wall") and a second component is available only in the large sample of the first phase, whereas the second phase yields a sub-sample with the terrestrial inventory data based on general tree inclusion probabilities. We propose a generalized version of the classical two-phase regression estimator, for global and local estimation and derive its asymptotic design-based variance. Cluster and two-stage sampling procedures are also considered.

\end{abstract}
\clearpage


\section{Introduction}\label{introduction}
\pagenumbering{arabic} \setcounter{page}{1}
The motivation for this work is due to the increasing need of using national or regional inventories for local estimation in order to meet tighter budgetary constraints, which is only feasible under extensive use of auxiliary information, provided e.g. by remote sensing. It is hoped that the proposed estimators will contribute to achieve this objective, particularly because they are easy to implement in software packages like SAS or R.\\
There is an extensive literature on the problem of small area estimation (or small domain estimation in general sampling). In this paper we shall investigate the properties of some estimators in the \textbf{model-assisted framework}, in which prediction models are used to improve the efficiency but are not assumed to be correct as in the \textbf{model-dependent approach}. The validity of the statistical procedures is ensured by the randomization principle: i.e. we are in the \textbf{design-based} inference framework, which has a definite advantage in official statistics. The reader is referred to (\cite{koehl}, section 3.8) for a good review of small-area estimation in forest inventory that presents alternative techniques, in particular Bayesian. Let us now define the sampling scheme.\\
The \textbf{first phase} draws a large sample $s_1$ of $n_1$ points
$x_{i}\in{s_1}$ ($i=1,2\ldots n_1$) that are independently and uniformly distributed
within the forest area $F$. At each of those points auxiliary
information is collected, very often coding information of qualitative nature
(e.g. following the  interpretation of aerial photographs) or quantitative (e.g. timber volume estimates  based on LIDAR measurements). We shall assume that the auxiliary information at point $x$
is described by the row vector $\pmb{Z}^t(x)=(\pmb{Z}^{(1)t}(x),\pmb{Z}^{(2)t}(x))\in{\Re^{p+q}}$ (the upper index $t$ denotes the transposition operator). The first component $\pmb{Z}^{(1)}(x)\in{\Re^{p}}$ of this vector is known at all points $x\in{F}$, it is the \textbf{exhaustive} part of the auxiliary information, e.g. it could be given by thematic maps. The second component $\pmb{Z}^{(2)}(x)\in{\Re}^q$ is known only at points $x\in{s}_1$.\par
 The \textbf{second phase} draws a small sample $s_2\subset{s_1}$ of
$n_2$ points from $s_1$ according to equal probability
sampling without replacement. In the forested area $F$ we consider a well defined population $ \mathcal{P}$ of $N$ trees with response variable
 $Y_i,\;i=1,2 \ldots$, e.g. the timber volume.  The objective is to estimate the spatial mean  $\bar{Y}=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i$, where $\lambda(F)$ denotes the surface area of $F$ (usually in ha). For each point $x\in{s_2}$ trees are drawn from the population $\mathcal{P}$ with probabilities $\pi_i$, for instance with concentric circles or angle count techniques. The
set of trees selected at point $x$ is denoted by $s_{2}(x)$. From each of the
selected trees $i\in{s_{2}(x)}$ one determines $Y_i$. The indicator variable $I_i$ is defined as
\begin{equation}\label{1stage}
 I_i(x)=\begin{cases}&1 \text{ if $i\in s_{2}(x)$}\\
                      &0 \text{ if $i\not\in s_{2}(x)$}
         \end{cases}
\end{equation}
At each point $x\in{s_2}$ the
terrestrial inventory provides the local density $Y(x)$
\begin{equation}\label{truelocaldensity}
 Y(x) =\frac{1}{\lambda(F)}\sum_{i=1}^N \frac{I_i(x)Y_i}{\pi_i}=\frac{1}{\lambda(F)}\sum_{i\in{s}_2(x)} \frac{Y_i}{\pi_i}
 \end{equation}
 The term $\frac{1}{\lambda(F)\pi_i}$ is the tree extrapolation factor $f_i$ with dimension $ha^{-1}$. Because of possible boundary adjustments $\lambda(F)\pi_i=\lambda(F \cap K_i)$, where $K_i$ is the inclusion circle of the $i$-th tree. In the infinite population or Monte Carlo approach one samples the function $Y(x)$ (\cite{mandallaz}) for which the following important relation holds:
 \begin{equation}\label{montecarlo}
 \EX_{x} (Y(x))=\frac{1}{\lambda(F)}\int_{F} Y(x)dx=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i=\bar{Y}
 \end{equation}
 Where $\EX_x$ denotes the expectation with respect to a random point $x$ uniformly distributed in $F$. This establishes the link between the infinite population (continuum) $\{x\in{F} \mid Y(x)\}$ and the finite population of trees $\{i=1,2 \ldots N \mid Y_i\}$.\par
 We shall work with the following linear models (see \cite{mandallaz}, Chapter 6, and \cite{mandallazreport1} for more details)
 \begin{enumerate}
 \item
 The large model $M$
 $$ Y(x)=\pmb{Z}(x)^t\pmb{\beta}+ R(x)=\pmb{Z}^{(1)t}(x)\pmb{\beta}^{(1)}
 +\pmb{Z}^{(2)t}(x)\pmb{\beta}^{(2)}+ R(x)$$
 \noindent with $\pmb{\beta}^t=({\pmb{\beta}^{(1)t}}, {\pmb{\beta}^{(2)t}})$.
 The intercept term is contained in $\pmb{Z}^{(1)}(x)$ or it is a linear combination of its components.\\
 The theoretical regression parameter $\pmb{\beta}$ minimizes
 $$\int_F (Y(x)-\pmb{Z}^t(x)\pmb{\beta})^2dx$$
 It satisfies the normal equation
 $$\big(\int_F\pmb{Z}(x)\pmb{Z}^t(x)dx\big)\pmb{\beta}=\int_F Y(x)\pmb{Z}(x)dx$$ and the orthogonality relationship
 $$\int_F R(x)\pmb{Z}(x)dx=\pmb{0}$$
 \noindent in particular the zero mean residual property
 $$\frac{1}{\lambda(F)}\int_F R(x)dx=0$$
 \item
 The reduced model $M_1$
 $$ Y(x)=\pmb{Z}^{(1)t}(x)\pmb{\alpha} + R_1(x)$$
 The theoretical regression parameter $\pmb{\alpha}$ minimizes
 $$\int_F (Y(x)-\pmb{Z}^{(1)t}(x)\pmb{\alpha})^2dx$$
 It satisfies the normal equation
 $$\big(\int_F \pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)dx\big)\pmb{\alpha}=\int_FY(x)\pmb{Z}^{(1)}(x)dx$$ the orthogonality relationship
 $$\int_F R_1(x)\pmb{Z}^{(1)}(x)dx=\pmb{0}$$
 \noindent in particular the zero mean residual property
 $$ \frac{1}{\lambda(F)}\int_F R_1(x)dx=0$$.
\end{enumerate}
Let us emphasize the fact that we do not assume that the regression models are correct: the inference is based on the sampling design, that is we are doing model-assisted (and not model-dependent or model-based) inference in the sense of S\"{a}rndal (see \cite{sarndal}).
\section{The generalized regression estimator}
We need the following design-based least squares estimators of the regression coefficients of the reduced model, which are essentially solutions of sample copies of the normal equations
\begin{eqnarray}\label{coeff1}
\hat{\pmb{\alpha}}_k &=& \Big(\frac{1}{n_k}\sum_{x\in{s}_k}\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)}(x)^t
\Big)^{-1}\frac{1}{n_k}\sum_{x\in{s}_k}Y(x)\pmb{Z}^{(1)}(x)\nonumber\\
&:=&(\pmb{A}^{(1)}_k)^{-1}
\frac{1}{n_k}\sum_{x\in{s}_k}Y(x)\pmb{Z}^{(1)}(x)\quad k=1,2
\end{eqnarray}
For the large model we set
\begin{equation}\label{coeff2}
\hat{\pmb{\beta}}_{k}=\Big(\frac{1}{n_2}\sum_{x\in{s}_k}\pmb{Z}(x)\pmb{Z}(x)^t
\Big)^{-1}\frac{1}{n_2}\sum_{x\in{s}_k}Y(x)\pmb{Z}(x):
=\pmb{A}^{-1}_k\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{Z}(x)
\end{equation}
Note that only $\hat{\pmb{\alpha}}_2$ and $\hat{\pmb{\beta}}_2$ are observable and that
in general the vector consisting of the first $p$ components of $\hat{\pmb{\beta}}_{2}$ is not equal to $\hat{\pmb{\alpha}}_2$ (they are if the corresponding explanatory variables are orthogonal in the classical least squares sense).\\
The large model yields the predictions $\hat{Y}(x)=\pmb{Z}^t(x)\hat{\pmb{\beta}}_{2}$ and the reduced model the predictions $\hat{Y}_1(x)=\pmb{Z}^{(1)t}(x)\hat{\pmb{\alpha}}_2$.\\
The \textbf{generalized regression estimate} is defined as
\begin{equation}\label{ygreg1}
\hat{Y}_{greg} = \frac{1}{\lambda(F)}\int_F \hat{Y}_1(x)dx
+\frac{1}{n_1}\sum_{x\in{s}_1}(\hat{Y}(x)-\hat{Y}_1(x))
+ \frac{1}{n_2}\sum_{x\in{s}_2}(Y(x)-\hat{Y}(x))
\end{equation}
This estimator is the Monte Carlo version of S\"{a}rndal's  regression estimator for two-phase sampling in finite population (see \cite{sarndal}, equation 9.7.20). It is clear by the law of large numbers that  $\hat{\pmb{\beta}}_2$ and $\hat{\pmb{\alpha}}_2$ are asymptotically design-unbiased estimators of $\pmb{\beta}$ and $\pmb{\alpha}$. This implies at once that
$$\EX_{1,2}\hat{Y}_{reg}= \EX_1\EX_{2 \mid 1}\hat{Y}_{reg} \approx \bar{Y}$$
$ \EX_{2 \mid 1}$ denotes the conditional expectation of the second phase given the first phase (i.e. simple random sampling without replacement in the population $s_1$) and $\EX_1$ denotes the expectation with respect to uniformly distribution points of the first phase (i.e. to $\EX_x$).\\The generalized regression estimate is therefore asymptotically design-unbiased.\\
To understand the potential usefulness of $\hat{Y}_{greg}$ we shall assume for the time being that the model is \textbf{external}, i.e. not fitted by the inventory data, and that the regression coefficients have given fixed values. Using the well known variance decomposition
\begin{equation}\label{conddecomposition}
\VAR(\hat{Y}_{greg})=\VAR_1\EX_{2 \mid 1}(\hat{Y}_{greg})+\EX_1\VAR_{2 \mid 1}(\hat{Y}_{greg})
\end{equation}we get the design-based variance as
\begin{equation}\label{avar1}
\VAR(\hat{Y}_{greg})=\frac{1}{n_1}\VAR_x(R_1(x))+(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR_x(R(x))
\end{equation}
The variances $\VAR_x( \cdot)$ are calculated under the uniform distribution in $F$ of the random point $x$. An unbiased estimate of the variance is given by
\begin{equation}\label{estvarclassic}
\hat{\VAR}(\hat{Y}_{greg})=\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s_2}}(R_1(x)-\hat{\bar{R}}_1)^2
+ (1-\frac{n_2}{n_1})\frac{1}{n_2(n_2-1)}\sum_{x\in{s_2}}(R(x)-\hat{\bar{R}})^2
\end{equation}
where $\hat{\bar{R}}_1=\frac{1}{n_2}\sum_{x\in{s_2}}R_1(x)$ and likewise for $\hat{\bar{R}}$.

This should be compared with the standard result for the variance of the regression estimator $\hat{Y}_{reg}$ under the large model
\begin{equation}\label{classicreg}
\hat{Y}_{reg}=\frac{1}{n_1}\sum_{x\in{s}_1}\hat{Y}(x)+\frac{1}{n_2}\sum_{x\in{s}_2}(Y(x)-\hat{Y}(x))
\end{equation}
whose theoretical variance is given by
\begin{equation}\label{avarclassic}
\VAR(\hat{Y}_{reg})=\frac{1}{n_1}\VAR_x(Y(x))+(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR_x(R(x))
\end{equation}

Thus, by using also the exhaustive information the variance of the observations in [\ref{avarclassic}] is replaced by the variance of the residuals under the reduced model, a very nice and intuitive result indeed. To have better insight consider the design-based coefficients of determination
\begin{equation}\label{coeffdeter}
R_1^2=\frac{\VAR_x(\hat{Y}_1(x))}{\VAR_x(Y(x))} \quad \text{and} \quad R^2=\frac{\VAR_x(\hat{Y}(x))}{\VAR_x(Y(x))}
\end{equation}
According to (\cite{mandallaz}, equation 5.4) the reduction in variance is easily found to be
\begin{equation}\label{varreduction}
\VAR(\hat{Y}_{reg})-\VAR(\hat{Y}_{greg})=\VAR(Y(x))\Big(\frac{R^2-R_1^2}{n_1}+\frac{1-R^2}{n_2}\Big)>0
\end{equation}
We now give an alternative definition of $\hat{Y}_{greg}$ based on the estimated regression coefficients which is essential to derive the design-based variance with \textbf{internal models}, i.e. fitted with the inventory data at hand, and for future generalization to the small-area estimation problem. To this end we need the following mean values
\begin{equation}\label{meanvalues}
\bar{\pmb{Z}}^{(1)}=\frac{1}{\lambda(F)}\int_F \pmb{Z}^{(1)}(x)dx ,\quad \hat{\bar{\pmb{Z}}}^{(1)}_1=\frac{1}{n_1}\sum_{x\in{s}_1}\pmb{Z}^{(1)}(x) ,
\quad \hat{\bar{\pmb{Z}}}_k=\frac{1}{n_k}\sum_{x\in{s}_k}\pmb{Z}(x),\;k=1,2
\end{equation}
The regression estimate can be rewritten as
\begin{eqnarray}\label{ygreg2}
\hat{Y}_{greg}&=&(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t\hat{\pmb{\alpha}}_2 + (\hat{\bar{\pmb{Z}}}_1-\hat{\bar{\pmb{Z}}}_2)^t\hat{\pmb{\beta}}_{2}+\frac{1}{n_2}
\sum_{x\in{s}_2}Y(x) \nonumber \\
&=& (\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t\hat{\pmb{\alpha}}_2 +
\hat{\bar{\pmb{Z}}}^t_1\hat{\pmb{\beta}}_{2}
\end{eqnarray}
The last equations follows from the fact that the sum of the residuals is zero by construction.
Note that it suffices to know the integral of $\pmb{Z}^{(1)}(x)$ and not necessarily the values at all points $x\in{F}$.

\section{Variance estimate}
To obtain a first estimate of the variance we can treat the internal model as an external one and replace in [\ref{avar1}] the theoretical residuals by their empirical versions $\hat{R}_1(x)=Y(x)-\hat{Y}_1(x)=Y(x)-\pmb{Z}^{(1)t}(x)\hat{\pmb{\alpha}}_2$ and $\hat{R}(x)=Y(x)-\hat{Y}(x)=Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_2$, which have zero means, to obtain
\begin{equation}\label{estavar1}
\hat{\VAR}(\hat{Y}_{greg})=\frac{1}{n_1}\frac{1}{n_2}\sum_{x\in{s}_2}\hat{R}_1^2(x)+
\frac{1}{n_2}(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{x\in{s}_2}\hat{R}^2(x)
\end{equation}
To derive better variance estimates we shall use the g-weights technique (for details see \cite{mandallaz}, section 6.2, for the Monte Carlo approach and \cite{sarndal}, sections 6.5 and 6.6 for finite populations). The g-weights are defined by
\begin{eqnarray}\label{gweight1}
g_2(x) &=& 1+(\hat{\bar{\pmb{Z}}}_1-\hat{\bar{\pmb{Z}}}_2)^t\pmb{A}_2 ^{-1}\pmb{Z}(x)  =\hat{\bar{\pmb{Z}}}_1^t\pmb{A}_2}^{-1}{\pmb{Z}(x)\nonumber \\
g^{(1)}_1(x) &=& 1+(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t(\pmb{A}^{(1)}_1)^{-1}\pmb{Z}^{(1)}(x) =\bar{\pmb{Z}}^{(1)t}(\pmb{A}^{(1)}_1)^{-1}\pmb{Z}^{(1)}(x)
\end{eqnarray}
That the two versions of the g-weights are equivalent is a consequence of the zero residual sum for any local density (see \cite{mandallaz}, section 6.2). The g-weights are therefore of order $1+O_p(n_k^{-\frac{1}{2}})$ in probability. Note that they depend not only on the point $x$ but also on the entire sample $s_2$, though weakly. Straightforward algebra leads to the following important calibration properties
\begin{eqnarray}\label{gweight2}
\frac{1}{n_2}\sum_{x\in{s}_2}g_2(x)\pmb{Z}(x)&=&\hat{\bar{\pmb{Z}}}_1 \nonumber \\
\frac{1}{n_1}\sum_{x\in{s}_1}g^{(1)}_1(x)\pmb{Z}^{(1)}(x)&=&\bar{\pmb{Z}}^{(1)}
\end{eqnarray}
and also to
\begin{eqnarray}\label{gweight3}
\frac{1}{n_1}\sum_{x\in{s}_1}g_1^{(1)}(x)Y(x)&=&\frac{1}{n_1}\sum_{x\in{s_1}}Y(x) +
(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_1)\hat{\pmb{\beta}}^{(1)}_1 \nonumber \\
\frac{1}{n_2}\sum_{x\in{s}_2}g_2(x)Y(x)&=&\frac{1}{n_2}\sum_{x\in{s}_2}Y(x) +
(\hat{\bar{\pmb{Z}}}_1-\hat{\bar{\pmb{Z}}}_2)^t\hat{\pmb{\beta}}_2 \nonumber \\
&=& \hat{\bar{\pmb{Z}}}_1^t\hat{\pmb{\beta}}_2=\hat{Y}_{reg}
\end{eqnarray}
$\hat{Y}_{reg}$ is the standard regression estimator based on the large model only (\cite{mandallaz}, section 5.1). The last equation in [\ref{gweight3}] follows again from the fact that the residuals sum up to zero. Also, note that the first quantity in [\ref{gweight3}] is not observable. \\
Intuitively, because the g-weights provide perfect estimates for the means of the auxiliary variables, they must perform well for the response variables if the models are adequate. Let us define
\begin{equation}\label{delta}
\Delta=(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t
(\hat{\pmb{\alpha}}_2-\hat{\pmb{\alpha}}_1)
\end{equation}
Then, using [\ref{gweight3}] and [\ref{ygreg2}] we obtain after some algebra the following formal decomposition of the regression estimate
\begin{equation}\label{yreg3}
\hat{Y}_{greg}=\frac{1}{n_1}\sum_{x\in{s}_1}g_1^{(1)}(x)Y(x)+
\frac{1}{n_2}\sum_{x\in{s}_2}g_2(x)Y(x)-\frac{1}{n_1}\sum_{x\in{s}_1}Y(x) +\Delta
\end{equation}
This is a purely formal identity because only the second term is observable. In the definition
[\ref{delta}] the first factor is of order $O(n_1^{-\frac{1}{2}})$ in design-probability and likewise the second factor of order $O(n_2^{-\frac{1}{2}})$. Thus $\Delta$ is of order $=O(n_2^{-1})$ in design-probability and can be neglected with respect to the first three terms in [\ref{yreg3}], which are of order $O(1)$.\\
In many applications with categorical explanatory variables it can happen that the matrices occurring in
[{\ref{gweight1}] are singular, in which case generalized inverse must be used instead. In this case any particular solution $\hat{\pmb{\beta}}_2^*$ of the consistent normal equations can be used and all statistically relevant quantities like g-weights and predictions are independent of the particular solution chosen and equations [\ref{gweight2}] remain valid (see \cite{Renssen} for details and further references). \\
We introduce the theoretical residuals $R_1(x)$ and $R(x)$ by the relations
\begin{eqnarray}\label{theorres}
Y(x)&=& R_1(x)+\pmb{Z}^{(1)t}(x)\pmb{\beta}^{(1)} \nonumber \\
Y(x) &=& R(x)+\pmb{Z}^t(x)\pmb{\beta}
\end{eqnarray}
Substituting these equalities into [\ref{yreg3}] and using [\ref{gweight2}] we obtain
following expression for the error term
\begin{eqnarray}\label{error1}
\hat{Y}_{greg}-\bar{Y} &=&\Big(\frac{1}{n_1}\sum_{x\in{s_1}}g_1^{(1)}(x)R_1(x)-\frac{1}{\lambda}\int_F R_1(x)dx\Big) \nonumber \\
&+& \Big(\frac{1}{n_2}\sum_{x\in{s_2}}g_2(x)R(x)-\frac{1}{n_1}\sum_{x\in{s_1}}R(x)\Big)+ \Delta
\end{eqnarray}
 This is the Monte Carlo version of equation 9.7.18 given in \cite{sarndal}.\\
According to general heuristic principles (described in \cite{mandallaz}, section 6.2 and \cite{sarndal}, section 6.6) it can be expected that the following variance estimate based on the g-weights has better performances
\begin{equation}\label{estavar2}
\hat{\VAR}(\hat{Y}_{greg})=\frac{1}{n_1}\frac{1}{n_2}\sum_{x\in{s}_2}(g_1^{(1)}(x))^2\hat{R}_1^2(x)+
\frac{1}{n_2}(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{x\in{s}_2}g_2^2(x)\hat{R}^2(x)
\end{equation}
This is the perfect Monte Carlo analogy of equation 9.7.22 in \cite{sarndal}.\\
We now propose a different technique to obtain the design-based variance of $\hat{Y}_{greg}$ which is better suited for the small-area estimation problem. The starting point is the following important result for the design-based variance of the regression coefficients based on the Taylor linearization technique (for proofs see \cite{mandallaz}, p. 125 and \cite{mandallazreport1}) which leads to the asymptotic covariance matrices
\begin{equation}\label{lemma1}
\pmb{\Sigma}_{\hat{\pmb{\beta}}_k}=\pmb{A}^{-1}
\Big(\frac{1}{n_k}\EX R^2(x)\pmb{Z}(x)\pmb{Z}^t(x)\Big)\pmb{A}^{-1}
\end{equation}
where $\pmb{A}=\EX_x \pmb{Z}(x)\pmb{Z}^t(x)$ and
\begin{equation}\label{lemma2}
\pmb{\Sigma}_{\hat{\pmb{\alpha}}_k}=(\pmb{A}^{(1)})^{-1}
\Big(\frac{1}{n_k}\EX R_1^2(x)\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)\Big)(\pmb{A}^{(1)})^{-1}
\end{equation}
We use the variance decomposition [\ref{conddecomposition}] on [\ref{ygreg2}]. One gets
$$\EX_{2 \mid 1}\hat{Y}_{greg}=(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}_1^{(1)})^t\hat{\pmb{\alpha}}_1+
\hat{\bar{\pmb{Z}}}_1^t\hat{\pmb{\beta}}_1 \approx \bar{\pmb{Z}}^{(1)t}\hat{\pmb{\alpha}}_1$$
because $\hat{\bar{\pmb{Z}}}_1^{(1)t}\hat{\pmb{\alpha}}_1$ and $\hat{\pmb{Z}}^t_1\hat{\pmb{\beta}}_1$ both tend to $\bar{Y}$ asymptotically. Therefore, one has
$$\VAR_1\EX_{2 \mid 1}(\hat{Y}_{greg})=\bar{\pmb{Z}}^{(1)t}\pmb{\Sigma}_{\hat{\pmb{\alpha}}_1}\bar{\pmb{Z}}^{(1)}
=\frac{n_2}{n_1}\bar{\pmb{Z}}^{(1)t}\pmb{\Sigma}_{\hat{\pmb{\alpha}}_2}\bar{\pmb{Z}}^{(1)}$$
To calculate $\VAR_{2 \mid 1}(\hat{Y}_{greg})$ we note that
$$\hat{Y}_{greg}-\EX_{2 \mid 1}Y_{greg}=(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}_1^{(1)})^t(\hat{\pmb{\alpha}}_2-\hat{\pmb{\alpha}}_1)
+\hat{\bar{\pmb{Z}}}_1^t(\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1) \approx \hat{\bar{\pmb{Z}}}_1^t(\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1) $$
because by the law of large numbers the first term is of order $O(n_2^{-1})$ and the second of order $O(n_2^{-\frac{1}{2}})$. Using the Taylor expansion given in \cite{mandallaz} (at point $(\pmb{A}_1,\;\hat{\pmb{\beta}}_1)$ instead of $(\pmb{A},\;\pmb{\beta}))$ we get
$$\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1\approx \pmb{A}_1^{-1}\big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\big)$$
\noindent By the properties of simple random sampling without replacement within $s_1$ and the approximations $\frac{n_2-1}{n_1-1} \approx\frac{n_2}{n_1}$, $\sum_{x\in{s_1}}R(x)\pmb{Z}(x)=0$ (orthogonality relationship), we obtain after some algebra
$$\EX_1\VAR_{2 \mid 1}(\hat{Y}_{greg})=(1-\frac{n_2}{n_1})\bar{\pmb{Z}}^t\pmb{\Sigma}_{\hat{\pmb{\beta}}_2}\bar{\pmb{Z}}$$
\noindent To get asymptotic estimates of the covariances matrices one can replace $\pmb{A}$ by $\pmb{A}_1$ or $\pmb{A}_2$, and likewise $\pmb{A}^{(1)}$ by $\pmb{A}^{(1)}_1$ or $\pmb{A}^{(1)}_2$. To ensure the important calibration properties [\ref{gweight2}] we will use
\begin{equation}\label{estvarygregnew}
\hat{\VAR}(\hat{Y}_{greg})=\frac{n_2}{n_1}\bar{\pmb{Z}}^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}
\bar{\pmb{Z}}^{(1)}+(1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}_1^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_2}
\hat{\bar{\pmb{Z}}}_1
\end{equation}
with
\begin{equation}\label{robustvar1}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_2}=\pmb{A}_2^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}^2(x)\pmb{Z}(x)\pmb{Z}^t(x)\Big)\pmb{A}_2^{-1}
\end{equation}
and
\begin{equation}\label{robustvar2}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}=(\pmb{A}^{(1)}_1)^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}_1^2(x)\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)\Big)(\pmb{A}^{(1)}_1)^{-1}
\end{equation}
are the estimated \textbf{design-based covariance matrices} of the regression coefficients under the large and reduced models, which have also been discussed in a totally different context, i.e.  model-dependent least squares theory under non-standard conditions, by \cite{huber1} and \cite{gregoire2}, they are sometimes called \textbf{ robust covariance matrices}. It is straightforward to see that [\ref{estvarygregnew}] and [\ref{estavar2}] are equal.\\
 To get further insight into $\hat{Y}_{greg}$ we note that one can write
 \begin{equation}\label{partition1}
 \pmb{A}_2=
\left[ \begin {array}{ll}
\pmb{A}^{(1)}_2 & \pmb{A}_2^{(12)} \\
 \pmb{A}_2^{(12)t} & \pmb{A}^{(2)}_2 \\ \end {array} \right]
\end{equation}
with $\pmb{A}^{(k)}_2=\frac{1}{n_2}\sum_{x\in{s}_2}\pmb{Z}^{(k)}(x)\pmb{Z}^{(k)t}(x),\;k=1,2$
and $\pmb{A}_2^{(12)}=\frac{1}{n_2}\sum_{x\in{s}_2}\pmb{Z}^{(1)}(x)\pmb{Z}^{(2)t}(x)$. Developing the normal equations accordingly one obtains after some algebra the well-known relation
 \begin{equation}\label{lsreduced1}
 \hat{\pmb{\beta}}^{(1)}_2=\hat{\pmb{\alpha}}_2-(\pmb{A}^{(1)}_2)^{-1}
 \pmb{A}_2^{(12)}\hat{\pmb{\beta}}^{(2)}_2
 \end{equation}
 where $\hat{\pmb{\beta}}^t_2=(\hat{\pmb{\beta}}_2^{(1)t},\hat{\pmb{\beta}}_2^{(2)t})$.
 Substituting in [\ref{ygreg2}] we obtain
 $$\hat{Y}_{greg}=\bar{\pmb{Z}}^{(1)t}\hat{\pmb{\beta}}_2^{(1)}+
 \hat{\bar{\pmb{Z}}}^{(2)t}_1\hat{\pmb{\beta}}^{(2)}_2+\delta$$
 where $\delta=(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)})^t
 (\pmb{A}_2^{(1)})^{-1}\pmb{A}_2^{(1,2)}\hat{\pmb{\beta}}^{(2)}_2$, which is of order
 $O(n_1^{-\frac{1}{2}})$ in design-probability. Hence, we have the asymptotic equivalence
 \begin{equation}\label{equivygreg1}
\hat{Y}_{greg}\doteq \bar{\pmb{Z}}^{(1)t}\hat{\pmb{\beta}}_2^{(1)}+\hat{\bar{\pmb{Z}}}^{(2)t}_1\hat{\pmb{\beta}}^{(2)}_2
\end{equation}
which is intuitively very appealing: the exhaustive component $\pmb{Z}^{(1)}(x)$ occurs with its known true mean and the non-exhaustive component $\pmb{Z}^{(2)}(x)$ with its estimated mean from the large sample, as compared with the classical two-phase estimator
\begin{equation}\label{yregbeta}
\hat{Y}_{reg}= \hat{\bar{\pmb{Z}}}^t\hat{\pmb{\beta}}_2= \hat{\bar{\pmb{Z}}}_1^{(1)t}\hat{\pmb{\beta}}_2^{(1)}+
\hat{\bar{\pmb{Z}}}^{(2)t}_1\hat{\pmb{\beta}}^{(2)}_2
\end{equation}
One could also consider
\begin{equation}\label{ygregmod}
\hat{Y}_{gregmod}=
\bar{\pmb{Z}}^{(1)t}\hat{\pmb{\beta}}_2^{(1)}+\hat{\bar{\pmb{Z}}}^{(2)t}_1\hat{\pmb{\beta}}^{(2)}_2
\end{equation}
\noindent as a further estimator in its own right. If $\pmb{A}_2^{(1,2)}=\pmb{0}$, i.e. if the exhaustive and non-exhaustive components are orthogonal ("independent") then we have exactly $\hat{Y}_{gregmod}=\hat{Y}_{greg}$. We do not advocate the use of $\hat{Y}_{gregmod}$ when $\pmb{Z}^{(1)}(x)$ and $\pmb{Z}^{(2)}(x)$ are non-orthogonal because, as we found out by simulations, $\hat{Y}_{gregmod}$ can have a larger variance than $\hat{Y}_{reg}$ with moderate sample sizes. Beside, the resulting formulae for the asymptotic covariance are more cumbersome than [\ref{estvarygregnew}].  For these reasons we shall not consider $\hat{Y}_{gregmod}$ any more and we now proceed to adapt the previous results to the important small-area estimation problem.




\section{Generalized small-area estimators}

We consider a small area $G\subset F$ and we want to estimate
$$\bar{Y}_G=\frac{1}{\lambda(G)}\sum_{i=1}^NI_G(i)Y_i=\frac{1}{\lambda(G)}\int_G Y(x)dx$$
where $I_G(i)=1 $ if the $i$-th tree is in $G$, otherwise $I_G(i)=0$. Strictly speaking the last equality holds if boundary adjustments are performed in $G$, whereas they are in most instances only performed with respect to $F$. We shall need the following notation: $s_{1,G}=s_1\cap G$, $s_{2,G}=s_2 \cap G$, $n_{k,G}=\sum_{s\in{s_2}}I_G(x), \;k=1,2$ (restriction of the samples and sample sizes to $G$, note that the $n_{k,G}$ are random variables). The simplest solution is to restrict the generalized regression estimator [\ref{ygreg1}] to $G$, i.e. to consider \textbf{the generalized small-area estimator}
\begin{equation}\label{smallareaest1}
\hat{Y}_{G,greg}=\frac{1}{\lambda(G)}\int_G \hat{Y}_1(x)dx+
\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}(\hat{Y}(x)-\hat{Y}_1(x))+
\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}(Y(x)-\hat{Y}(x))
\end{equation}
 and treat the internal model as an external one to obtain the estimated conditional variance (i.e. given the $n_{k,G}$)
 \begin{equation}\label{extsmallareaestvariance1}
 \hat{\VAR}(\hat{Y}_{greg})=\frac{1}{n_{1,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s}_{2,G}}
 (\hat{R}_1(x)-\hat{\bar{R}}_{1,G})^2
+ (1-\frac{n_{2,G}}{n_{1,G}})
\frac{1}{n_{2,G}(n_{2,G}-1)}\sum_{x\in{s_2}}(\hat{R}(x)-\hat{\bar{R}}_G)^2
\end{equation}
where $\hat{\bar{R}}_{1,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R_1(x)$ and likewise for $\hat{\bar{R}}_G$. This variance estimate neglects the uncertainty of the regression coefficients but there is empirical evidence that this is acceptable in large samples (see \cite{mandallazreport1} for examples with $\hat{Y}_{reg}$).\\
We can rewrite $\hat{Y}_{G,greg}$ as
\begin{equation}\label{smallareaest2}
\hat{Y}_{G,greg}=(\bar{\pmb{Z}}_G^{(1)}-\hat{\bar{\pmb{Z}}}_G^{(1)})^t\hat{\pmb{\alpha}}_2
+\hat{\bar{\pmb{Z}}}^t_{1,G}\hat{\pmb{\beta}}_2+\frac{1}{n_{2,G}}\sum_{x\in{s}_{2,G}}\hat{R}(x)
\end{equation}
where we have set
$$\bar{\pmb{Z}}_G^{(1)}=\frac{1}{\lambda(G)}\int_G \pmb{Z}^{(1)}(x)dx,\;\;
\hat{\bar{\pmb{Z}}}_{1,G}=\frac{1}{n_{1,G}}\sum_{x\in{s}_{1,G}}\pmb{Z}(x)
=(\hat{\bar{\pmb{Z}}}^{(1)t}_{1,G},\hat{\bar{\pmb{Z}}}^{(2)t}_{1,G})^t$$
The essential difference with $\hat{Y}_{greg}=\hat{Y}_{F,greg}$ is that the mean residual term in [\ref{smallareaest2}] does no longer vanish in general, which makes the calculation of the variance very difficult. To bypass this difficulty we use the technique presented in \cite{mandallazreport1} by extending the model with the indicator variable $I_G(x)$ of the small area $G$, \textbf{which insures zero mean residual over $F$ and $G$}. We can include $I_G(x)$ in $\pmb{Z}^{(1)}(x)$ or $\pmb{Z}^{(2)}(x)$. It seems more natural to include it in the first component so that the zero mean residual properties will hold for both the reduced and the large model. Also, it is reasonable to assume that the perimeter and consequently the surface area of $G$ are known. We consider therefore the following extended models with auxiliary vectors:
$\pmb{\mathcal{Z}}^t(x)=(\pmb{\mathcal{Z}}^{(1)t}(x),\pmb{\mathcal{Z}}^{(2)t}(x))$,
where $\pmb{\mathcal{Z}}^{(1)t}(x)=(\pmb{Z}^{(1)t}(x), I_G^t(x))$ and $\pmb{\mathcal{Z}}^{(2)t}(x)=\pmb{Z}^{(2)t}(x)$. To have a uniform notation throughout we also change the notation for the second component, i.e. we will use $\pmb{\mathcal{Z}}^{(2)}(x)$ instead of $\pmb{Z}^{(2)}(x)$ in this section. We have therefore the following set up

\begin{enumerate}
 \item
 The large extended model $M$
 $$ Y(x)=\pmb{\mathcal{Z}}(x)^t\pmb{\theta}+ \mathcal{R(}x)=\pmb{\mathcal{Z}}^{(1)t}(x)\pmb{\theta}^{(1)}
 +\pmb{\mathcal{Z}}^{(2)t}(x)\pmb{\theta}^{(2)}+ \mathcal{R}(x)$$
 \noindent with $\pmb{\theta}^t=({\pmb{\theta}^{(1)t}}, {\pmb{\theta}^{(2)t}})$.
 The intercept term is contained in $\pmb{\mathcal{Z}}^{(1)}(x)$ or it is a linear combination of its components.\\
 The theoretical regression parameter $\pmb{\theta}$ minimizes
 $$\int_F (Y(x)-\pmb{\mathcal{Z}}^t(x)\pmb{\theta})^2dx$$
 It satisfies the normal equation
 $$\big(\int_F \pmb{\mathcal{Z}}(x)\pmb{\mathcal{Z}}^t(x)dx\big)\pmb{\theta}
 =\int_F(Y(x)\pmb{\mathcal{Z}}(x)dx)$$ and the orthogonality relationship
 $$\int_F \mathcal{R}(x)\pmb{\mathcal{Z}}(x)dx=\pmb{0}$$
 \noindent in particular the zero mean residual properties
 $$ \frac{1}{\lambda(F)}\int_F \mathcal{R}(x)dx=\frac{1}{\lambda(G)}\int_G \mathcal{R}(x)dx=0$$

 \item
 The reduced extended model $M_1$
 $$ Y(x)=\pmb{\mathcal{Z}}^{(1)t}(x)\pmb{\gamma} + \mathcal{R}_1(x)$$
 The theoretical regression parameter $\pmb{\gamma}$ minimizes
 $$\int_F (Y(x)-\pmb{\mathcal{Z}}^{(1)t}(x)\pmb{\gamma})^2dx$$
 It satisfies the normal equation
 $$\big(\int_F \pmb{\mathcal{Z}}^{(1)}(x)\pmb{\mathcal{Z}}^{(1)t}(x)dx\big)\pmb{\gamma}
 =\int_F Y(x)\pmb{\mathcal{Z}}^{(1)}(x)dx$$
 and the orthogonality relationship
 $$\int_G R_1(x)\pmb{\mathcal{Z}}^{(1)}(x)dx=\pmb{0}$$
 \noindent in particular the zero mean residual properties
 $$\frac{1}{\lambda(F)}\int_F \mathcal{R}_1(x)dx=\frac{1}{\lambda(G)}\int_G \mathcal{R}_1(x)dx=0$$
\end{enumerate}
We can obviously apply mutatis mutandis all the previous results. The estimated regression coefficients are
\begin{eqnarray}\label{extcoeff1}
\hat{\pmb{\gamma}}_2 &=& \Big(\frac{1}{n_2}\sum_{x\in{s}_2}\pmb{\mathcal{Z}}^{(1)}(x)\pmb{\mathcal{Z}}^{(1)t}(x)
\Big)^{-1}\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}^{(1)}(x)\nonumber\\
&:=&(\pmb{\mathcal{A}}^{(1)}_2)^{-1}
\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}^{(1)}(x)
\end{eqnarray}
and
\begin{equation}\label{extcoeff2}
\hat{\pmb{\theta}}_{2}=\Big(\frac{1}{n_2}\sum_{x\in{s}_2}\pmb{\mathcal{Z}}(x)\pmb{\mathcal{Z}}^t(x)
\Big)^{-1}\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}(x):
=\pmb{\mathcal{A}}^{-1}_2\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}(x)
\end{equation}
The estimated covariance matrices are according to [{\ref{robustvar1}] and [\ref{robustvar2}]
\begin{eqnarray}\label{robustvar3}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_2}&=&\pmb{\mathcal{A}}_2^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{\mathcal{R}}^2(x)\pmb{\mathcal{Z}}(x)
\pmb{\mathcal{Z}}^t(x)\Big)\pmb{\mathcal{A}}_2^{-1} \nonumber \\
\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_2}&=&(\pmb{\mathcal{A}}^{(1)}_1)^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{\mathcal{R}}_1^2(x)\pmb{\mathcal{Z}}(x)
\pmb{\mathcal{Z}}^t(x)\Big)(\pmb{\mathcal{A}}^{(1)}_1)^{-1} \nonumber \\
\end{eqnarray}
\noindent
where $\hat{\mathcal{R}}(x)=Y(x)-\pmb{\mathcal{Z}}^t(x)\hat{\pmb{\theta}}_2$ and
$\hat{\mathcal{R}}_1(x)=Y(x)-\pmb{\mathcal{Z}}^{(1)t}(x)\hat{\pmb{\gamma}}_2$ are the residuals. \\

\noindent Because the sum of the residuals over $s_{2,G}$ is now zero we can write the new small-area estimator $\hat{\tilde{Y}}_{G,greg}$ as in [\ref{ygreg2}]

\begin{equation}\label{extsmallareaest1}
\hat{\tilde{Y}}_{G,greg}
= (\bar{\pmb{\mathcal{Z}}}_G^{(1)}-\hat{\bar{\pmb{\mathcal{Z}}}}_G^{(1)})^t\hat{\pmb{\gamma}}_2 +
\hat{\bar{\pmb{\mathcal{Z}}}}_G^t\hat{\pmb{\theta}}_{2}
\end{equation}
where we have set
 $$\bar{\pmb{\mathcal{Z}}}_G^{(1)}=\frac{1}{\lambda(G)}\int_G \mathcal{Z}(x)dx,\; \hat{\bar{\pmb{\mathcal{Z}}}}_G^{(1)}=
 \frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\pmb{\mathcal{Z}}^{(1)}(x)$$
 To get an estimate of the design-based variance we use mutatis mutandis [\ref{estvarygregnew}]

 \begin{equation}\label{estvarygreg}
\hat{\VAR}(\hat{\tilde{Y}}_{greg})=\frac{n_2}{n_1}\bar{\pmb{\mathcal{Z}}}_G^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_2}
\bar{\pmb{\mathcal{Z}}}_G^{(1)}+(1-\frac{n_2}{n_1})\hat{\bar{\pmb{\mathcal{Z}}}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_2}
\hat{\bar{\pmb{\mathcal{Z}}}}_G
\end{equation}



\section{Generalization to cluster sampling}
We follow the description of cluster sampling as defined in \cite{mandallaz} (especially section 5.5) and \cite{mandallazreport1}. A cluster is identified by its origin $x$, uniformly distributed in $\tilde{F}\supset F$. The geometry of the cluster is given by $M$ vectors $e_1,\ldots e_M$ defining the random cluster $x_l=x+e_l$. $M(x)=\sum_{l=1}^MI_{F}(x_l)$ is the random number of points of the cluster falling into the forest area $F$. We define the local density at the cluster level by $Y_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)Y(x_l)}{M(x)}$, likewise we set $\pmb{Z}_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)\pmb{Z}(x_l)}{M(x)}$. The set $\tilde{F}$ above can be mathematically defined as the smallest set $\{x\in{\mathcal{R}}^2 \mid M(x) \ne 0 \}$. In the first phase we have $n_1$ clusters identified by $x\in{s_1}$ and in the second phase $n_2$ clusters with $x\in{s_2}$, obtained by simple random sampling from $s_1$.\\
We shall use the model-based approach, in which the regression coefficient $\pmb{\beta}_c$ at the cluster level, under the large model with $\pmb{Z}^t=(\pmb{Z}^{(1)t}(x), \pmb{Z}^{(2t)}(x))$,  minimizes
$$\int_F M(x)(Y_c(x)-\pmb{\beta}_c^{t}\pmb{Z}_c(x))^2dx$$
In the pure design-based approach the weights will be $M^2(x)$ but this leads to non-zero mean residual (thought close zero in practice), and the definitions of the regression estimator and of the normal equation are slightly different (see \cite{mandallaz}, section 5.5 for details). The choice of $M(x)$ rather than $M^2(x)$ as weights is suggested by the model-dependent approach. When $Y_c(x)$ is the mean of the
$M(x)$ observations, its variance can be expected to be inversely
proportional to $M(x)$.  This procedure  leads to the normal equation
$$\Big(\int_F M(x)\pmb{Z_c}(x)\pmb{Z_c}^t(x)dx\Big)\pmb{\beta}_c=\int_F M(x)Y_c(x)\pmb{Z_c}(x)dx$$
and to $\int_F M(x)R_c(x)=0$. An asymptotically design-unbiased estimate $\hat{\pmb{\beta}}_{c,2}$  for $\pmb{\beta}_c$ can be obtained  by taking a sample copy of the above equation, i.e.
 \begin{eqnarray}\label{estbetacluster1}
 \hat{\pmb{\beta}}_{c,2}&=&
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{Z}_c(x)\pmb{Z}_c^t(x)\Big)^{-1}
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \nonumber\\
 &:=&\pmb{A}^{-1}_{c,s_2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \end{eqnarray}
 The empirical residuals at the cluster level are
 $$\hat{R}_{c}(x)=Y_c(x)-\pmb{Z}^t_c(x)\hat{\pmb{\beta}}_{c,2}$$
 which satisfy the orthogonality relation
 $$\sum_{x\in{s_2}}M(x)\hat{R}_c(x)\pmb{Z}_c(x)=0$$
 and in particular the zero mean residual property
$$\frac{\sum_{x\in{s_2}}M(x)\hat{R}_c(x)}{\sum_{x\in{s_2}}M(x)}=0$$
With obvious notational changes we get the corresponding results under the reduced model with
$\pmb{Z}^{(1)}$ alone, where the regression coefficient $\pmb{\alpha}_c$ minimizes
$$\int_F M(x)(Y_c(x)-\pmb{\alpha}_c^{t}\pmb{Z}^{(1)}_c(x))^2dx$$
with estimate
\begin{eqnarray}\label{estalphacluster1}
 \hat{\pmb{\alpha}}_{c,2}&=&
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{Z}^{(1)}_c(x)\pmb{Z}_c^{(1)t}(x)\Big)^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \nonumber\\
 &:=&(\pmb{A}^{(1)}_{c,2})^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}^{(1)}_c(x)\Big)
 \end{eqnarray}
 The residuals for the reduced model are $\hat{R}_{1,c}(x)=Y_c(x)-\pmb{Z}^{(1)t}_c(x)\hat{\pmb{\alpha}}_{c,2}$ and enjoy the same properties as
 $\hat{R}_{c}(x)$.\\
 \noindent Using mutatis mutandis exactly the same arguments as in simple random sampling we get the asymptotic robust design-based estimated covariance matrices
\begin{eqnarray}\label{estcovmatrixcluster1}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,2}}&=&\pmb{A}^{-1}_{c,2}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}}M^2(x)\hat{R}^2_c(x)
\pmb{Z}_c(x)\pmb{Z}^t_c(x)\Big)\pmb{A}^{-1}_{c,2}\nonumber \\
\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_{c,2}}&=&(\pmb{A}^{(1)}_{c,1})^{-1}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}}M^2(x)\hat{R}_{1,c}^2(x)
\pmb{Z}_c(x)\pmb{Z}^t_c(x)\Big)(\pmb{A}^{(1)}_{c,1})^{-1}
\end{eqnarray}
where $\pmb{A}^{(1)}_{c,1}=\frac{1}{n_1}\sum_{x\in{s_1}}M(x)\pmb{Z}^{(1)}_c(x)\pmb{Z}_c^{(1)t}(x)$.\\

We define the \textbf{generalized regression estimator in cluster sampling} directly by adapting [\ref{ygreg2}]
\begin{equation}\label{ygregcluster}
\hat{Y}_{c,greg}= (\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_{c,1})^t\hat{\pmb{\alpha}}_{c,2} +
\hat{\bar{\pmb{Z}}}^t_{c,1}\hat{\pmb{\beta}}_{c,2}
\end{equation}
where the mean is now defined as
$$\hat{\bar{\pmb{Z}}}^t_{c,1}=\frac{\sum_{x\in{s_1}}M(x)\pmb{Z}_c(x)}{\sum_{x\in{s_1}}M(x)}$$
and similarly for $\hat{\bar{\pmb{Z}}}^{(1)}_{c,1}$.\\
With the same technique as in simple random sampling we obtain the estimated design-based variance
\begin{equation}\label{estvarclusterygereg}
\hat{\VAR}(\hat{Y}_{c,greg})=\frac{n_2}{n_1}\bar{\pmb{Z}}^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_{c,2}}
\bar{\pmb{Z}}^{(1)}+(1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}_{c,1}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,2}}
\hat{\bar{\pmb{Z}}}_{c,1}
\end{equation}
For small-area estimation we shall work with the extended model
$$\pmb{\mathcal{Z}}^t(x)=(\pmb{\mathcal{Z}}^{(1)t}(x), \pmb{\mathcal{Z}}^{(2)t}(x))$$
 with
$\pmb{\mathcal{Z}}^{(1)t}(x)=(\pmb{Z}^{(1)t}(x), I_G^t(x))$, $\pmb{\mathcal{Z}}^{(2)t}(x)=\pmb{Z}^{(2)t}(x)$ and $I_G(x)$ is the indicator of the small area $G$. At the cluster level we have $\pmb{\mathcal{Z}}_c^{(1)t}(x)=(\pmb{Z}_c^{(1)t}(x),I^t_{c,G}(x))$ where $I_{c,G}(x)=\frac{\sum_{l=1}^M I_G(x_l)}{M(x)}$. In extensive inventories we can reasonably assume that all the points of a cluster lying in the forest area $F$ will belong to the same small area $G$ so that in fact $I_{c,G}(x) \equiv 1$ for all $x\in{\tilde{G}}=\{x \mid \sum_{l=1}^M I_G(x_l)>0\}$. This ensures again that we will have zero mean residual over $F$ and $G$.  For the regression estimates in the extended model we have

\begin{equation}\label{regbettaclusterext}
\hat{\pmb{\gamma}}_{c,2}=(\pmb{\mathcal{A}}_{c,2}^{(1)})^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}^{(1)}_c(x)Y(x)\Big)
\end{equation}
with $\pmb{\mathcal{A}}_{c,2}^{(1)}=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}^{(1)}_c(x)\pmb{\mathcal{Z}}^{(1)t}_c(x)$. Likewise we get
\begin{equation}\label{regbettaclusterext}
\hat{\pmb{\theta}}_{c,2}=\pmb{\mathcal{A}}^{-1}_{c,2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}_c(x)Y(x)\Big)
\end{equation}
with $\pmb{\mathcal{A}}_{c,2}=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}^t_c(x)$.\\
We define as in [\ref{extsmallareaest1}] the generalized small-area estimator by

\begin{equation}\label{extclustersmallareaest1}
\hat{\tilde{Y}}_{c,G,greg}
= (\bar{\pmb{\mathcal{Z}}}_G^{(1)}-\hat{\bar{\pmb{\mathcal{Z}}}}_{c,G}^{(1)})^t\hat{\pmb{\gamma}}_{c,2} +
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,G}^t\hat{\pmb{\theta}}_{c,2}
\end{equation}
where we have set
 $$\bar{\pmb{\mathcal{Z}}}_G^{(1)}=\frac{1}{\lambda(G)}\int_G \mathcal{Z}(x)dx,\; \hat{\bar{\pmb{\mathcal{Z}}}}_{c,G}^{(1)}=
 \frac{\sum_{x\in{s_{1,G}}}M(x)\pmb{\mathcal{Z}}_c^{(1)}(x)}{\sum_{x\in{s_{1,G}}}M(x)},\;
 \hat{\bar{\pmb{\mathcal{Z}}}}_{c,G}=
 \frac{\sum_{x\in{s_{1,G}}}M(x)\pmb{\mathcal{Z}}_c(x)}{\sum_{x\in{s_{1,G}}}M(x)}$$

The estimated design-based covariance matrix are now
\begin{eqnarray}\label{gammathetaclusterextestvar}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_{c,2}}&=&
(\pmb{\mathcal{A}}^{(1)}_{c,1})^{-1}\Big(\frac{1}{n^2_2}\sum_{\in{s_2}}M^2(x)\hat{\mathcal{R}}_{1,c}^2(x)
\pmb{\mathcal{Z}}^{(1)}_c(x)\pmb{\mathcal{Z}}^{(1)t}_c(x)\Big)(\pmb{\mathcal{A}}^{(1)}_{c,1})^{-1}\nonumber \\
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,2}}&=&
\pmb{\mathcal{A}}^{-1}_{c,2}\Big(\frac{1}{n^2_2}\sum_{\in{s_2}}M^2(x)\hat{\mathcal{R}}_c^2(x)
\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}^t_c(x)\Big)\pmb{\mathcal{A}}^{-1}_{c,2}
\end{eqnarray}
with the residuals in the extended models $\hat{\mathcal{R}}_{1,c}(x)=Y_c(x)-\pmb{\mathcal{Z}}^{(1)t}_c(x)\hat{\pmb{\gamma}}_{c,2}$ and
$\hat{\mathcal{R}}_{c}(x)=Y_c(x)-\pmb{\mathcal{Z}}^{t}_c(x)\hat{\pmb{\theta}}_{c,2}$ and
$\pmb{\mathcal{A}}_{c,1}=\frac{1}{n_1}\sum_{x\in{s_1}}M(x)\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}^t_c(x)$.\\
We obtain as in [\ref{estvarygregnew}] the estimated design-based variance

\begin{equation}\label{extendedclustersmallestvariance}
\hat{\VAR}(\hat{Y}_{c,G,greg}) =\frac{n_2}{n_1}\bar{\pmb{\mathcal{Z}}}_G^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_{c,2}}
\bar{\pmb{\mathcal{Z}}}_G^{(1)}+(1-\frac{n_2}{n_1})\hat{\bar{\pmb{\mathcal{Z}}}}_{c,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,2}}
\hat{\bar{\pmb{\mathcal{Z}}}}_{G,1}
\end{equation}
As shown in \cite{mandallazreport1} it is straightforward to consider simultaneously several
small areas by extending the model with as many small area indicator variables.\\

\section{Generalization to two-stage sampling}
In many applications costs to measure the response variable $Y_i$ are high. For instance,
a good determination of the volume may require that one records
$DBH$, as well as the diameter at $7m$ above ground and total height
in order to utilize a three-way volume function. However, one could rely
on a coarser, but cheaper, approximation of the volume based only on
$DBH$. Nonetheless, it may be most sensible to assess those three
parameters only on a sub-sample of trees. We now briefly formalize this
simple idea, which is used in the Swiss National Forest Inventory. The reader is referred to (\cite{mandallaz}, section 4.4, 4.5, 5.4 and 9.5) for details. For each point $x\in{s_2}$ trees are drawn with
probabilities $\pi_i$. The set of selected trees is denoted by
$s_{2}(x)$. From each of the selected trees $i\in{s_{2}(x)}$ one
gets an approximation $Y_i^*$ of the exact value $Y_i$. From the
finite set $s_{2}(x)$ one draws a sub-sample
$s_{3}(x)\subset{s_{2}(x)}$ of trees by Poisson sampling. For each tree $i\in{s_{3}(x)}$
one then measures the exact variable $Y_i$. Let us now define the
second stage indicator variable
\begin{equation}
 J_i(x)=\begin{cases}&1 \text{ if $i\in s_{3}(x)$}\\
                      &0 \text{ if $i\not\in s_{3}(x)$}
         \end{cases}
\end{equation}

 \par To construct a good point estimate, we must have
 the residual $R_i=Y_i-Y_i^*$ which is known only for trees
 $i\in{s_{3}(x)}$. The generalized local density $Y^*(x)$ is defined
 according to    \index{Generalized local density}
 \begin{eqnarray}\label{gdens}
 Y^*(x)&=&\frac{1}{\LF}\left( \sum_{i=1}^N \frac{I_{i}(x)Y_i^*}{\pi_i} +
 \sum_{i=1}^N \frac{I_{i}(x)J_{i}(x)R_i}{\pi_{i}p_i}\right)\nonumber \\
 &=&\frac{1}{\LF}\left( \sum_{i\in{s_{2}(x)}} \frac{Y_i^*}{\pi_i}
 +\sum_{i\in{s_{3}(x)}} \frac{R_i}{\pi_{i}p_i}\right)
 \end{eqnarray}
 where the $p_i$ are the conditional inclusion probabilities for the the second stage sampling, i.e. $p_i=\PR(J_i(x)=1 \mid I_i(x)=1)$.
 It follows from general principles presented in (\cite{mandallaz}, sections 4.4 and 4.5) that one can use all the previous results by replacing everywhere the exact local densities $Y(x)$, or $Y(x_l)$ in cluster sampling, by the corresponding generalized local densities $Y^*(x)$ or $Y^*(x_l)$. The second-stage variance is automatically taken into account.

\section{Example: Post-stratification}\label{examples}

We consider the forested area $F$ embedded in a subset $\tilde{F}$, which in many national inventories is simply the whole country or part of it. Hence, $\tilde{F}\setminus F=F_0$ is the non-forested area. The forested area itself is partitioned in $L$ strata $F_k$, i.e. $F=\cup_{k=1}^L F_k$. We assume that the surface areas $\lambda(\tilde{F})$ and $\lambda(F_0)$ are known exactly, \textbf{whereas the surface areas of the strata $\lambda(F_k)$ for $k \ge 1$ are not}. The first-phase sampling consists of $n_1$ points, uniformly distributed in $\tilde{F}$ (set $s_1$), out of which $n_2$ points (set $s_2$) are selected by equal probability sampling without replacement (in practice one uses grids and sub-grids). The local density $Y(x)$ is set to zero whenever $x\in{F}_0$ (even if trees in $F$ could be selected from $x\in{F}_0$), and is calculated according to [\ref{truelocaldensity}] if $x\in{F}$ (i.e. with boundary adjustments if necessary). This ensures that $\int_{\tilde{F}}Y(x)dx=\int_{F}Y(x)dx=\sum_{i=1}^NY_i$. We define the (random) sample sizes $n_{1k}=\sum_{x\in{s}_1}I_{F_k}(x)$ ,
$n_{2k}=\sum_{x\in{s}_2}I_{F_k}(x)$ for $k\ge 0$ and $n_{1F}=\sum_{k=1}^Ln_{1k}$, $n_{2F}=\sum_{k=1}^L n_{2k}$.\\
For the reduced model we define the one dimensional explanatory vector according to $\pmb{Z}^{(1)}(x)=I_F(x)$. Trivial calculations lead to
$\hat{\pmb{\alpha}}=\hat{\bar{Y}}_F^t=\frac{1}{n_{2,F}}\sum_{x\in{s}_2}Y(x)$ (i.e. the empirical mean of $Y(x)$ in the forested area $F$). We get $\bar{\pmb{Z}}^{(1)}=p_F=\frac{\lambda(F)}{\lambda(\tilde{F})}$. For the g-weight one obtains
$g^{(1)}(x)=p_F\frac{n_1}{n_{1F}}I_F(x)$. For the predictions one obtains $\hat{Y}_1(x)=\hat{\bar{Y}}_FI_F(x)$. \\
For the large model, we define $\pmb{Z}^{(2)}(x)$ as the $L$ dimensional vector defined by the indicator variables of all strata in the forested area, i.e. $\pmb{Z}^{(2)}(x)=(Z_1(x),Z_2(x), \ldots Z_{L}(x))^t$, $Z_k(x)=I_{F_k}(x)$ for $k=1,2 \ldots L$. Note that all the components of $\pmb{Z}(x)$ are zero for $x\not\in{F}$. The $(L+1,L+1)$ matrix $\pmb{A}=\sum_{x\in{s}_1}\pmb{Z}(x)\pmb{Z}(x)^t$ is almost diagonal
\begin{equation*}
\pmb{A}= \left[ \begin {array}{cccccc}
{\it n_{2F}} &{\it n_{21}}&{\it n_{22}}& {\it n_{23} }&...& {\it n_{2L}}\\
\noalign{\medskip}{\it n_{21}}&{\it n_{21}}& $0$ & $0$  &...& $0$ \\
\noalign{\medskip}{\it n_{22}}& $0$ & {\it n_{22} } & $0$  & ...& $0$ \\
\noalign{\medskip} ...  & & ... & ... & ...& $0$ \\
\noalign{\medskip} {\it n_{2k} } & ... & $0$ & {\it n_{2k} }& ... & $0$ \\
\noalign{\medskip} ...  & ... & ... & ...& $0$ \\
\noalign{\medskip}{\it n_{2L}}  & $0$ & $0$ &... & $0$ & {\it n_{2L}} \end {array} \right]
\end{equation*}
but it is singular because the first column is the sum of the last $L$ columns. We set
$\pmb{\beta}=(\beta_0,\beta_1,\ldots \beta_L)^t$. A particular solution of the singular normal equation is easily found to be $\hat{\beta}_0=\hat{\bar{Y}}_F$ and $\hat{\beta}_k=\hat{\bar{Y}}_k-\hat{\bar{Y}}_F$
where $\hat{\bar{Y}}_k=\frac{1}{n_{2,k}}\sum_{x\in{F_k}}Y(x)$ (this corresponds to the solution of the standard one-way ANOVA). This leads to the intuitively obvious predictions $\hat{Y}(x)=0$ for $x\in{F}_0$ and $\hat{Y}(x)=\hat{\bar{Y}}_k$ for $x\in{F}_k$. Note also that all empirical residuals are zero outside $F$. \\
The standard regression estimate (with respect to $\tilde{F}$) based on the large model alone is easily found to be [\ref{gweight3}]
$$\hat{Y}_{reg}=\hat{Y}_{post,standard}=\sum_{k=1}^L \frac{n_{1k}}{n_1}\hat{\bar{Y}}_k=\frac{1}{n_2}\sum_{x\in{s}_2}g_2(x)Y(x)$$
Since this algebraic identity is true for an arbitrary density $Y(x)$ we have
\begin{equation}\label{gweightpost}
g_2(x)=\frac{n_2}{n_1}\frac{n_{1k}}{n_{2k}} \quad \text{for} \quad x\in{F}_k, \;k=0,1,2,\ldots
\end{equation}
Simple algebra yields then for [\ref{ygreg2}] the result
\begin{equation}\label{post1}
\hat{Y}_{greg}=\hat{Y}_{post,new}=(p_F-\hat{p}_F)\hat{\bar{Y}}_F+\sum_{k=1}^L\hat{p}_k\hat{\bar{Y}}_k
\end{equation}
where we have set $p_F=\frac{\lambda(F)}{\lambda(\tilde{F})}$, $\hat{p}_F=\frac{n_{1F}}{n_1}$ and
$\hat{p}_k=\frac{n_{1k}}{n_1}$. For $n_1,n_2 \rightarrow \infty$ this is obviously a consistent estimate of the density with respect to $\tilde{F}$, i.e. of $\bar{Y}_{\tilde{F}}=\frac{1}{\lambda(\tilde{F})}\sum_{i=1}^N Y_i$.\\
The standard post-stratified estimate, i.e. with respect to $\tilde{F}$ but without knowledge of the exact surface area $\lambda(F)$ and $\lambda(\tilde{F})$, is given by the second term only (see \cite{mandallaz}, section 5.2.1):
\begin{equation}\label{post2}
\hat{Y}_{post,standard}=\sum_{k=1}^L\hat{p}_k\hat{\bar{Y}}_k
\end{equation}
The new estimate [\ref{post1}] is intuitively appealing: if the number of points falling in the forested area $F$ is above or below its expected value, then the classical estimate is corrected accordingly.
Tedious but simple calculations and the approximations $ n_{2k}-1 \approx n_{2k}$ lead to the following estimated variances
\begin{equation}\label{estpostvar1}
\hat{\VAR}(\hat{Y}_{post,new})=\frac{1}{n_1n_2}\frac{p_F^2}{\hat{p}_F^2}(n_{2F}-1)\hat{\sigma}^2_F+
(1-\frac{n_2}{n_1})\sum_{k=1}^L\hat{p}^2_k\frac{\hat{\sigma}_k^2}{n_{2k}}
\end{equation}
where $p_k=\frac{\lambda(F_k)}{\lambda(\tilde{F})},\; \hat{p}_k=\frac{n_{1k}}{n_1}$,
$\hat{\sigma}_F^2=\frac{1}{n_{2F}-1}\sum_{x\in{s_2}\cap F} (Y(x)-\hat{\bar{Y}}_F)^2$.\\
The main advantage of the g-weight variance estimate is that the strata weights $p_k$ are estimated from the large sample and the contribution of the strata to the variance is inversely proportional to the sample sizes $n_{2k}$. Result [\ref{estpostvar1}] is similar to previous findings given in (\cite{mandallaz}, pp 84 and 107-108).\\
The asymptotic variance ($n_1,\; n_2 \rightarrow \infty$ ) is then
\begin{equation}\label{asymvarpostnew}
\hat{\VAR}(\hat{Y}_{post,new})=\frac{1}{n_1}p_F\sigma^2_F +
(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{k=1}^Lp_k\sigma^2_k
\end{equation}
where $\sigma^2_F=\frac{1}{\lambda(F)}\int_F (Y(x)-\bar{Y}_F)^2dx$ is the overall variance within $F$ and the $\sigma^2_k=\frac{1}{\lambda(F_k)}\int_{F_k} (Y(x)-\bar{Y}_k)^2dx$ (with the strata means $\bar{Y}_k=\frac{1}{\lambda(F_k)}$)  are the variances within strata. \\
To calculate the variance of $\hat{Y}_{post,standard}$ we use the external model assumption and equation [5.5.1] in \cite{mandallaz} to get
$$\VAR(\hat{Y}_{post,standard})=\frac{1}{n_1}\VAR_{x\in{\tilde{F}}}(Y(x))+
(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR_{x\in{\tilde{F}}}(R(x))$$
\noindent where $R(x)=0$ for $x\not\in{F}$ and $R(x)=Y(x)-\bar{Y}_k$ for $x\in{F_k}$.\\
Noting that $\bar{Y}_{\tilde{F}}=\bar{Y}=p_F\bar{Y}_F,\;\bar{R}_{\tilde{F}}=\bar{R}_F=0$ and writing $(Y(x)-\bar{Y})=(Y(x)-\bar{Y}_F+\bar{Y}_F-p_F\bar{Y}_F)$ we obtain after some algebra the asymptotic variance of the standard post-stratified estimate with respect to $\tilde{F}$ as:
\begin{equation}\label{postvar2}
\VAR(\hat{Y}_{post,standard})=\frac{1}{n_1}(p_F\sigma^2_F + p_F(1-p_F)\bar{Y}^2_F) +(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{k=1}^Lp_k\sigma_k^2 > \VAR(\hat{Y}_{post,new})
\end{equation}
Hence, as expected, using the exhaustive information $I_F(x)$ reduces the variance.\\
In practice forest inventories occasionally present totals over the entire region $\tilde{F}$ based on post-stratification with the non forested area $F_0=\tilde{F}\setminus F$ viewed as a further ordinary stratum, that is $\lambda(\tilde{F})$ is known, but neither $\lambda(F_0)$, not the $\lambda(F_k)$ are known. We have therefore to consider the following estimates of totals and their asymptotic variances:
\begin{eqnarray}\label{totals}
\hat{T}_{post,standard} &=& \lambda(\tilde{F})\hat{Y}_{post,standard} \nonumber \\
\VAR(\hat{T}_{post,standard}) &=& \lambda^2(\tilde{F})\Big(\frac{1}{n_1}(p_F\sigma^2_F + p_F(1-p_F)\bar{Y}^2_F) +(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{k=1}^Lp_k\sigma_k^2\Big) \nonumber \\
\hat{T}_{post,new} &=& \lambda(\tilde{F})\hat{Y}_{post,new} \nonumber \\
\VAR(\hat{T}_{post,new}) &=& \lambda^2(\tilde{F})\Big(\frac{1}{n_1}p_F\sigma^2_F  +(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{k=1}^Lp_k\sigma_k^2\Big)
\end{eqnarray}
 In the present framework, because $F$ is assumed to be known, we can also consider the conditional estimate based only on the $n_{1F}$ and $n_{2F}$ points falling into $F$, that is
 \begin{eqnarray}\label{postcond}
 \hat{Y}_{post,cond}&=&\sum_{k=L}^L\hat{\tilde{p}}_k\hat{\bar{Y}}_k \nonumber \\
 \VAR(\hat{Y}_{post,cond}) &=& \frac{1}{n_{1F}}\sigma^2_F+(1-\frac{n_{2F}}{n_{1F}})\frac{1}{n_{2F}}\sum_{k=1}^L\tilde{p}_k\sigma_k^2 \nonumber \\
 \hat{T}_{post,cond}&=& \lambda(F) \hat{Y}_{post,cond} \nonumber \\
 \VAR(\hat{T}_{post,cond}) &=& \lambda^2(F)\VAR(\hat{Y}_{post,cond})
 \end{eqnarray}
 where we have set $\hat{\tilde{p}}_k=\frac{n_{1k}}{n_{1F}}$ and $\tilde{p}_k=\frac{\lambda(F_k)}{\lambda(F)}$. Using the facts that $\tilde{p}_k=\frac{p_k}{p_F}$, $\lambda(F)=p_F\lambda(\tilde{F})$, $\EX(n_{1F})=n_1p_F$, $\EX(n_{2F})=p_F n_2)$, we see that in large samples $\hat{Y}_{post,cond}$ and
 $\hat{T}_{post,new}$ are equivalent and better than $\VAR(\hat{T}_{post,standard})$, in the sense that
 $$\VAR(\hat{T}_{post,new}) \approx  \VAR(\hat{T}_{post,cond}) < \VAR(\hat{T}_{post,standard}) $$
 This fact is surprisingly not widely known and many national inventories are using estimators related somehow to $\hat{T}_{post,standard}$. Usually the decision for $x\notin{F}$ is easy (implying necessarily $Y(x)=0$ ), whereas the delineation of the forested area can be more problematic (i.e. to decide for $x\in{F}$, even if $Y(x)=0$ is still possible). With the technological advances in remote sensing it is only a matter of time until we can assume that $F$ and $\lambda(F)$ are known with the same accuracy as $\tilde{F}$ and $\lambda(\tilde{F})$. From a pragmatic point of view and as far as the estimation of total is concerned a coarse delineation by polygons defining a set $F$ containing the "true" forest should suffice (so that $p_F$ is close to $1$ and the extra variance term $p_F(1-p_f)\bar{Y}_F^2$ is small). \\
 We emphasize again the fact that the surface areas of the strata within the forest area need not be known. In any case, the conditional estimator $\hat{Y}_{post,cond}$ and the new estimator
 $\hat{T}_{post,new}$ should be preferred  to the occasionally used practice with $\hat{T}_{post,standard}$, particularly if $\tilde{F}$ is much larger than $F$.
\newpage


\bibliography{biblio1}

\end{document}

