\documentclass[a4paper,12pt,leqno, titlepage]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{a4}
\usepackage{graphicx}
\usepackage{flafter}
\usepackage{bm}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{natbib}
\bibliographystyle{mystyle2}
\newcommand{\LF}{\ensuremath{\lambda(F)}}
\newcommand{\LFC}{\ensuremath{\lambda^2(F)}}
\newcommand{\EX}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\VAR}{\mathbb{V}}
\newcommand{\COV}{\mathbb{COV}}
\newcommand{\MAV}{\mathbb{MAV}}
\newcommand{\MRAV}{\mathbb{MRAV}}
\newcommand{\POP}{\mathcal{P}}
\newcommand{\SAMP}{\mathcal{S}}
\newcommand{\RE}{\mathbb{RE}}
\newcommand{\PLAN}{\Re^2}
\newcommand{\SUR}{\mathbb{S}}
\newcommand{\ING}{\mathbb{I}}
\newcommand{\DEP}{\mathbb{D}}
\newcommand{\R}{\mathbb{R}}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{3mm}
\setlength{\headsep}{1cm}
\setlength{\topskip}{0cm}
\setlength{\textwidth}{16cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\makeatletter
\def\tagform@#1{\maketag@@@{[\ignorespaces#1\unskip\@@italiccorr]}}
\makeatother
\begin{document}
\doublespacing
\pagestyle{plain}
\pagenumbering{arabic}

\title{Regression estimators in forest inventories with three-phase sampling and two multivariate components of auxiliary information}
\author{Daniel Mandallaz,\thanks{Tel. ++41(0)44 6323186 e-mail
    daniel.mandallaz@env.ethz.ch, CHN K74.1, CH 8092 Zurich, Switzerland} \\ETH Zurich\\ Department of Environmental Systems Science\\ Chair of Land Use Engineering}
\date{}

\maketitle
\newpage

\begin{abstract}
We consider three-phase sampling schemes where one multivariate component of the auxiliary information is known in the very large sample of the so-called null phase ("almost exhaustive") and the second multivariate component is available only in the large sample of the first phase, whereas the second phase provides the terrestrial inventory data. We extend the generalized regression estimator with partially exhaustive information to this three-phase set-up, for both global and local estimation and derive its asymptotic design-based variance. We also consider further extensions to cluster and two-stage sampling.
\end{abstract}
\clearpage


\section{Introduction}\label{introduction}
\pagenumbering{arabic} \setcounter{page}{1}

The motivation for this work is due to the increasing need of using national or regional inventories for local estimation in order to meet tighter budgetary constraints, which is only feasible under extensive use of auxiliary information, provided e.g. by remote sensing (aerial photographs or LiDAr data). The small-area estimation problem is in this context of the utmost importance. This paper adapts the so-called generalized regression estimator proposed in \cite{mandallaz4} to the case where the first component of the auxiliary information is no longer exhaustive, but is provided by a very large sample, the null-phase. The second component is available on a sub-sample of the null-phase, the first-phase, and the terrestrial inventory is performed on a sub-sample of the first phase, the second-phase. This set-up is particularly useful for national or regional inventories for two reasons: (1) the first component may not be available exhaustively (2) even if it were it may be computationally prohibitive for some of its variables, particularly those based on sophisticated algorithms requiring single tree identification (as in \cite{mandallaz4}). The methodology and terminology rests upon the design-based Monte-Carlo approach to sampling theory for forest theory. The reader unfamiliar with this topic should first consult \cite{mandallaz3} for a short bibliographical review and a thorough but terse analysis of the regression estimator in a new setting, particularly useful in the small-area estimation context. The book \cite{mandallaz}, chapters 4 and 5, is recommended for a first perusal, as well as the on-line technical reports \cite{mandallazreport1,mandallazreport2} for the mathematical details. \\


\section{Methodology}
The \textbf{null phase} draws a very large sample $s_0$ of $n_0$ points
$x_{i}\in{s_0}$ ($i=1,2\ldots n_0$) that are independently and uniformly distributed
within the forest area $F$. At each of those points auxiliary
information is collected, very often coding information of qualitative nature
(e.g. following the  interpretation of aerial photographs) or quantitative (e.g. timber volume estimates  based on LiDAR measurements). We shall assume that the auxiliary information at point $x$
is described by the column vector $\pmb{Z}^{(1)}(x)\in{\Re^p}$. We shall call this component \textbf{pseudo-exhaustive} by analogy with the terminology used in \cite{mandallaz3}, which deals with the case $n_0=\infty$ (i.e. $\pmb{Z}^{(1)}(x)$ is in this case the \textbf{exhaustive} component). The \textbf{first phase} draws a large sample $s_1 \subset s_0$ of $n_1 <<n_0$ points by simple random sampling in $s_0$. Note that the points $x\in{s_1}$ are also uniformly independently distributed in $F$. For each point in the first phase a further component $\pmb{Z}^{(2)}(x)\in{\Re^q}$ of the auxiliary information is available and hence also the vector $\pmb{Z}^t(x)=(\pmb{Z}^{(1)t}(x),\pmb{Z}^{(2)t}(x))\in{\Re^{p+q}}$ (the upper index $t$ denotes the transposition operator). The \textbf{second phase} draws a small sample $s_2\subset{s_1}$ of
$n_2$ points from $s_1$ by simple random sampling and consists of the terrestrial inventory. Note that we have used the terms null, first and second phases instead of first, second and third phases simply to ensure compatibility of the terminology used in previous work, in which the null-phase was exhaustive.\\
The reason for introducing the null phase sample $s_0$ is that the component $\pmb{Z}^{(1)}(x)$ can be very computing intensive to calculate exhaustively (see \cite{mandallaz3} for a case study with LiDAR data).\\
In the forested area $F$ we consider a well defined population $ \mathcal{P}$ of $N$ trees with response variable
 $Y_i,\;i=1,2 \ldots$, e.g. the timber volume.  The objective is to estimate the spatial mean  $\bar{Y}=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i$, where $\lambda(F)$ denotes the surface area of $F$ (usually in ha). For each point $x\in{s_2}$ trees are drawn from the population $\mathcal{P}$ with probabilities $\pi_i$, for instance with concentric circles or angle count techniques. The
set of trees selected at point $x$ is denoted by $s_{2}(x)$. From each of the
selected trees $i\in{s_{2}(x)}$ one determines $Y_i$. The indicator variable $I_i$ is defined as
\begin{equation}\label{1stage}
 I_i(x)=\begin{cases}&1 \text{ if $i\in s_{2}(x)$}\\
                      &0 \text{ if $i\not\in s_{2}(x)$}
         \end{cases}
\end{equation}
At each point $x\in{s_2}$ the
terrestrial inventory provides the local density $Y(x)$
\begin{equation}\label{truelocaldensity}
 Y(x) =\frac{1}{\lambda(F)}\sum_{i=1}^N \frac{I_i(x)Y_i}{\pi_i}=\frac{1}{\lambda(F)}\sum_{i\in{s}_2(x)} \frac{Y_i}{\pi_i}
 \end{equation}
 The term $\frac{1}{\lambda(F)\pi_i}$ is the tree extrapolation factor $f_i$ with dimension $ha^{-1}$. Because of possible boundary adjustments $\lambda(F)\pi_i=\lambda(F \cap K_i)$, where $K_i$ is the inclusion circle of the $i$-th tree. In the infinite population or Monte Carlo approach one samples the function $Y(x)$ and we have $\EX_{x} (Y(x))=\frac{1}{\lambda(F)}\int_{F} Y(x)dx=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i=\bar{Y}$, where $\EX_x$ denotes the expectation with respect to a random point $x$ uniformly distributed in $F$.
 \section{The models}
 We shall work with the following linear models (see \cite{mandallazreport2} for more details)
 \begin{enumerate}
 \item
 The large model $M$
 \begin{equation}\label{largemodel}
 Y(x)=\pmb{Z}^t(x)\pmb{\beta}+ R(x)=\pmb{Z}^{(1)t}(x)\pmb{\beta}^{(1)}
 +\pmb{Z}^{(2)t}(x)\pmb{\beta}^{(2)}+ R(x)=:\hat{Y}_{theo}(x)+R(x)
 \end{equation}
 \noindent with $\pmb{\beta}^t=({\pmb{\beta}^{(1)t}}, {\pmb{\beta}^{(2)t}})$ and the theoretical predictions
 $\hat{Y}_{theo}(x)=\pmb{Z}^t(x)\pmb{\beta}$.\\
 The intercept term is contained in $\pmb{Z}^{(1)}(x)$ or it is a linear combination of its components.\\
 The theoretical regression parameter $\pmb{\beta}$ minimizes
 $\int_F (Y(x)-\pmb{Z}^t(x)\pmb{\beta})^2dx$, it satisfies the normal equation
 $\big(\int_F\pmb{Z}(x)\pmb{Z}^t(x)dx\big)\pmb{\beta}=\int_F Y(x)\pmb{Z}(x)dx$ and the orthogonality relationship
 $\int_F R(x)\pmb{Z}(x)dx=\pmb{0}$, in particular the zero mean residual property
 $\frac{1}{\lambda(F)}\int_F R(x)dx=0$.
 \item
 The reduced model $M_1$
 \begin{equation}\label{reducedmodel}
 Y(x)=\pmb{Z}^{(1)t}(x)\pmb{\alpha} + R_1(x)=:\hat{Y}_{theo,1}(x)+ R_1(x)
 \end{equation}
 The theoretical regression parameter $\pmb{\alpha}$ minimizes
 $\int_F (Y(x)-\pmb{Z}^{(1)t}(x)\pmb{\alpha})^2dx$. It satisfies the normal equation
 $\big(\int_F \pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)dx\big)\pmb{\alpha}=\int_FY(x)\pmb{Z}^{(1)}(x)dx$ and the orthogonality relationship
 $\int_F R_1(x)\pmb{Z}^{(1)}(x)dx=\pmb{0}$, in particular the zero mean residual property $ \frac{1}{\lambda(F)}\int_F R_1(x)dx=0$. $\hat{Y}_{theo,1}(x)=\pmb{Z}^{(1)t}(x)\pmb{\alpha}$ are the theoretical predictions.
\end{enumerate}
Let us emphasize the fact that in this paper we consider only the properties of estimators in the \textbf{design-based} paradigm and that we do not assume the above models to be correct in the sense of \textbf{model-dependent} inference
(see \cite{mandallaz3}).

\section{The generalized regression estimator}
We consider the following design-based least squares estimators of the regression coefficients of the reduced model, which are solutions of sample copies of the normal equations
\begin{eqnarray}\label{coeff1}
\hat{\pmb{\alpha}}_k &=& \Big(\frac{1}{n_k}\sum_{x\in{s}_k}\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)
\Big)^{-1}\frac{1}{n_k}\sum_{x\in{s}_k}Y(x)\pmb{Z}^{(1)}(x)\nonumber\\
&:=&(\pmb{A}^{(1)}_k)^{-1}
\frac{1}{n_k}\sum_{x\in{s}_k}Y(x)\pmb{Z}^{(1)}(x)=:(\pmb{A}^{(1)}_k)^{-1}\pmb{U}^{(1)}_k,\quad  k=0,1,2
\end{eqnarray}
Likewise for the large large model we set
\begin{eqnarray}\label{coeff2}
\hat{\pmb{\beta}}_{k}&=&\Big(\frac{1}{n_k}\sum_{x\in{s}_k}\pmb{Z}(x)\pmb{Z}^t(x)
\Big)^{-1}\frac{1}{n_k}\sum_{x\in{s}_k}Y(x)\pmb{Z}(x)\nonumber \\
&=&\pmb{A}^{-1}_k\frac{1}{n_k}\sum_{x\in{s}_k}Y(x)\pmb{Z}(x)=:\pmb{A}^{-1}_k\pmb{U}_k,\quad k=0,1,2
\end{eqnarray}
Note that only $\hat{\pmb{\alpha}}_2$ and $\hat{\pmb{\beta}}_2$ are observable and that
in general the vector consisting of the first $p$ components of $\hat{\pmb{\beta}}_{2}$ is not equal to $\hat{\pmb{\alpha}}_2$ (they are if the corresponding explanatory variables are orthogonal in the classical least squares sense).\\
The large model yields the predictions $\hat{Y}(x)=\pmb{Z}^t(x)\hat{\pmb{\beta}}_{2}$ and the reduced model the predictions $\hat{Y}_1(x)=\pmb{Z}^{(1)t}(x)\hat{\pmb{\alpha}}_2$.\\
The \textbf{generalized regression estimate} introduced and discussed by \cite{mandallazreport2,mandallaz3} is defined as
\begin{equation}\label{ygreg1}
\hat{Y}_{F,greg} = \frac{1}{\lambda(F)}\int_F \hat{Y}_1(x)dx
+\frac{1}{n_1}\sum_{x\in{s}_1}(\hat{Y}(x)-\hat{Y}_1(x))
+ \frac{1}{n_2}\sum_{x\in{s}_2}(Y(x)-\hat{Y}(x))
\end{equation}
This estimator is the Monte Carlo version of S\"{a}rndal's  regression estimator for two-phase sampling in finite population (see \cite{sarndal}, equation 9.7.20). It is clear by the law of large numbers that  $\hat{\pmb{\beta}}_2$ and $\hat{\pmb{\alpha}}_2$ are asymptotically design-unbiased estimators of $\pmb{\beta}$ and $\pmb{\alpha}$. This implies at once that $\EX_{1,2}\hat{Y}_{greg}= \EX_1\EX_{2 \mid 1}\hat{Y}_{greg} \approx \bar{Y}$ (with $\EX_{2 \mid 1}$ denoting the conditional expectation of the second phase given the first phase, i.e. simple random sampling without replacement in the set $s_1$), and $\EX_1$ denoting the expectation with respect to uniformly distribution points of the first phase (i.e. to $\EX_x$). The generalized regression estimate is therefore asymptotically design-unbiased.\\
Under the \textbf{external model assumption} the design-based variance is given by
\begin{equation}\label{externalvarYgreg}
\VAR(\hat{Y}_{F,greg})=\frac{1}{n_1}\VAR_x(R_1(x))+(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR_x(R(x))
\end{equation}
This should be compared with the standard result for the variance of the regression estimator $\hat{Y}_{reg}$ under the large model
\begin{equation}\label{classicreg}
\hat{Y}_{F,reg}=\frac{1}{n_1}\sum_{x\in{s}_1}\hat{Y}(x)+\frac{1}{n_2}\sum_{x\in{s}_2}(Y(x)-\hat{Y}(x))
\end{equation}
whose theoretical variance is given by
\begin{equation}\label{externalvarYreg}
\VAR(\hat{Y}_{F,reg})=\frac{1}{n_1}\VAR_x(Y(x))+(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR_x(R(x))
\end{equation}
Thus, by using the exhaustive information the variance of the observations in [\ref{externalvarYreg}] is replaced by the variance of the residuals under the reduced model, a very nice and intuitive result indeed.\\
The generalized regression estimator can be rewritten as
\begin{eqnarray}\label{ygreg2}
\hat{Y}_{F,greg}&=&(\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t\hat{\pmb{\alpha}}_2 + (\hat{\bar{\pmb{Z}}}_1-\hat{\bar{\pmb{Z}}}_2)^t\hat{\pmb{\beta}}_{2}+\frac{1}{n_2}
\sum_{x\in{s}_2}Y(x) \nonumber \\
&=& (\bar{\pmb{Z}}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t\hat{\pmb{\alpha}}_2 +
\hat{\bar{\pmb{Z}}}^t_1\hat{\pmb{\beta}}_{2}
\end{eqnarray}
Consistent estimates of the design-based variance of the regression coefficients are given by
\begin{equation}\label{robustvar1}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_2}=\pmb{A}_2^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}^2(x)\pmb{Z}(x)\pmb{Z}^t(x)\Big)\pmb{A}_2^{-1}
\end{equation}
and
\begin{equation}\label{robustvar2}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}=(\pmb{A}^{(1)}_1)^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}_1^2(x)\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)\Big)(\pmb{A}^{(1)}_1)^{-1}
\end{equation}
with the empirical residuals $\hat{R}(x)=Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{2}$ and $\hat{R_1}(x)=Y(x)-\pmb{Z}^{1)t}(x)\hat{\pmb{\alpha}}_2$ (see section \ref{proofofvariance} in the Appendix for a short review and \cite{mandallaz},pp 124-125 for details).\\
Furthermore
\begin{equation}\label{estvarygregnew}
\hat{\VAR}(\hat{Y}_{F,greg})=\frac{n_2}{n_1}\bar{\pmb{Z}}^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}
\bar{\pmb{Z}}^{(1)}+(1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}_1^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_2}
\hat{\bar{\pmb{Z}}}_1
\end{equation}
\noindent is a consistent estimate of the \textbf{design-based variance} of $\hat{Y}_{F,greg}$ (see \cite{mandallazreport2} for the proof).\\
The g-weights are defined as
\begin{eqnarray}\label{gweightygreg1}
g_2(x) &=& \hat{\bar{\pmb{Z}}}_1^t\pmb{A}_2}^{-1}{\pmb{Z}(x)\nonumber \\
g^{(1)}_1(x) &=& \bar{\pmb{Z}}^{(1)t}(\pmb{A}^{(1)}_1)^{-1}\pmb{Z}^{(1)}(x)
\end{eqnarray}
They enjoy the calibration properties
\begin{eqnarray}\label{gweightygreg2}
\frac{1}{n_2}\sum_{x\in{s}_2}g_2(x)\pmb{Z}(x)&=&\hat{\bar{\pmb{Z}}}_1 \nonumber \\
\frac{1}{n_1}\sum_{x\in{s}_1}g^{(1)}_1(x)\pmb{Z}^{(1)}(x)&=&\bar{\pmb{Z}}^{(1)}
\end{eqnarray}
The variance [\ref{estvarygregnew}] can be rewritten as
\begin{equation}\label{estvarygreggweights}
\hat{\VAR}(\hat{Y}_{F,greg})=\hat{\VAR}(\hat{Y}_{F,greg})=\frac{1}{n_1}\frac{1}{n_2}\sum_{x\in{s}_2}(g_1^{(1)}(x))^2\hat{R}_1^2(x)+
\frac{1}{n_2}(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{x\in{s}_2}g_2^2(x)\hat{R}^2(x)
\end{equation}
Under the external model assumptions it is straightforward to obtain approximate estimates of the design-based variances of $\hat{Y}_{F,reg}$ and $\hat{Y}_{F,greg}$ by using the standard variance estimates in [\ref{externalvarYgreg}] and [\ref{externalvarYreg}] after replacing the theoretical residuals by the empirical ones.
The generalized regression estimator $\hat{Y}_{G,greg}$ for any small-area $G\subset F$ has been discussed in \cite{mandallazreport2, mandallaz4}. $\hat{Y}_{F,greg}$ and $\hat{Y}_{G,greg}$ can be viewed as limit cases of the estimators presented in the next two sections.

\section{The generalized regression estimator in three-phase sampling}
If $\pmb{Z}^{(1)}(x)$ is no longer exhaustive we replace the first term in [\ref{ygreg1}] by its sample mean in the null phase sample and we define the new three-phase estimator as
\begin{equation}\label{y0greg1}
\hat{Y}_{F,g3reg} = \frac{1}{n_0} \sum_{x\in{s_0}} \hat{Y}_1(x)
+\frac{1}{n_1}\sum_{x\in{s}_1}(\hat{Y}(x)-\hat{Y}_1(x))
+ \frac{1}{n_2}\sum_{x\in{s}_2}(Y(x)-\hat{Y}(x))
\end{equation}
By using the properties of conditional expectations and variances (see \cite{mandallaz}, appendix B) we have
\begin{equation}\label{condexp1}
\EX_{0,1,2}(\hat{Y}_{F,g3reg})=\EX_0 \EX_{1 \mid 0}\EX_{2 \mid 0,1}(\hat{Y}_{F,g3reg})
\end{equation}
and
\begin{eqnarray}\label{condvar1}
\VAR_{0,1,2}(\hat{Y}_{F,g3reg})&=&\EX_0 \EX_{1 \mid 0}\VAR_{2 \mid 0,1}(\hat{Y}_{F,g3reg})+
\EX_0 \VAR_{1 \mid 0}\EX_{2 \mid 0,1}(\hat{Y}_{F,g3reg})
+\VAR_0 \EX_{1 \mid 0}\EX_{2 \mid 0,1}(\hat{Y}_{F,g3reg})\nonumber \\
&=:& T_1 + T_2 + T_3
\end{eqnarray}
Under the external model assumption it follows from [\ref{condvar1}] after some algebra that $\hat{Y}_{F,g3reg}$ is exactly unbiased with variance
\begin{equation}\label{varexterny0greg}
\VAR_{0,1,2}(\hat{Y}_{F,g3reg})=\frac{1}{n_0}\VAR_{x}(Y(x))+(1-\frac{n_1}{n_0})\frac{1}{n_1}\VAR_x(R_1(x))+
(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR(R(x))
\end{equation}
If the predictions $\hat{Y}_1(x)$ and the residuals $R_1(x)$ are orthogonal in the design-based sense (this is the case for their theoretical versions) then, because $\VAR_x(Y(x))=\VAR_x(\hat{Y}_1(x))+\VAR_x(R_1(x))$, one obtains
\begin{equation}\label{varexterny0gregbis}
\VAR_{0,1,2}(\hat{Y}_{F,g3reg})=\frac{1}{n_0}\VAR_{x}(\hat{Y}_1(x))+\frac{1}{n_1}\VAR_x(R_1(x))+
(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR(R(x))
\end{equation}
Let us now define
\begin{equation}\label{meanvalues}
\hat{\bar{\pmb{Z}}}^{(1)}_0=\frac{1}{n_0}\sum_{x\in{s_0}} \pmb{Z}^{(1)}(x)dx ,\quad \hat{\bar{\pmb{Z}}}^{(1)}_1=\frac{1}{n_1}\sum_{x\in{s}_1}\pmb{Z}^{(1)}(x) ,
\quad \hat{\bar{\pmb{Z}}}_k=\frac{1}{n_k}\sum_{x\in{s}_k}\pmb{Z}(x),\;k=1,2
\end{equation}
and rewrite $\hat{Y}_{F,g3reg}$ as

\begin{eqnarray}\label{0,ygreg2}
\hat{Y}_{F,g3reg}&=&(\hat{\bar{\pmb{Z}}}^{(1)}_0-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t\hat{\pmb{\alpha}}_2 + (\hat{\bar{\pmb{Z}}}_1-\hat{\bar{\pmb{Z}}}_2)^t\hat{\pmb{\beta}}_{2}+\frac{1}{n_2}
\sum_{x\in{s}_2}Y(x) \nonumber \\
&=& (\hat{\bar{\pmb{Z}}}^{(1)}_0-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t\hat{\pmb{\alpha}}_2 +
\hat{\bar{\pmb{Z}}}^t_1\hat{\pmb{\beta}}_{2}
\end{eqnarray}
The last equation follows from the fact that the sum of the residuals is zero by construction.\\
We now calculate the variance of [\ref{0,ygreg2}]  by using [\ref{robustvar1},\ref{robustvar2},\ref{condvar1}]. We begin with the easiest part, namely the last term $T_3$ in [\ref{condvar1}]. First, we obtain the asymptotic equivalence
\begin{equation}\label{equivalenz1}
\EX_{2\mid 0,1}(\hat{Y}_{F,g3reg})\approx ( \hat{\bar{\pmb{Z}}}^{(1)}_0-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t\hat{\pmb{\alpha}}_1 +
\hat{\bar{\pmb{Z}}}^t_1\hat{\pmb{\beta}}_{1}\approx \hat{\bar{\pmb{Z}}}^{(1)t}_0\hat{\pmb{\alpha}}_{1}
\end{equation}
The last equation results from the fact that both $\hat{\bar{\pmb{Z}}}^{(1)t}_1\hat{\pmb{\alpha}}_{1}$ and $
\hat{\bar{\pmb{Z}}}^{t}_1\hat{\pmb{\beta}}_{1}$ tend to $\bar{Y}$.\\
 We then get
$\EX_{1 \mid 0}\EX_{2\mid 0,1}(\hat{Y}_{F,g3reg})\approx \hat{\bar{\pmb{Z}}}^{(1)t}_0\hat{\pmb{\alpha}}_0$.
From [\ref{appendixhatt3}] in the Appendix (section \ref{proofofvariance}) we get the asymptotically consistent estimate of
 $T_3\approx \VAR_0(\hat{\bar{\pmb{Z}}}^{(1)t}_0\hat{\pmb{\alpha}}_0)$
\begin{equation}\label{hatt3}
\hat{T}_3=\frac{n_2}{n_0}\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}\hat{\bar{\pmb{Z}}}_0^{(1)}
+ \hat{\pmb{\alpha}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_0}\hat{\pmb{\alpha}}_2
\end{equation}
where
\begin{equation}\label{estcovarz}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_{0}}=
\frac{1}{n_{0}}\frac{\sum_{x\in{s_{0}}}(\pmb{Z}^{(1)}(x)-
\hat{\bar{\pmb{Z}}}^{(1)}_{0})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}^{(1)}_{0})^t}{n_0-1}
\end{equation}
\noindent The calculations for $T_2$ and $T_1$ are more complicated and rely on Taylor expansions for the regression coefficients. Details are given in section \ref{proofofvariance} of the Appendix.\\
According to [\ref{appendixhatt2}] (section \ref{proofofvariance} of the Appendix) we have the following asymptotically consistent estimate of $T_2$
\begin{equation}\label{hatt2}
\hat{T}_2=\frac{n_2}{n_1}(1-\frac{n_1}{n_0})\hat{\bar{\pmb{Z}}}^{(1)t}_0
\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_{2}}\hat{\bar{\pmb{Z}}}^{(1)}_0
\end{equation}
\noindent and according to [\ref{appendixhatt1}] the following asymptotically consistent estimate of $T_1$
\begin{equation}\label{hatt1}
\hat{T}_1 \approx (1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}^{t}_1\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_2}\hat{\bar{\pmb{Z}}}^{t}_1
\end{equation}
and consequently the following asymptotically consistent estimate of the overall three-phase variance ([\ref{estvary0greg2}], section \ref{proofofvariance} of the Appendix)
\begin{equation}\label{estvarianceY0greg}
\hat{\VAR}_{0,1,2}(\hat{Y}_{F,g3reg})=\hat{\pmb{\alpha}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_0}\hat{\pmb{\alpha}}_2
+\frac{n_2}{n_1}\hat{\bar{\pmb{Z}}}^{(1)t}_0
\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_{2}}\hat{\bar{\pmb{Z}}}^{(1)}_0 + (1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}^{t}_1\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_2}\hat{\bar{\pmb{Z}}}^{t}_1
\end{equation}
\noindent By comparing with [\ref{estvarygregnew}] we see that in large samples we have
\begin{equation}\label{comparison1}
\hat{\VAR}(\hat{Y}_{F,g3reg})- \hat{\VAR}(\hat{Y}_{F,greg})=
\hat{\pmb{\alpha}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_0}\hat{\pmb{\alpha}}_2
=\frac{1}{n_0}\frac{\sum_{x\in{s_0}}(\hat{Y}_1(x)-\hat{\bar{Y}}_1)^2}{n_0-1}
\end{equation}
\noindent where we have set $\hat{Y}_1(x)=\pmb{Z}^{(1)t}(x)\hat{\pmb{\alpha}}_2$ and
$\hat{\bar{Y}}_1=\frac{1}{n_0}\sum_{x\in{s_0}}\hat{Y}_1(x)$. Hence, the extra term in the variance due to three-phase sampling is given by the variance of the predictions of the reduced model in the very large null-phase sample, which can be made very small while cutting down significantly the computing time.
The g-weights are defined as
\begin{eqnarray}\label{gweight1}
g(x) &=& \hat{\bar{\pmb{Z}}}_1^t\pmb{A}_2^{-1}\pmb{Z}(x)\nonumber \\
g^{(1)}(x) &=& \hat{\bar{\pmb{Z}}}_0^{(1)t}(\pmb{A}^{(1)}_1)^{-1}\pmb{Z}^{(1)}(x)
\end{eqnarray}
They satisfy the calibration properties
\begin{eqnarray}\label{gweight2}
\frac{1}{n_2}\sum_{x\in{s}_2}g(x)\pmb{Z}(x)&=&\hat{\bar{\pmb{Z}}}_1 \nonumber \\
\frac{1}{n_1}\sum_{x\in{s}_1}g^{(1)}(x)\pmb{Z}^{(1)}(x)&=&\hat{\bar{\pmb{Z}}}_0^{(1)}
\end{eqnarray}
\noindent Note the slights differences between [\ref{gweight1}], [\ref{gweight2}] and [\ref{gweightygreg1}],[\ref{gweightygreg2}].\\
It can be shown (see \cite{mandallazreport1, mandallazreport2}) that $g^{(1)}(x)$ and $g(x)$ tend to $1$ for $n_1,n_2 \to \infty$ and that
\begin{eqnarray*}
\frac{1}{n_1}\sum_{x_{s_1}}g^{(1)}(x)&=& 1  \\
\frac{1}{n_2}\sum_{x\in{s_2}}g(x)&=& 1\\
\end{eqnarray*}
\noindent One can rewrite [\ref{estvarianceY0greg}] as
\begin{eqnarray}\label{estvary0greggweights}
\hat{\VAR}(\hat{Y}_{F,g3reg})&=&\hat{\pmb{\alpha}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_0}\hat{\pmb{\alpha}}_2
\nonumber\\
&+&\frac{1}{n_1}\frac{1}{n_2}\sum_{x\in{s}_2}(g^{(1)}(x))^2\hat{R}_1^2(x)+
\frac{1}{n_2}(1-\frac{n_2}{n_1})\frac{1}{n_2}\sum_{x\in{s}_2}g^2(x)\hat{R}^2(x)
\end{eqnarray}
Under the external model assumptions one obtains from [\ref{varexterny0gregbis}] the following variance estimate
\begin{eqnarray}\label{extexternvary0greg}
\hat{\VAR}_{ext}(\hat{Y}_{F,g3reg})&=&\frac{1}{n_0}\frac{\sum_{x\in{s_0}}(\hat{Y}_1(x)-\hat{\bar{Y}}_1)^2}{n_0-1}
\nonumber\\
&+&\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s}_2}(\hat{R}_1(x)-\hat{\bar{R_1}})^2+
(1-\frac{n_2}{n_1})\frac{1}{n_2(n_2-1)}\sum_{x\in{s}_2}(\hat{R}(x)-\hat{\bar{R}})^2
\end{eqnarray}
\noindent where $\hat{\bar{Y}}_1=\frac{1}{n_0}\sum_{x\in{s_0}}\hat{Y}_1(x)$, $\hat{\bar{R}}_1=\frac{1}{n_2}\sum_{x\in{s_2}}\hat{R}_1(x)=0$ and
$\hat{\bar{R}}=\frac{1}{n_2}\sum_{x\in{s_2}}\hat{R}(x)=0$.
Since the g-weights $g^{(1)}(x)$ and $g(x)$ tend to $1$ for $n_1,n_2 \to \infty$ we see that both variance estimates [\ref{estvary0greggweights}] and [\ref{extexternvary0greg}] are asymptotically equivalent. However, the g-weight version offers several statistical advantages from a theoretical point of view.
\section{Application to small-area estimation}

We consider a small area $G\subset F$ and we want to estimate
$$\bar{Y}_G=\frac{1}{\lambda(G)}\sum_{i=1}^NI_G(i)Y_i=\frac{1}{\lambda(G)}\int_G Y(x)dx$$
where $I_G(i)=1 $ if the $i$-th tree is in $G$, otherwise $I_G(i)=0$. Strictly speaking the last equality holds if boundary adjustments are performed in $G$, whereas they are in most instances only performed with respect to $F$. We shall need the following notation: $s_{0,G}=s_o \cap G$, $s_{1,G}=s_1\cap G$, $s_{2,G}=s_2 \cap G$, $n_{k,G}=\sum_{s\in{s_k}}I_G(x), \;k=0,1,2$. \\
The simplest procedure is to restrict the samples to $G$, i.e. to consider \textbf{the generalized small-area estimator}
\begin{equation}\label{smallareaest1}
\hat{Y}_{G,g3reg}=\frac{1}{n_{0,G}}\sum_{x\in{s_{0,G}}} \hat{Y}_1(x)+
\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}(\hat{Y}(x)-\hat{Y}_1(x))+
\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}(Y(x)-\hat{Y}(x))
\end{equation}
 and treat the internal model as an external one to obtain from [\ref{varexterny0greg}] the variance estimate
 \begin{eqnarray}\label{extsmallareaestvariance1}
 \hat{\VAR}_{ext1}(\hat{Y}_{G,g3reg})&=& \frac{1}{n_{0,G}(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}(Y(x)-\hat{\bar{Y}}_G)^2 \nonumber \\
 &+& (1-\frac{n_{1,G}}{n_{0,G}}){\frac{1}{n_{1,G}(n_{2,G}-1)}\sum_{x\in{s}_{2,G}}}
 (\hat{R}_1(x)-\hat{\bar{R}}_{1,G})^2 \nonumber \\
 &+& (1-\frac{n_{2,G}}{n_{1,G}})\frac{1}{n_{2,G}(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\hat{\bar{R}}_G)^2
 \end{eqnarray}
where $\hat{\bar{Y}}_G=\frac{1}{n_{0,G}}\sum_{x\in{s_{0,G}}}Y(x)$, $\hat{\bar{R}}_{1,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}_1(x)$ and $\hat{\bar{R}}_G=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)$. This variance estimate neglects the uncertainty of the regression coefficients but there is empirical evidence that this is acceptable in large samples (see \cite{mandallaz3} for examples with $\hat{Y}_{greg}$).\\
If the empirical covariance between predictions and residuals can be assumed to be zero over $G$ (it is over $F$) then one could use [\ref{varexterny0gregbis}] instead to obtain
\begin{eqnarray}\label{extsmallareaestvariance2}
 \hat{\VAR}_{ext2}(\hat{Y}_{G,g3reg})&=& \frac{1}{n_{0,G}(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}(\hat{Y}_1(x)-\hat{\bar{Y}}_{1,G})^2 \nonumber \\
 &+& {\frac{1}{n_{1,G}(n_{2,G}-1)}\sum_{x\in{s}_{2,G}}}
 (\hat{R}_1(x)-\hat{\bar{R}}_{1,G})^2 \nonumber \\
 &+& (1-\frac{n_{2,G}}{n_{1,G}})\frac{1}{n_{2,G}(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\hat{\bar{R}}_G)^2
 \end{eqnarray}
In the first term $\frac{1}{(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}(\hat{Y}_1(x)-\hat{\bar{Y}}_{1,G})^2 $ is the empirical variance of the predictions in $s_{2,G}$ and can be replaced by the variance of the prediction in $s_{0,G}$, i.e. by $\frac{1}{n_{0,G}-1}\sum_{x\in{s_{0,G}}}(\hat{Y}_1(x)-\hat{\bar{Y}}_{1,G})^2$ (with
$\hat{\bar{Y}}_{1,G}=\frac{1}{n_{0,G}}\sum_{x\in{s_{0,G}}} \hat{Y}_1(x)$ ) to obtain the following variance estimate under the external model assumption.
 \begin{eqnarray}\label{extsmallareaestvariance3}
 \hat{\VAR}_{ext}(\hat{Y}_{G,g3reg})&=& \frac{1}{n_{0,G}(n_{0,G}-1)}\sum_{x\in{s_{0,G}}}(\hat{Y}_1(x)-\hat{\bar{Y}}_{1,G})^2 \nonumber \\
 &+& {\frac{1}{n_{1,G}(n_{2,G}-1)}\sum_{x\in{s}_{2,G}}}
 (\hat{R}_1(x)-\hat{\bar{R}}_{1,G})^2 \nonumber \\
 &+& (1-\frac{n_{2,G}}{n_{1,G}})\frac{1}{n_{2,G}(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\hat{\bar{R}}_G)^2
 \end{eqnarray}
 \noindent with
 \begin{eqnarray*}
 \hat{\bar{Y}}_{1,G}&=&\frac{1}{n_{0,G}}\sum_{x\in{s_{0,G}}}\hat{Y}_1(x) \\
 \hat{\bar{R}}_{1,G}&=&\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}_1(x) \\
 \hat{\bar{R}}_{G}&=&\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)
 \end{eqnarray*}
 \noindent Note that the first term in [\ref{extsmallareaestvariance3}] can be expected to be more stable than the first terms in [\ref{extsmallareaestvariance1}] and [\ref{extsmallareaestvariance2}]. Also, as we shall see,  $\hat{\VAR}_{ext3}(\hat{Y}_{G,0,greg})$ is asymptotically equivalent to a g-weight version of the variance, and should therefore be preferred to [\ref{extsmallareaestvariance1}] and [\ref{extsmallareaestvariance2}].

The main difficulty to obtain better variance estimate is that the residual third term in [\ref{smallareaest1}] is no longer zero in general. The most elegant way to bypass this difficulty is by extending the model with the indicator variable $I_G(x)$ of the small area $G$. Since the procedure has been discussed in details in \cite{mandallazreport1, mandallazreport2, mandallaz3, mandallaz4} we can here briefly sketch the procedure.\\
We have the extended models with auxiliary vectors:
$\pmb{\mathcal{Z}}^t(x)=(\pmb{\mathcal{Z}}^{(1)t}(x),\pmb{\mathcal{Z}}^{(2)t}(x))$,
where $\pmb{\mathcal{Z}}^{(1)t}(x)=(\pmb{Z}^{(1)t}(x), I_G^t(x))$ and $\pmb{\mathcal{Z}}^{(2)t}(x)=\pmb{Z}^{(2)t}(x)$.
\begin{enumerate}
 \item
 The large extended model $M$
 \begin{equation*}
 Y(x)=\pmb{\mathcal{Z}}(x)^t\pmb{\theta}+ \mathcal{R(}x)=\pmb{\mathcal{Z}}^{(1)t}(x)\pmb{\theta}^{(1)}
 +\pmb{\mathcal{Z}}^{(2)t}(x)\pmb{\theta}^{(2)}+ \mathcal{R}(x)
 \end{equation*}
 \noindent with $\pmb{\theta}^t=({\pmb{\theta}^{(1)t}}, {\pmb{\theta}^{(2)t}})$.
 The intercept term is contained in $\pmb{\mathcal{Z}}^{(1)}(x)$ or it is a linear combination of its components.
 \item
 The reduced extended model $M_1$
 \begin{equation*}
 Y(x)=\pmb{\mathcal{Z}}^{(1)t}(x)\pmb{\gamma} + \mathcal{R}_1(x)
 \end{equation*}
\end{enumerate}
We can obviously apply mutatis mutandis all the previous results. The estimated regression coefficients are
\begin{eqnarray}\label{extcoeff1}
\hat{\pmb{\gamma}}_2 &=& \Big(\frac{1}{n_2}\sum_{x\in{s}_2}\pmb{\mathcal{Z}}^{(1)}(x)\pmb{\mathcal{Z}}^{(1)t}(x)
\Big)^{-1}\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}^{(1)}(x)\nonumber\\
&:=&(\pmb{\mathcal{A}}^{(1)}_2)^{-1}
\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}^{(1)}(x)
\end{eqnarray}
and
\begin{equation}\label{extcoeff2}
\hat{\pmb{\theta}}_{2}=\Big(\frac{1}{n_2}\sum_{x\in{s}_2}\pmb{\mathcal{Z}}(x)\pmb{\mathcal{Z}}^t(x)
\Big)^{-1}\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}(x):
=\pmb{\mathcal{A}}^{-1}_2\frac{1}{n_2}\sum_{x\in{s}_2}Y(x)\pmb{\mathcal{Z}}(x)
\end{equation}
The estimated covariance matrices are
\begin{eqnarray}\label{robustvar3}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_2}&=&\pmb{\mathcal{A}}_2^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{\mathcal{R}}^2(x)\pmb{\mathcal{Z}}(x)
\pmb{\mathcal{Z}}^t(x)\Big)\pmb{\mathcal{A}}_2^{-1} \nonumber \\
\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_2}&=&(\pmb{\mathcal{A}}^{(1)}_1)^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{\mathcal{R}}_1^2(x)\pmb{\mathcal{Z}}^{(1)}(x)
\pmb{\mathcal{Z}}^{(1)t}(x)\Big)(\pmb{\mathcal{A}}^{(1)}_1)^{-1} \nonumber \\
\end{eqnarray}
\noindent
where $\hat{\mathcal{R}}(x)=Y(x)-\pmb{\mathcal{Z}}^t(x)\hat{\pmb{\theta}}_2$ and
$\hat{\mathcal{R}}_1(x)=Y(x)-\pmb{\mathcal{Z}}^{(1)t}(x)\hat{\pmb{\gamma}}_2$ are the residuals. \\

\noindent Because the sum of the residuals over $s_{2,G}$ is now zero we can write the new small-area estimator $\hat{\tilde{Y}}_{G,g3reg}$ as in [\ref{ygreg2}]

\begin{equation}\label{smallarea0greg}
\hat{\tilde{Y}}_{G,g3reg}
= (\hat{\bar{\pmb{\mathcal{Z}}}}_{0,G}^{(1)}-\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^{(1)})^t\hat{\pmb{\gamma}}_2 +
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^t\hat{\pmb{\theta}}_{2}
\end{equation}
where we have set
\begin{eqnarray*}
 \hat{\bar{\pmb{\mathcal{Z}}}}_{0,G}^{(1)}&=&\frac{1}{n_{0,G}}\sum_{x\in{s_{0,G}}} \pmb{\mathcal{Z}}^{(1)}(x) \\ \hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^{(1)}&=&
 \frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\pmb{\mathcal{Z}}^{(1)}(x)\\
 \hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}&=&
 \frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\pmb{\mathcal{Z}}(x)
 \end{eqnarray*}
 To get an estimate of the design-based variance we use mutatis mutandis [\ref{estvarianceY0greg}]
and we obtain
\begin{eqnarray}\label{estvarianceY_G0greg}
\hat{\VAR}_{0,1,2}(\hat{Y}_{G,g3reg})&=&\hat{\pmb{\gamma}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G}}
\hat{\pmb{\gamma}}_2 +\frac{n_2}{n_1}\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)t}_{0,G}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_{2}}\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G} \nonumber \\ &+& (1-\frac{n_2}{n_1})\hat{\bar{\pmb{\mathcal{Z}}}}^{t}_{1,G}\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_2}
\hat{\bar{\pmb{\mathcal{Z}}}}^{t}_{1,G}
\end{eqnarray}
where
\begin{equation}\label{estcovarextz-overG}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G}}=
\frac{1}{n_{0,G}}\frac{1}{n_{0,G}-1}  \sum_{x\in{s_{0,G}}}(\pmb{\mathcal{Z}}^{(1)}(x)-
\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G})(\pmb{\mathcal{Z}}(x)-\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G})^t
\end{equation}
\noindent is the empirical covariance matrix of $\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G}$ over $G$.\\
The first term in [\ref{estvarianceY_G0greg}] can be rewritten as the empirical variance of the predictions
$\hat{\tilde{Y}}_1(x)=\pmb{\mathcal{Z}}^{(1)t}\hat{\pmb{\gamma}}_2$, that is
\begin{equation*}
\hat{\pmb{\gamma}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G}}\hat{\pmb{\gamma}}_2=
\frac{1}{n_{0,G}-1}\frac{1}{n_{0,G}}\sum_{x\in{s_{0,G}}}(\hat{\tilde{Y}}_1(x)-\hat{\bar{\tilde{Y}}}_{1,G})^2
\end{equation*}
\noindent with $\hat{\bar{\tilde{Y}}}_{1,G}=\frac{1}{n_{0,G}}\sum_{x\in{s_{0,G}}}\hat{\tilde{Y}}_1(x)$.\\
It is shown in section \ref{proofofequivalence} that the external variance estimate [\ref{extsmallareaestvariance3}]
and the g-weight variance estimate [\ref{estvarianceY_G0greg}] are asymptotically equivalent, which is not obvious at first glance.\\
For a very small area $H$ the number of points $n_{2,H}$ may be to small or even zero. In such a case one could imbed $H$ in a somewhat larger area $G\supset H$ with $n_{2,G}$ sufficiently large and consider the synthetic estimator in the extended model with respect to $G$: i.e. one replaces $\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G}$ by $\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,H}$, $\hat{\bar{\pmb{\mathcal{Z}}}}^{t}_{1,G}$ by $\hat{\bar{\pmb{\mathcal{Z}}}}^{t}_{1,H}$  in equations [\ref{smallarea0greg}], [\ref{estvarianceY_G0greg}] and
$\hat{\pmb{\gamma}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,G}}\hat{\pmb{\gamma}}_2$ by
$\hat{\pmb{\gamma}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{0,H}}\hat{\pmb{\gamma}}_2
=\frac{1}{n_{0,H}-1}\frac{1}{n_{0,H}}\sum_{x\in{s_{0,H}}}(\hat{\tilde{Y}}_1(x)-\hat{\bar{\tilde{Y}}}_{1,H})^2$ with
$\hat{\bar{\tilde{Y}}}_{1,H}=\frac{1}{n_{0,H}}\sum_{x\in{s_{0,H}}}\hat{\tilde{Y}}_1(x)$.\\
The variance $\hat{\VAR}_{0,1,2}(\hat{Y}_{H,g3reg})$ will be small but at the cost of a potential bias, which can be expected to be smaller than the bias of the synthetic estimator with respect to $G=F$.\\
As shown in \cite{mandallazreport1} it is straightforward to consider simultaneously several
small areas by extending the model with as many small area indicator variables.\\

\section{Generalization to cluster sampling}
We follow the description of cluster sampling as defined in \cite{mandallaz} (especially section 5.5) and \cite{mandallazreport1}. A cluster is identified by its origin $x$, uniformly distributed in $\tilde{F}\supset F$. The geometry of the cluster is given by $M$ vectors $e_1,\ldots e_M$ defining the random cluster $x_l=x+e_l$. $M(x)=\sum_{l=1}^MI_{F}(x_l)$ is the random number of points of the cluster falling into the forest area $F$. We define the local density at the cluster level by $Y_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)Y(x_l)}{M(x)}$, likewise we set $\pmb{Z}_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)\pmb{Z}(x_l)}{M(x)}$. The set $\tilde{F}$ above can be mathematically defined as the smallest set $\{x\in{\mathcal{R}}^2 \mid M(x) \ne 0 \}$. In the null phase we have $n_0$ clusters identified by $x\in{s_0}$, in the first phase $n_1$ clusters identified by $x\in{s_1}$, and in the second phase $n_2$ clusters with $x\in{s_2}$, obtained by simple random sampling in $s_0$, or $s_1$ respectively.
As the estimators $\hat{Y}_{reg},\hat{Y}_{F,greg}$ and $\hat{\tilde{Y}}_{G,greg}$ have been generalized to cluster sampling in \cite{mandallazreport2,mandallaz3,mandallaz4} we will only sketch the main features. \\
The design-unbiased estimate $\hat{\pmb{\beta}}_{c,2}$  for $\pmb{\beta}_c$ (the equivalent of $\pmb{\beta}$ in simple random sampling) is
 \begin{eqnarray}\label{estbetacluster1}
 \hat{\pmb{\beta}}_{c,2}&=&
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{Z}_c(x)\pmb{Z}_c^t(x)\Big)^{-1}
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \nonumber\\
 &:=&\pmb{A}^{-1}_{c,s_2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \end{eqnarray}
For the reduced model with $\pmb{Z}^{(1)}(x)$ alone we obtain similarly
\begin{eqnarray}\label{estalphacluster1}
 \hat{\pmb{\alpha}}_{c,2}&=&
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{Z}^{(1)}_c(x)\pmb{Z}_c^{(1)t}(x)\Big)^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \nonumber\\
 &:=&(\pmb{A}^{(1)}_{c,2})^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}^{(1)}_c(x)\Big)
 \end{eqnarray}
 The residuals for the reduced model are $\hat{R}_{1,c}(x)=Y_c(x)-\hat{Y}_{1,c}$ with the predictions $\hat{Y}_{1,c}=\pmb{Z}^{(1)t}_c(x)\hat{\pmb{\alpha}}_{c,2}$.\\
 \noindent Using mutatis mutandis exactly the same arguments as in simple random sampling we get the asymptotic robust design-based estimated covariance matrices
\begin{eqnarray}\label{estcovmatrixcluster1}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,2}}&=&\pmb{A}^{-1}_{c,2}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}}M^2(x)\hat{R}^2_c(x)
\pmb{Z}_c(x)\pmb{Z}^t_c(x)\Big)\pmb{A}^{-1}_{c,2}\nonumber \\
\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_{c,2}}&=&(\pmb{A}^{(1)}_{c,1})^{-1}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}}M^2(x)\hat{R}_{1,c}^2(x)
\pmb{Z}_c(x)\pmb{Z}^t_c(x)\Big)(\pmb{A}^{(1)}_{c,1})^{-1}
\end{eqnarray}
where $\pmb{A}^{(1)}_{c,1}=\frac{1}{n_1}\sum_{x\in{s_1}}M(x)\pmb{Z}^{(1)}_c(x)\pmb{Z}_c^{(1)t}(x)$.\\

We define the \textbf{generalized regression estimator in cluster sampling} directly by adapting [\ref{ygreg2}]
\begin{equation}\label{y0gregcluster}
\hat{Y}_{c,g3reg}= (\hat{\bar{\pmb{Z}}}_{c,0}^{(1)}-\hat{\bar{\pmb{Z}}}^{(1)}_{c,1})^t\hat{\pmb{\alpha}}_{c,2} +
\hat{\bar{\pmb{Z}}}^t_{c,1}\hat{\pmb{\beta}}_{c,2}
\end{equation}
where
\begin{eqnarray*}
\hat{\bar{\pmb{Z}}}^{(1)}_{c,0}&=&\frac{\sum_{x\in{s_0}}M(x)\pmb{Z}^{(1)}_c(x)}{\sum_{x\in{s_0}}M(x)} \\
\hat{\bar{\pmb{Z}}}^{(1)}_{c,1}&=&\frac{\sum_{x\in{s_1}}M(x)\pmb{Z}^{(1)}_c(x)}{\sum_{x\in{s_1}}M(x)} \\
\hat{\bar{\pmb{Z}}}_{c,1}&=& \frac{\sum_{x\in{s_1}}M(x)\pmb{Z}_c(x)}{\sum_{x\in{s_1}}M(x)}
\end{eqnarray*}

With the same technique as in simple random sampling we obtain the estimated design-based variance
\begin{eqnarray}\label{estvarclustery0greg}
\hat{\VAR}_{0,1,2}(\hat{Y}_{c,g3reg})&=& \hat{\pmb{\alpha}}^t_{c,2}\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_{c,0}}\hat{\pmb{\alpha}}_{c,2} \nonumber \\
&+& \frac{n_2}{n_1}\hat{\bar{\pmb{Z}}}_{c,0}^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_{c,2}}
\hat{\bar{\pmb{Z}}}_{c,0}^{(1)}+(1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}_{c,1}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,2}}
\hat{\bar{\pmb{Z}}}_{c,1}
\end{eqnarray}
where
\begin{equation*}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_{c,0}}=
\frac{1}{n_0(n_0-1)}\sum_{x\in{s_0}}\big(\frac{M(x)}{\bar{M}_0}\big)^2(\pmb{Z}^{(1)}_c(x)-\hat{\bar{\pmb{Z}}}^{(1)}_{c,0})
(\pmb{Z}^{(1)}_c(x)-\hat{\bar{\pmb{Z}}}^{(1)}_{c,0})^t
\end{equation*}
\noindent is the empirical covariance matrix of the $\pmb{Z}^{(1)}_c(x)$ in $s_0$.\\
The first term in [\ref{estvarclustery0greg}] can be rewritten as the empirical variance over $s_0$ of the predictions of the reduced model, i.e.
\begin{equation*}
\frac{1}{n_0(n_0-1)}\sum_{x\in{s_0}}\big(\frac{M(x)}{\bar{M}_0}\big)^2(\hat{Y}_{1,c}(x)-
\hat{\bar{Y}}_{1,0})^2
\end{equation*}
with $\hat{\bar{Y}}_{1,0}=\frac{\sum_{x\in{s_0}}M(x)\hat{Y}_{1,c}(x)}{\sum_{x\in{s_0}}M(x)}$ and
$\bar{M}_0=\frac{1}{n_0}\sum_{x\in{s_0}}M(x)$.\\
The external version of the variance estimate can be easily adapted from [\ref{extexternvary0greg}] after introducing the weights $\big(\frac{M(x)}{\bar{M}_k}\Big)^2$.\\
For small-area estimation we shall work with the extended model
\begin{equation*}
\pmb{\mathcal{Z}}^t(x)=(\pmb{\mathcal{Z}}^{(1)t}(x), \pmb{\mathcal{Z}}^{(2)t}(x))
\end{equation*}
 with
$\pmb{\mathcal{Z}}^{(1)t}(x)=(\pmb{Z}^{(1)t}(x), I_G^t(x))$, $\pmb{\mathcal{Z}}^{(2)t}(x)=\pmb{Z}^{(2)t}(x)$ and $I_G(x)$ is the indicator of the small area $G$. At the cluster level we have $\pmb{\mathcal{Z}}_c^{(1)t}(x)=(\pmb{Z}_c^{(1)t}(x),I^t_{c,G}(x))$ where $I_{c,G}(x)=\frac{\sum_{l=1}^M I_G(x_l)}{M(x)}$. In extensive inventories we can reasonably assume that all the points of a cluster lying in the forest area $F$ will belong to the same small area $G$ so that in fact $I_{c,G}(x) \equiv 1$ for all $x\in{\tilde{G}}=\{x \mid \sum_{l=1}^M I_G(x_l)>0\}$. This ensures again that we will have zero mean residual over $F$ and $G$.  For the regression estimates in the extended model we have

\begin{equation}\label{regbettaclusterext}
\hat{\pmb{\gamma}}_{c,2}=(\pmb{\mathcal{A}}_{c,2}^{(1)})^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}^{(1)}_c(x)Y(x)\Big)
\end{equation}
with $\pmb{\mathcal{A}}_{c,2}^{(1)}=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}^{(1)}_c(x)\pmb{\mathcal{Z}}^{(1)t}_c(x)$. Likewise we get
\begin{equation}\label{regbettaclusterext}
\hat{\pmb{\theta}}_{c,2}=\pmb{\mathcal{A}}^{-1}_{c,2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}_c(x)Y(x)\Big)
\end{equation}
with $\pmb{\mathcal{A}}_{c,2}=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}^t_c(x)$.\\
We define as in [\ref{y0gregcluster}] the generalized small-area estimator by
\begin{equation}\label{extclustersmallareaY_0greg}
\hat{\tilde{Y}}_{c,G,g3reg}
= (\hat{\bar{\pmb{\mathcal{Z}}}}_{c,0,G}^{(1)}-\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^{(1)})^t\hat{\pmb{\gamma}}_{c,2} +
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^t\hat{\pmb{\theta}}_{c,2}
\end{equation}
where we have set
\begin{eqnarray*}
 \hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{c,0,G}&=&\frac{\sum_{x\in{s_{0,G}}}M(x)\mathcal{Z}_c^{(1)}(x)}{\sum_{x\in{s_{0,G}}}M(x)} \\ \hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^{(1)}&=&
 \frac{\sum_{x\in{s_{1,G}}}M(x)\pmb{\mathcal{Z}}_c^{(1)}(x)}{\sum_{x\in{s_{1,G}}}M(x)} \\
 \hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}&=&
 \frac{\sum_{x\in{s_{1,G}}}M(x)\pmb{\mathcal{Z}}_c(x)}{\sum_{x\in{s_{1,G}}}M(x)}
\end{eqnarray*}
The estimated design-based covariance matrix are now
\begin{eqnarray}\label{gammathetaclusterextestvar}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_{c,2}}&=&
(\pmb{\mathcal{A}}^{(1)}_{c,1})^{-1}\Big(\frac{1}{n^2_2}\sum_{\in{s_2}}M^2(x)\hat{\mathcal{R}}_{1,c}^2(x)
\pmb{\mathcal{Z}}^{(1)}_c(x)\pmb{\mathcal{Z}}^{(1)t}_c(x)\Big)(\pmb{\mathcal{A}}^{(1)}_{c,1})^{-1}\nonumber \\
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,2}}&=&
\pmb{\mathcal{A}}^{-1}_{c,2}\Big(\frac{1}{n^2_2}\sum_{\in{s_2}}M^2(x)\hat{\mathcal{R}}_c^2(x)
\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}^t_c(x)\Big)\pmb{\mathcal{A}}^{-1}_{c,2}
\end{eqnarray}
\noindent where
\begin{eqnarray*}
\hat{\mathcal{R}}_{1,c}(x)&=&Y_c(x)-\hat{\tilde{Y}}_{1,c}(x):=Y_c(x)-\pmb{\mathcal{Z}}^{(1)t}_c(x)\hat{\pmb{\gamma}}_2\\
\hat{\mathcal{R}}_{c}(x)&=&Y_c(x)-\hat{\tilde{Y}}_c(x):=Y_c(x)-\pmb{\mathcal{Z}}^{t}_c(x)\hat{\pmb{\theta}}_2
\end{eqnarray*}
We obtain as in [\ref{estvarclustery0greg}] the estimated design-based variance

\begin{eqnarray}\label{extendedclustersmallestvariance}
\hat{\VAR}_{0,1,2}(\hat{Y}_{c,G,g3reg}) &=&\hat{\pmb{\gamma}}^t_{c,2}\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,0,G}^{(1)}}\hat{\pmb{\gamma}}^t_{c,2} \nonumber \\
&+& \frac{n_2}{n_1}\hat{\bar{\pmb{\mathcal{Z}}}}_{c,0,G}^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\gamma}}_{c,2}}
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,0,G}^{(1)}+(1-\frac{n_2}{n_1})\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,2}}
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}
\end{eqnarray}
\noindent where
\begin{equation*}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,0,G}^{(1)}}=
\frac{1}{n_{0,G}(n_{0,G}-1)}\sum_{x\in{s_{0,G}}}\Big(\frac{M(x)}{\bar{M}_0}\Big)^2
(\pmb{\mathcal{Z}}^{(1)}_c(x)-\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{c,0,G})(\pmb{\mathcal{Z}}^{(1)}_c(x)-\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)}_{c,0,G})^t
\end{equation*}
is the empirical covariance of $\hat{\bar{\pmb{\mathcal{Z}}}}_{c,0,G}^{(1)}$ over $G$.\\
The first term in [\ref{extendedclustersmallestvariance}] can be rewritten as the variance of the predictions, that is
\begin{equation*}
\hat{\pmb{\gamma}}^t_{c,2}\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,0,G}^{(1)}}\hat{\pmb{\gamma}}^t_{c,2}
=\frac{1}{n_{0,G}(n_{0,G}-1)}\sum_{x\in{s_{0,G}}}\Big(\frac{M(x)}{\bar{M}_0}\Big)^2
(\hat{\tilde{Y}}_{1,c}(x)-\bar{\tilde{Y}}_{1,c})^2
\end{equation*}
\noindent with $\bar{\tilde{Y}}_{1,c}=\frac{\sum_{x\in{s_{0,G}}}M(x)\hat{\tilde{Y}}_{1,c}(x)}{\sum_{x\in{s_{0,G}}}M(x)}$. \\
The external version of the variance estimate can be easily adapted from [\ref{extsmallareaestvariance3}]: replace all occurring expressions $U(x)$ by the cluster means $U_c(x)$, calculate the mean values according to
$\bar{U}=\frac{\sum_x M(x)Uc_(x)}{\sum_x M(x)}$ and introduce the weights $\Big(\frac{M(x)}{\bar{M}_k}\Big)^2$.\\


\section{Generalization to two-stage sampling at the plot level}
In many applications costs to measure the response variable $Y_i$ are high. For instance,
a good determination of the volume may require that one records
$DBH$, as well as the diameter at $7m$ above ground and total height
in order to utilize a three-way volume function. However, one could rely
on a coarser, but cheaper, approximation of the volume based only on
$DBH$. Nonetheless, it may be most sensible to assess those three
parameters only on a sub-sample of trees. We now briefly formalize this
simple idea, which is used in the Swiss National Forest Inventory. The reader is referred to (\cite{mandallaz}, section 4.4, 4.5, 5.4 and 9.5) for details. For each point $x\in{s_2}$ trees are drawn with
probabilities $\pi_i$. The set of selected trees is denoted by
$s_{2}(x)$. From each of the selected trees $i\in{s_{2}(x)}$ one
gets an approximation $Y_i^*$ of the exact value $Y_i$. From the
finite set $s_{2}(x)$ one draws a sub-sample
$s_{3}(x)\subset{s_{2}(x)}$ of trees by Poisson sampling. For each tree $i\in{s_{3}(x)}$
one then measures the exact variable $Y_i$. Let us now define the
second stage indicator variable
\begin{equation}
 J_i(x)=\begin{cases}&1 \text{ if $i\in s_{3}(x)$}\\
                      &0 \text{ if $i\not\in s_{3}(x)$}
         \end{cases}
\end{equation}

 \par To construct a good point estimate, we must have
 the residual $R_i=Y_i-Y_i^*$ which is known only for trees
 $i\in{s_{3}(x)}$. The generalized local density $Y^*(x)$ is defined
 according to    \index{Generalized local density}
 \begin{eqnarray}\label{gdens}
 Y^*(x)&=&\frac{1}{\LF}\left( \sum_{i=1}^N \frac{I_{i}(x)Y_i^*}{\pi_i} +
 \sum_{i=1}^N \frac{I_{i}(x)J_{i}(x)R_i}{\pi_{i}p_i}\right)\nonumber \\
 &=&\frac{1}{\LF}\left( \sum_{i\in{s_{2}(x)}} \frac{Y_i^*}{\pi_i}
 +\sum_{i\in{s_{3}(x)}} \frac{R_i}{\pi_{i}p_i}\right)
 \end{eqnarray}
 where the $p_i$ are the conditional inclusion probabilities for the the second stage sampling, i.e. $p_i=\PR(J_i(x)=1 \mid I_i(x)=1)$.\\
  One can use all the previous results by making the following changes:\\
  \begin{itemize}
  \item
   $Y(x)\rightarrow Y^{*}(x)$
    \item $\hat{\pmb{\beta}}\rightarrow \hat{\pmb{\beta}}_2^{*}=\pmb{A}_2^{-1}\Big(\frac{1}{n_2}\sum_{x\in_{s_2}}Y^{*}(x)\pmb{Z}(x)\Big)$,
  $\hat{\pmb{\alpha}}_2^{*}=(\pmb{A}^{(1)}_2)^{-1}\Big(\frac{1}{n_2}\sum_{x\in_{s_2}}Y^{*}(x)\pmb{Z}^{(1)}(x)\Big)$
\item
  $\hat{R}_1(x)\rightarrow\hat{R}_1^{*}(x)=Y^{*}(x)-\pmb{Z}^{(1)t}(x)\hat{\pmb{\alpha}}_2^{*}$,
   $\hat{R}(x)\rightarrow \hat{R}^{*}(x)=Y^{*}(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_2^{*}$
    \item
   Similarly in cluster sampling with $Y(x_l)\rightarrow Y^{*}(x_l)$, $Y_c(x)\rightarrow Y_c^{*}(x)$ etc.
   \end{itemize}
 One obtains for the resulting three-phase two-stage estimator $\hat{Y}^{*}_{F,g3reg}$ the theoretical variance
 \begin{equation}\label{3phasetwostegevariance}
 \VAR_{0,1,2,3}(\hat{Y}^{*}_{F,g3reg})=\VAR_{0,1,2,3}(\hat{Y}_{F,g3reg})+\frac{1}{n_2}\EX_{x\in{F}}V(x)
 \end{equation}
 \noindent where $\EX_{x\in{F}}V(x)=\frac{1}{\lambda^2(F)}\sum_{i=1}^N\frac{R_i^2(1-p_i)}{\pi_ip_i}$ is the second stage variance. The subscript $3$ in $\VAR_{0,1,2,3}$ refers to the second-stage random selection at points $x\in{s_2}$.\\
 The proof of [\ref{3phasetwostegevariance}] is straightforward under the external model assumption (including the small-area version), it suffices to use the techniques presented in (\cite{mandallaz}, section 4.4 pp. 71-73, section 4.5 pp. 82-83), which also show that one can use the variance estimates [\ref{extsmallareaestvariance2}] and [\ref{extsmallareaestvariance3}] (valid also for $G=F$), after making the afore mentioned changes $Y(x)\rightarrow Y^{*}(x)$ etc. The second-stage variance is automatically taken into account. The same applies to cluster sampling (by using \cite{mandallaz}, section 5.3 pp. 86-83). Because of the algebraic asymptotic equivalence established in section \ref{proofofequivalence} of the Appendix this will also hold true for all the g-weights variance estimates.

 \bibliography{biblio1}
\begin{appendix}
\clearpage
\newpage
\section{Variance of $\hat{Y}_{F,g3reg}$}\label{proofofvariance}
According to \cite{mandallaz} (pp 124-125) the asymptotic covariance matrices of the regression coefficients are given
by
\begin{equation}\label{lemma1}
\pmb{\Sigma}_{\hat{\pmb{\beta}}_k}=\pmb{A}^{-1}
\Big(\frac{1}{n_k}\EX R^2(x)\pmb{Z}(x)\pmb{Z}^t(x)\Big)\pmb{A}^{-1}
\end{equation}
\begin{equation}\label{lemma2}
\pmb{\Sigma}_{\hat{\pmb{\alpha}}_k}=(\pmb{A}^{(1)})^{-1}
\Big(\frac{1}{n_k}\EX R_1^2(x)\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)\Big)(\pmb{A}^{(1)})^{-1}
\end{equation}
\noindent where $\pmb{A}=\EX_x \pmb{Z}(x)\pmb{Z}^t(x)$  and $\pmb{A}^{(1)}=\EX_x \pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)$, which can be consistently estimated by $\pmb{A}_k$ ($k=1,2$) and  $\pmb{A}_k^{(1)}$ ($k=0,1,2$). To ensure the calibration properties we will use $\pmb{A}_2$ and $\pmb{A}_1^{(1)}$.\\
We start with the calculation of
$T_3=\VAR_0(\EX_{1 \mid 0}\EX_{1,2 \mid 0}(\hat{Y}_{F,g3reg}))\approx \VAR_0(\hat{\bar{\pmb{Z}}}_0^{(1t)}\hat{\pmb{\alpha}}_0)$. Since
$\EX_0(\hat{\bar{\pmb{Z}}}_0^{(1t)}\hat{\pmb{\alpha}}_0) \approx \bar{\pmb{Z}}^{(1t)}\pmb{\alpha}=\bar{Y}$
we consider
\begin{equation*}
\Delta=\hat{\bar{\pmb{Z}}}^{(1)t}\hat{\pmb{\alpha}}_{0}-\bar{\pmb{Z}}^{(1)t}\pmb{\alpha}=
\hat{\bar{\pmb{Z}}}_0^{(1)t}(\hat{\pmb{\alpha}}_{0}-\pmb{\alpha})+ (\hat{\bar{\pmb{Z}}}_0^{(1)t}-\bar{\pmb{Z}}^{(1)t})\pmb{\alpha}
\end{equation*}

Asymptotically we get
\begin{equation*}
\EX \Delta^2=\bar{\pmb{Z}} ^{(1)t}\pmb{\Sigma}_{\hat{\pmb{\alpha}}_{0}}\bar{\pmb{Z}}^{(1)}+
\pmb{\alpha}^t\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}^{(1)}_{0}}\pmb{\alpha}+
2\EX\Big((\hat{\pmb{\alpha}}_{0}-\pmb{\alpha})^t\hat{\bar{\pmb{Z}}}^{(1)}_{0}(\hat{\bar{\pmb{Z}}}^{(1)}_{0}
-\bar{\pmb{Z}}^{(1)})^t\pmb{\alpha}\Big)
\end{equation*}
\noindent where $\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}^{(1)}_{0}}$ is the covariance matrix of
$\hat{\bar{\pmb{Z}}}^{(1)}_{0}$. The first two terms are of order $n_0^{-1}$. For the third term we note that
$\EX (\hat{\bar{\pmb{Z}}}^{(1)}_{0}(\hat{\bar{\pmb{Z}}}^{(1)}_{0}-\bar{\pmb{Z}}^{(1)})^t)$ is equal to the covariance matrix $\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}^{(1)}_{0}}$ and therefore of order $n_0^{-1}$ and that
$\hat{\pmb{\alpha}}_{0}-\pmb{\alpha}$ is of order $n_0^{-\frac{1}{2}}$. The last term is therefore of smaller order than the first two. Since $\pmb{\Sigma}_{\hat{\alpha}_0}=\frac{n_2}{n_0}\pmb{\Sigma}_{\hat{\alpha}_2}$ we get the following asymptotically consistent estimate
\begin{equation}\label{appendixhatt3}
\hat{T}_3 =
\frac{n_2}{n_0}\hat{\bar{\pmb{Z}}}^{(1)t}_{0}\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_{2}}\hat{\bar{\pmb{Z}}}^{(1)}_{0}
+ \hat{\pmb{\alpha}}_{2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}^{(1)}_{0}}\hat{\pmb{\alpha}}_{2}
\end{equation}


We have to calculate
\begin{equation*}
T_2=\EX_0\VAR_{1 \mid 0}\EX_{2\mid 0,1}(\hat{Y}_{F,g3reg})\approx \EX_0\VAR_{1 \mid 0}(\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\alpha}}_1)
\end{equation*}
because, as we have seen, $\EX_{2\mid 0,1}(\hat{Y}_{F,g3reg})\approx \hat{\bar{\pmb{Z}}}^{(1)t}_0\hat{\pmb{\alpha}}_1$.
 First, we note that
\begin{eqnarray*}
 \VAR_{1 \mid 0}(\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\alpha}}_1)&=&
\EX_{ 1\mid 0}(\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\alpha}}_1-\EX_{1 \mid 0}\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\alpha}}_1)^2 \nonumber\\
& \approx & \EX_{1 \mid 0}(\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\alpha}}_1-\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\alpha}}_0)^2 \nonumber \\
&=& \hat{\bar{\pmb{Z}}}_0^{(1)t}\EX_{1 \mid 0}(\hat{\pmb{\alpha}}_1-\hat{\pmb{\alpha}}_0)(\hat{\pmb{\alpha}}_1-\hat{\pmb{\alpha}}_0)^t\hat{\bar{\pmb{Z}}}_0^{(1)t}
\end{eqnarray*}
\noindent Using the Taylor expansion given in \cite{mandallaz} (p. 124, but at point $(\pmb{A}^{(1)}_0,\;\hat{\pmb{\alpha}}_0)$ instead of $(\pmb{A}^{(1)},\;\pmb{\alpha}))$ we get (with the notation defined in [\ref{coeff1}])
\begin{eqnarray*}
\hat{\pmb{\alpha}}_1-\hat{\pmb{\alpha}}_0 &\approx &
-(\pmb{A}^{(1)}_0)^{-1}\big(\pmb{A}^{(1)}_1-\pmb{A}^{(1)}_0\big)(\pmb{A}^{(1)}_0)^{-1}\pmb{U}_0+
(\pmb{A}^{(1)}_0)^{-1}(\pmb{U}_1-\pmb{U}_0) \nonumber \\
&=& (\pmb{A}^{(1)}_0)^{-1}(\pmb{U}_1-\pmb{A}^{(1)}_1\hat{\pmb{\alpha}}_0)
\end{eqnarray*}
Since $\hat{\pmb{\alpha}}_0=\pmb{\alpha} + O(n_0^{-\frac{1}{2}})$ in probability this is asymptotically equivalent
to
\begin{eqnarray}\label{tayloralpha0}
\hat{\pmb{\alpha}}_1-\hat{\pmb{\alpha}}_0 & \approx &
(\pmb{A}^{(1)}_0)^{-1}\Big(\frac{1}{n_1}\sum_{x\in{s_1}}(Y(x)-\pmb{Z}^{(1)t}(x)\pmb{\alpha})\pmb{Z}^{(1)}(x)\Big)\nonumber \\
&=&(\pmb{A}^{(1)}_0)^{-1}\Big(\frac{1}{n_1}\sum_{x\in{s_1}}R_1(x)\pmb{Z}^{(1)}(x)\Big)
\end{eqnarray}
With the indicator variables $J(x)=1$ if $x\in{s_0}$ and $J(x)=0$ if $x\not\in{s_0}$ we obtain
\begin{eqnarray*}
\EX_{1 \mid 0}(\hat{\pmb{\alpha}}_1-\hat{\pmb{\alpha}}_0 )(\hat{\pmb{\alpha}}_1-\hat{\pmb{\alpha}}_0)^t & \approx &
(\pmb{A}^{(1)}_0)^{-1}\frac{1}{n^2_1}\EX_{1 \mid 0}\sum_{x\in{s_0}}J(x)R_1^2(x)\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)\nonumber \\
&+& (\pmb{A}^{(1)}_0)^{-1}\frac{1}{n^2_1}\EX_{1 \mid 0}\sum_{x\ne y\in{s_0}}J(x)J(y)R_1(x)\pmb{Z}^{(1)}(x)R_1(y)
\pmb{Z}^{(1)t}(y)
\end{eqnarray*}
Using simple random sampling within the set $s_0$ we have
\begin{eqnarray*}
\EX_{1 \mid 0}J(x)&=&\frac{n_1}{n_0}\nonumber \\
\EX_{1 \mid 0}J(x)J(y)&=& \frac{n_1(n_1-1)}{n_0(n_0-1)} \\
\sum_{x\ne y\in{s_0}}J(x)J(y)R_1(x)\pmb{Z}^{(1)}(x)R_1(y)
\pmb{Z}^{(1)t}(y)&=&(\sum_{x\in{s_0}}J(x)R_1(x)\pmb{Z}^{(1)}(x))^2 \\
&-&\sum_{x\in{s_0}}R^2_1(x)
\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x) \\
\EX_0\EX_{1 \mid 0}\sum_{x\in{s_0}}J(x)R_1(x)\pmb{Z}^{(1)}(x)&=&0\quad  \text{(orthogonality relationship)}
\end{eqnarray*}
With the approximation $\frac{n_1-1}{n_0-1}\approx \frac{n_1}{n_0}$ we obtain after tedious but simple algebra
\begin{eqnarray}\label{appendixt2}
T_2=(1-\frac{n_1}{n_0})\bar{\pmb{Z}}^{(1)t}(\pmb{A}^{(1)})^{-1}\Big(\frac{1}{n_1}\EX_x( R^2_1 (x)\pmb{Z}^{(1)}(x)
\pmb{Z}^{(1)t}(x))\Big)(\pmb{A}^{(1)})^{-1}\bar{\pmb{Z}}^{(1)}
\end{eqnarray}
\noindent and the 'plug-in' asymptotically consistent estimate

\begin{eqnarray}\label{appendixhatt2}
\hat{T}_2 & \approx &
(1-\frac{n_1}{n_0})\frac{n_2}{n_1}\hat{\bar{\pmb{Z}}}_0^{(1)t}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}\hat{\bar{\pmb{Z}}}_0^{(1)} \nonumber \\
&\approx & (1-\frac{n_1}{n_0})\frac{n_2}{n_1}\hat{\bar{\pmb{Z}}}_0^{(1)t}(\pmb{A}^{(1)}_1)^{-1}\Big(\frac{1}{n_2^2}
\sum_{x\in{s_2}}\hat{R}^2_1(x)\pmb{Z}^{(1)}(x)\pmb{Z}^{(1)t}(x)\Big)(\pmb{A}^{(1)}_1)^{-1}\hat{\bar{\pmb{Z}}}_0^{(1)}
\end{eqnarray}
We now proceed to calculate the first term $T_1$. First we note that
\begin{equation}\label{equivalence2}
\hat{Y}_{F,g3reg}-\EX_{2 \mid 0,1}(\hat{Y}_{F,g3reg})\approx (\hat{\bar{\pmb{Z}}}^{(1)}_0-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t
(\hat{\pmb{\alpha}}_{2}-\hat{\pmb{\alpha}}_{1})+\hat{\bar{\pmb{Z}}}^{t}_1(\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1)
\approx \hat{\bar{\pmb{Z}}}^{t}_1(\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1)
\end{equation}
The last asymptotic equivalence in [\ref{equivalence2}] holds because  $(\hat{\bar{\pmb{Z}}}^{(1)}_0-\hat{\bar{\pmb{Z}}}^{(1)}_1)^t(\hat{\pmb{\alpha}}_{2}-\hat{\pmb{\alpha}}_{1})$ is the product of two terms of order $O(n_2^{-\frac{1}{2}})$ in probability whereas the second is of order $O(n_2^{-\frac{1}{2}})$. One gets therefore
\begin{eqnarray*}
\VAR_{2 \mid 0,1}(\hat{Y}_{F,g3reg}-\EX_{2 \mid 0,1}(\hat{Y}_{F,g3reg})) &\approx &
\hat{\bar{\pmb{Z}}}^{t}_1\EX_{2 \mid 0,1}(\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1)(\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1)^t\hat{\bar{\pmb{Z}}}_1 \nonumber \end{eqnarray*}
Using mutatis mutandis [\ref{tayloralpha0}] (i.e. Taylor expansion at $\hat{\pmb{\beta}}_1$) we have
\begin{equation}\label{taylorbeta1}
\hat{\pmb{\beta}}_2-\hat{\pmb{\beta}}_1  \approx \pmb{A}^{-1}_1\Big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\Big)
\end{equation}
Using the same techniques as for $T_2$ (simple random sampling within the set $s_1$ instead of $s_0$)
we have
\begin{eqnarray}\label{appendixt1}
T_1=(1-\frac{n_2}{n_1})\bar{\pmb{Z}}^{t}\pmb{A}^{-1}\Big(\frac{1}{n_2}\EX_x( R^2 (x)\pmb{Z}(x)
\pmb{Z}^{t}(x))\Big)\pmb{A}^{-1}\bar{\pmb{Z}}
\end{eqnarray}
and
\begin{equation}\label{appendixt1}
T_1 \approx (1-\frac{n_2}{n_1})\bar{\pmb{Z}}^{t}\pmb{\Sigma}_{\pmb{\hat{\beta}}_2}\bar{\pmb{Z}}
\end{equation}
which leads to the 'plug-in' asymptotically consistent estimate
\begin{eqnarray}\label{appendixhatt1}
\hat{T}_1 &\approx& (1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}_1^{t}\hat{\pmb{\Sigma}}_{\pmb{\hat{\beta}}_2}\hat{\bar{\pmb{Z}}}_1\nonumber \\
&=&(1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}_1^{t}\pmb{A}_2^{-1}\big(\frac{1}{n^2_2}\sum_{x\in{s_2}}
\hat{R}^2(x)\pmb{Z}(x)\pmb{Z}^t(x)\Big)\pmb{A}_2^{-1}\hat{\bar{\pmb{Z}}}_1
\end{eqnarray}
Hence, one obtains the asymptotically consistent estimate of the variance
\begin{equation*}\label{estvary0greg1}
\hat{\VAR}(\hat{Y}_{F,g3reg}) \approx  \hat{T}_3 + \hat{T}_2 + \hat{T}_1
\end{equation*}
Because two terms cancel out we finally obtain
\begin{equation}\label{estvary0greg2}
\hat{\VAR}(\hat{Y}_{F,g3reg}) \approx  \hat{\pmb{\alpha}}_2^t\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}_0}\hat{\pmb{\alpha}}_2
+ \frac{n_2}{n_1}\hat{\bar{\pmb{Z}}}_0^{(1)t}\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}\hat{\bar{\pmb{Z}}}_0^{(1)}
+ (1-\frac{n_2}{n_1})\hat{\bar{\pmb{Z}}}_1^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\alpha}}_2}\hat{\bar{\pmb{Z}}}_1
\end{equation}
Since the variance-covariance matrix of the $\pmb{Z}^{(1)}(x)$ is given by
\begin{equation*}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}^{(1)}_{0}}=
\frac{1}{n_{0}}\frac{\sum_{x\in{s_{0}}}(\pmb{Z}^{(1)}(x)-
\hat{\bar{\pmb{Z}}}^{(1)}_{0})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}^{(1)}_{0})^t}{n_{0}-1}
\end{equation*}
it is easily checked that the first term in [\ref{estvary0greg2}] is equal to the variance of the predictions under the reduced model, i.e.
\begin{equation*}
\frac{1}{n_0}\frac{\sum_{x\in{s_0}}(\hat{Y}_1(x)-\hat{\bar{Y}}_1)^2}{n_0-1}
\end{equation*}
\noindent with $\hat{Y}_1(x)=\pmb{Z}^{(1)t}(x)\hat{\pmb{\alpha}}_2$ and
$\hat{\bar{Y}}_1=\frac{1}{n_0}\sum_{x\in{s_0}}\hat{Y}_1(x)$.

\section{Asymptotic equivalence of variance estimates for small areas} \label{proofofequivalence}
We define the g-weights
\begin{eqnarray}\label{smallgweights}
\tilde{g}(x)&=& \hat{\bar{\pmb{\mathcal{Z}}}}^{t}_{1,G}\pmb{\mathcal{A}}_2^{-1}\pmb{\mathcal{Z}}(x) \nonumber \\
\tilde{g}^{(1)}(x)&=&\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)t}_{0,G}(\pmb{\mathcal{A}}^{(1)}_1)^{-1}\pmb{\mathcal{Z}}^{(1)}(x)
\end{eqnarray}
First, we note that [\ref{estvarianceY_G0greg}] can be rewritten as
\begin{eqnarray*}
\hat{\VAR}_{0,1,2}(\hat{Y}_{G,g3reg})&=&\frac{1}{n_{0,G}(n_{0,G}-1)}\sum_{x\in{s_{0,G}}}
(\hat{Y}_1(x)-\hat{\bar{Y}}_{1,G})^2 \\
&+& \frac{1}{n_1n_2}\sum_{x\in{s_2}} (\tilde{g}^{(1)}(x))^2\hat{\mathcal{R}}_1^2(x)
+(1-\frac{n_2}{n_1})\frac{1}{n_2^2}\sum_{x\in{s_2}}\tilde{g}^2(x)\hat{\mathcal{R}}^2(x)
\end{eqnarray*}
Second, we consider
\begin{equation*}
\hat{\bar{\pmb{\mathcal{Z}}}}^{(1)t}_{0,G}\hat{\pmb{\gamma}}_1=\frac{1}{n_1}\sum_{x\in{s_1}}\tilde{g}^{(1)}(x)Y(x)\approx
\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}Y(x)
\end{equation*}
 \noindent because all terms converge towards $\bar{Y}_G$ (the fact that they are not observable is irrelevant in this respect). Hence, we see that $\tilde{g}^{(1)}(x) \to 0$ for $x\not\in{G}$ and $\tilde{g}^{(1)}(x)\to \frac{n_1}{n_{1,G}}$ for $x\in{G}$. Likewise we have
\begin{equation*}
\hat{\bar{\pmb{\mathcal{Z}}}}^{t}_{1,G}\hat{\pmb{\theta}}_2=\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}(x)Y(x)\approx
\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}Y(x)
\end{equation*}
\noindent so that $g(x) \to 0$ for $x\not\in{G}$ and $g(x)\to \frac{n_2}{n_{2,G}}$ for $x\in{G}$.  Note that the behavior of the g-weights for $G$ and for $F$ is completely different. \\
Since obviously $\frac{n_{2,G}}{n_{1,G}}\approx \frac{n_2}{n_1}$ we get
\begin{eqnarray}
\hat{\VAR}_{0,1,2}(\hat{Y}_{G,g3reg})&\approx&\frac{1}{n_{0,G}(n_{0,G}-1)}\sum_{x\in{s_{0,G}}}
(\hat{Y}_1(x)-\hat{\bar{Y}}_{1,G})^2 \\
&+& \frac{1}{n_{1,G}n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{\mathcal{R}}_1^2(x)
+(1-\frac{n_{2,G}}{n_{1,G}})\frac{1}{n_{2,G}^2}\sum_{x\in{s_{2,G}}}\hat{\mathcal{R}}^2(x)
\end{eqnarray}
\noindent Finally, using the relation between $\hat{\pmb{\beta}}_2$ and $\hat{\pmb{\theta}}_2$ given in equation [42]
of \cite{mandallazreport1}, we see that $\hat{\mathcal{R}}(x)=\hat{R}(x)-\phi(x)\hat{\bar{R}}_G$ with $\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\phi(x)=1$, and likewise $\hat{\mathcal{R}}_1(x)=\hat{R}_1(x)-\phi_1(x)\hat{\bar{R}}_{1,G}$ with $\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\phi_1(x)=1$. Asymptotically we can replace $\hat{\mathcal{R}}(x)$ by
$\hat{R}(x)-\hat{\bar{R}}_G$ and $\hat{\mathcal{R}}_1(x)$ by
$\hat{R}_1(x)-\hat{\bar{R}}_{1,G}$, which leads to
$\hat{\VAR}_{0,1,2}(\hat{Y}_{G,g3reg})\approx \hat{\VAR}_{ext3}(\hat{Y}_{G,g3reg})$ and the asymptotic equivalence
of external and g-weights variance estimates.

\end{appendix}

\end{document}









