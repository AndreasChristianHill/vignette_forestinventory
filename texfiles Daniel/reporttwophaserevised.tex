\documentclass[a4paper,12pt,leqno, titlepage]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{a4}
\usepackage{graphicx}
\usepackage{flafter}
\usepackage{bm}
\usepackage{setspace}
\usepackage{lineno}
\usepackage{natbib}
\bibliographystyle{mystyle2}
\newcommand{\LF}{\ensuremath{\lambda(F)}}
\newcommand{\LFC}{\ensuremath{\lambda^2(F)}}
\newcommand{\EX}{\mathbb{E}}
\newcommand{\PR}{\mathbb{P}}
\newcommand{\VAR}{\mathbb{V}}
\newcommand{\COV}{\mathbb{COV}}
\newcommand{\MAV}{\mathbb{MAV}}
\newcommand{\MRAV}{\mathbb{MRAV}}
\newcommand{\POP}{\mathcal{P}}
\newcommand{\SAMP}{\mathcal{S}}
\newcommand{\RE}{\mathbb{RE}}
\newcommand{\PLAN}{\Re^2}
\newcommand{\SUR}{\mathbb{S}}
\newcommand{\ING}{\mathbb{I}}
\newcommand{\DEP}{\mathbb{D}}
\setlength{\topmargin}{0cm}
\setlength{\headheight}{3mm}
\setlength{\headsep}{1cm}
\setlength{\topskip}{0cm}
\setlength{\textwidth}{16cm}
\setlength{\textheight}{22cm}
\setlength{\oddsidemargin}{0cm}
\makeatletter
\def\tagform@#1{\maketag@@@{[\ignorespaces#1\unskip\@@italiccorr]}}
\makeatother




\begin{document}
\doublespacing
\nolinenumbers
\pagestyle{empty}

\title{Design-based properties of some small-area estimators in forest inventory with two-phase sampling \\ \small{revised version}}
\author{Daniel Mandallaz, Andreas Hill, Alexander Massey \\Deparment of Environmental Systems Science \\Chair of Land Use Engineering\\ETH Zurich\\CH 8092 Zurich, Switzerland}
\date{January 2016}

\maketitle
\newpage
$ $
\newpage
\begin{abstract}
We consider the small-area estimation problem in forest inventories with two-phase sampling schemes. We propose an improvement of the synthetic estimator, when the true mean of the auxiliary variables over the small-area is unknown and must be estimated, likewise for the residual corrected small-area estimator. We derive the asymptotic design-based variances of these new estimators, \textbf{the pseudo-synthetic and pseudo small-area estimators}, by incorporating also the design-based variance of the regression coefficients. We then propose a very simple mathematical device that transforms pseudo small-area estimators into pseudo-synthetic estimators, which is very convenient to derive asymptotic variances. The results are extended to cluster and two-stage sampling at the plot level. To illustrate the theory we consider the case of post-stratification and a case study.
\begin{center}
\textbf{R\'{e}sum\'{e}}
\end{center}
\begin{sloppypar}
Nous consid\`{e}rons le probl\`{e}me de l'estimation pour petits domaines dans le contexte d'inventaires forestiers en deux phases. Nous proposons une am\'{e}lioration simple de l'estimateur synth\'{e}tique quand la moyenne des variables auxiliaires dans le petit domaine doit \^{e}tre estim\'{e}e en premier lieu, de m\^{e}me pour l'estimateur pour petit domain bas\'{e} sur les r\'{e}sidus. Nous calculons la variance sous le plan de sondage de ces nouveaux estimateurs  en tenant compte de la variance des coefficients de r\'{e}gression.  De plus, nous proposons un artifice math\'{e}matique qui permet de transformer un estimateur pour petit domaine en un estimateur synth\'{e}tique, ce qui simplifie le calcul de la variance asymptotique. L'extension aux sondages par satellites et ï¿½ deux degr\'{e}s au niveau de la placette est aussi trait\'{e}e. La th\'{e}orie est illustr\'{e}e par la post-stratification et par une \'{e}tude de cas.
\end{sloppypar}
\end{abstract}
\clearpage
\section*{Foreword}
The first version of this technical report has been published in the e-collection in September 2012. Since then many technical reports on further developments have been published in the e-collection
 (\cite{mandallazreport2,mandallazreport3,mandallazreport4,mandallazreport5,masseyphd}) as well as papers in the Canadian Journal of Forest Research, CJFR, (\cite{mandallaz3,mandallaz4,mandallaz5,massey1,massey2,massey3}. Recently, Dr. Alexander Massey and Andreas Hill, MSc, started writing an R-package implementing the various procedures discussed in the aforementioned publications.  Writing this software revealed some minor notational ambiguities used in cluster sampling as well as small numerical discrepancies between the new case study results produced in R and the original ones obtained with the IML procedure of the SAS package.  Detective work revealed that some SAS programs contained a minor error in a small-area variance formulae that caused the mean number of points per cluster in the small area to be taken from the second-phase instead of from the first-phase sample.  We emphasize that the underlying concept, i.e. "borrowing strength" by estimating regression coefficients over an unrestricted large area to improve the calculation of point and variance estimates in a restricted small area, remains precisely the same and that all the original conclusions of this technical report, including all relevant implications on subsequent works, remain valid.  This revised version improves the notation used for cluster sampling, presents updated tables with the corrected results for the case study and provides an enhanced discussion of the results. Some comments have been added to take into account results obtained since 2012. The mathematical derivations have also been carefully checked and, so far, confirmed.


\newpage
\pagestyle{plain}
\pagenumbering{arabic}
\section{Introduction}\label{introduction}
\pagenumbering{arabic} \setcounter{page}{1}
There is an extensive literature on the problem of small area estimation (or small domain estimation in general sampling). In this paper we shall investigate the properties of some estimators in the \textbf{model-assisted framework}, in which prediction models are used to improve the efficiency but are not assumed to be correct as in the \textbf{model-dependent approach}. The validity of the statistical procedures is ensured by the randomization principle: i.e. we are in the \textbf{design-based} inference framework, which has a definite advantage in official statistics. The reader is referred to (\cite{koehl}, section 3.8) for a good review of small-area estimation in forest inventory that presents alternative techniques, in particular Bayesian. Let us now define the sampling scheme.\par
The \textbf{first phase} draws a large sample $s_1$ of $n_1$ points that are independently and uniformly distributed within the forest area $F$. At each point $x\in{s}_1$ auxiliary
information is collected, very often coding information of qualitative nature
(e.g. following the  interpretation of aerial photographs) or quantitative (e.g. timber volume estimates  based on LIDAR measurements). We shall assume that the auxiliary information at point $x$ is described by the column vector $\mathbf{Z}(x)\in{\Re^{p}}$. \par
 The \textbf{second phase} draws a small sample $s_2\subset{s_1}$ of
$n_2$ points from $s_1$ according to \textbf{equal probability
sampling without replacement}. In the forested area $F$ we consider a well-defined population $ \mathcal{P}$ of $N$ trees with response variable
 $Y_i,\;i=1,2 \ldots$, e.g. the timber volume.  \textbf{The objective is to estimate the overall spatial mean}  $\bar{Y}=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i$, where $\lambda(\cdot)$ denotes the surface area (usually in ha) and \textbf{the mean over a small area} $G\subset F$, defined as
 \begin{equation}\label{small1}
 \bar{Y}_G=\frac{1}{\lambda(G)}\sum_{i=1}^N I_G(i)Y_i=:\frac{1}{\lambda(G)}\sum_{i\in{G}}Y_i
 \end{equation}
 where the indicator variable $I_G(i)$ is $1$ if the $i$-th tree lies in $G$, and $0$ otherwise.\\
 For each point $x\in{s_2}$ trees are drawn from the population $\mathcal{P}$ with probabilities $\pi_i$, for instance with concentric circles or angle count techniques. The
set of trees selected at point $x$ is denoted by $s_{2}(x)$. From each of the
selected trees $i\in{s_{2}(x)}$ one determines $Y_i$. The indicator variable $I_i$ is defined as
\begin{equation}\label{1stage}
 I_i(x)=\begin{cases}&1 \text{ if $i\in s_{2}(x)$}\\
                      &0 \text{ if $i\not\in s_{2}(x)$}
         \end{cases}
\end{equation}
At each point $x\in{s_2}$ the
terrestrial inventory provides the \textbf{local density} $Y(x)$
\begin{equation}\label{truelocaldensity}
 Y(x) =\frac{1}{\lambda(F)}\sum_{i=1}^N \frac{I_i(x)Y_i}{\pi_i}=\frac{1}{\lambda(F)}\sum_{i\in{s}_2(x)} \frac{Y_i}{\pi_i}
 \end{equation}
 The term $\frac{1}{\lambda(F)\pi_i}$ is the tree extrapolation factor $f_i$ with dimension $ha^{-1}$. One must include possible boundary adjustments, $\lambda(F)\pi_i=\lambda(F \cap K_i)$, where $K_i$ is the inclusion circle of the $i$-th tree. In the infinite population or Monte Carlo approach one samples the function $Y(x)$ (\cite{mandallaz}) for which the following important relation holds:
 \begin{equation}\label{montecarlo}
 \EX_{x} (Y(x))=\frac{1}{\lambda(F)}\int_{F} Y(x)dx=\frac{1}{\lambda(F)}\sum_{i=1}^NY_i=\bar{Y}
 \end{equation}
 Where $\EX_x$ denotes the expectation with respect to a random point $x$ uniformly distributed in $F$. This establishes the link between the infinite population (continuum) $\{x\in{F} \mid Y(x)\}$ and the finite population of trees $\{i=1,2 \ldots N \mid Y_i\}$.\par
 Usually boundary adjustments are performed only with respect to $F$ and not with respect to the small area $G$. However, we shall assume that we also have
 \begin{equation}\label{small2}
 \bar{Y}_G=\frac{1}{\lambda(G)}\int_{G} Y(x)dx
 \end{equation}
 The afore mentioned randomization principle assume that we have uniformly independently distributed points or clusters in the forested area $F$, whereas in practice systematic grids are used. There is reasonable theoretical and empirical evidence that treating systematic grids as simple random samples is acceptable for point estimation and also for variance estimation (which will be in most instances slightly overestimated) for extensive forest inventories. From a mathematical point of view the only correct, and also most efficient, procedure, is the geostatistical Kriging technique (see \cite{mandallaz}, chapter 7 for a brief introduction and further references), which, however, is difficult to use and not uncontroversial in some aspects (e.g. choice of spatial correlation models and stationarity assumptions).
 \section{The model}
 We consider the linear model (the upper script on vector or matrices denotes thereafter the transposition operator)
 \begin{equation}\label{linearmodel1}
 Y(x)=\pmb{Z}^t(x)\pmb{\beta}+ R(x)
 \end{equation}
 In the \textbf{model-dependent approach} the point $x$ is fixed and $R(x)$ is a random variable with zero mean and a given covariance structure. In the \textbf{design-based approach}
  $Y(x), \pmb{Z}(x), R(x)$ are random variables  because $x$ is random. The true regression coefficient $\pmb{\beta}$ is by definition the least squares estimate minimizing
 $$\int_F R^2(x)dx=\int_F(Y(x)-\pmb{Z}^t(x)\pmb{\beta})^2 dx$$
 It satisfies the normal equation
 \begin{equation}\label{normaleq1}
 \Big(\int_F\pmb{Z}(x)\pmb{Z}^t(x)dx\Big)\pmb{\beta}=\int_F Y(x)\pmb{Z}(x)dx
 \end{equation}
 and the orthogonality relationship
 \begin{equation}\label{linearmodel2}
 \int_F R(x)\pmb{Z}(x)dx=\pmb{0}
 \end{equation}
  We shall assume that $\pmb{Z}(x)$ contains the intercept term $1$, or, more generally, that the intercept can be expressed as a linear combination of the component of $\pmb{Z}(x)$, which then insures that the mean residual is zero, i.e.
 $$\int_F R(x)dx=0$$
   The important case of stratification amounts to taking $\pmb{Z}^t(x)=(I_{F_1}(x),I_{F_2}(x),\ldots I_{F_L}(x))$, where $F=\cup_{k=1}^L F_k$ and $I_{F_k}(x)$ is the zero-one indicator variable of the $k$-th stratum $F_k$.\\
    We emphasize the fact that in the design-based model-assisted approach the model [\ref{linearmodel1}] is not viewed as an adequate description of the complex stochastic process generating the $Y(x)$, but, more pragmatically, simply as a tool to reduce the variance of estimators of $\bar{Y}, \bar{Y}_G$. Of course, ideally, the model should capture qualitatively the main features of the underlying natural phenomenon. \\To simplify the notation let us
 set $\pmb{A}=\EX_x\pmb{Z}(x)\pmb{Z}^t(x)$, $\pmb{U}(x)=Y(x)\pmb{Z}(x)$.
 The normal equation then reads
 $$\pmb{A}\pmb{\beta}=\EX_x\pmb{U}(x):=\pmb{U}$$
 Of course, only a sample-based normal equation is available, i.e.
 $$\pmb{A}_{s_2}\hat{\pmb{\beta}}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{U}(x)=\pmb{U}_{s_2}$$
 where we have set
 $$\pmb{A}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x)$$ and
 $$\pmb{U}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)$$
 The theoretical and empirical regression vector parameters are
 \begin{eqnarray}\label{linearmodel3}
 \pmb{\beta}&=&\pmb{A}^{-1}\pmb{U} \nonumber \\
 \hat{\pmb{\beta}}_{s_2}&=&\pmb{A}_{s_2}^{-1}\pmb{U}_{s_2}
 \end{eqnarray}
$\hat{\pmb{\beta}}_{s_2}$ is asymptotically design-unbiased for
$\pmb{\beta}$.
 To calculate the design-based variance-covariance matrix of the regression coefficients we need
 $$\EX(\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})(\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})^t$$
 we shall use the Taylor linearization technique. Let us consider the function
 $f(\cdot,\cdot)$ of an arbitrary $(p,p)$ matrix $\pmb{A}$ and an arbitrary $(p,1)$
vector $\pmb{U}$ defined by
 $f(\pmb{A},\pmb{U})=\pmb{A}^{-1}\pmb{U}$. We can write
 $$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}=f(\pmb{A}_{s_2},\pmb{U}_{s_2})-f(\pmb{A},\pmb{U})$$
 which can be viewed as the differential of the function $f()$ at
 the point $\pmb{P}_0=(\pmb{A},\pmb{U})$, which is the expected value of
 the random point $\pmb{P}_{s_2}=(\pmb{A}_{s_2},\pmb{U}_{s_2})$. The distances between the fixed and the random point are of the order
 $n_2^{-\frac{1}{2}}$ in design-probability (by the law of large numbers for $\pmb{U}_{s_2}$ and $\pmb{A}_{s_2}$ and the continuity of the inverse operation). The differential of $f(\cdot,\cdot)$ at $\pmb{P}_0$ is, by the derivation
 rule for product
 $$df=d(\pmb{A}^{-1})\pmb{U}+\pmb{A}^{-1}d\pmb{U}$$
 Differentiating the identity $\pmb{A}^{-1}\pmb{A}=\pmb{I}$ one gets
 $$d(\pmb{A}^{-1})=-\pmb{A}^{-1}(d\pmb{A})\pmb{A}^{-1}$$
 and the following first-order Taylor expansion:
 $$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}\approx
 -\pmb{A}^{-1}(\pmb{A}_{s_2}-\pmb{A})\pmb{A}^{-1}\pmb{U}+\pmb{A}^{-1}(\pmb{U}_{s_2}-\pmb{U})$$
 Expanding this expression and substituting
 $\pmb{A}^{-1}\pmb{U}=\pmb{\beta}$ we obtain the Taylor
 linearization \index{Linearization!Taylor}
 $$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}\approx \pmb{A}^{-1}\Big(-\pmb{A}_{s_2}\pmb{\beta}+\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{U}(x)\Big)$$
 which is, by definition, equal to
 $$\pmb{A}^{-1}\Big(-\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}(x)^t\pmb{\beta}+\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)\Big)$$
 and consequently also to
 $$\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\big(Y(x)-\pmb{Z}(x)^t\pmb{\beta}\big)\pmb{Z}(x)\Big)
 =\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\Big)$$
 Thus, we finally arrive at
 \begin{equation}\label{Taylor}
 \hat{\pmb{\beta}}_{s_2}-\pmb{\beta}\approx \pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\Big)
 \end{equation}
 Using [\ref{linearmodel2}] and the independence of the $R(x)\pmb{Z}(x)$ one obtains the design-based variance-covariance matrix of
$\hat{\pmb{\beta}}_{s_2}$
\begin{equation}\label{varmatrix}
\pmb{\Sigma}_{\hat{\pmb{\beta}}_{s_2}}\approx
\pmb{A}^{-1}\Big(\frac{1}{n_2}\EX_xR^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}^{-1}
\end{equation}
which can be estimated by replacing the theoretical residual $R(x)$
with their empirical counterparts
$\hat{R}(x)=Y(x)-\hat{Y}(x)$, with $\hat{Y}(x)=\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}$, and $\pmb{A}$
with $\pmb{A}_{s_2}$. We then get the \textbf{estimated design-based
variance-covariance matrix} as
\begin{equation}\label{estvarmatrix}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}:=\pmb{A}_{s_2}^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}_{s_2}^{-1}
\end{equation}
Interestingly this is precisely the \textbf{robust estimate of the model-dependent covariance matrix} given in \cite{gregoire2} (see also \cite{mandallaz} p. 107). Setting
$\hat{\sigma}^2=\frac{\sum_{x\in{s_2}}\hat{R}^2(x)}{n_2}$ we get $\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\approx \frac{\hat{\sigma^2}}{n_2}\pmb{A}_{s_2}^{-1}$ whereas the model-dependent ordinary least squares theory gives the unbiased estimate of the covariance matrix as $(\frac{n_2}{n_2-p}\hat{\sigma}^2)\frac{1}{n_2}\pmb{A}_{s_2}^{-1}$.\\
The empirical residuals satisfy the sample orthogonality relation
\begin{equation}\label{sampleortho}
\frac{1}{n_2}\sum_{x\in{s_2}}\hat{R}(x)\pmb{Z}(x)=\pmb{0}
\end{equation}
Theoretically one may use the exact matrix $\pmb{A}$ if it is available or its estimate $\pmb{A}_{s_1}=\frac{1}{n_1}\sum_{x\in{s_1}}\pmb{Z}(x)\pmb{Z}^t(x)$ based on the large sample. However, the resulting point estimates are not always intuitively convincing and not optimal in the model-dependent framework. Besides, they are not available from the usual statistical software packages. For these reasons we shall only work with $\pmb{A}_{s_2}$.
\section{The estimators}
\subsection{External models}
If the prediction model is \textbf{external}, i.e. not fitted with the inventory data at hand, the regression estimate is defined as
\begin{equation}\label{external1}
\hat{Y}_{reg}=\frac{1}{n_1}\sum_{x\in{s_1}}\hat{Y}_0(x)+ \frac{1}{n_2}\sum_{x\in{s_2}}R_0(x)
\end{equation}
with the predictions $\hat{Y}_0(x)=\pmb{Z}^t(x)\pmb{\beta_0}$ and the residuals $R_0(x)=Y(x)-\hat{Y}_0(x)$, where $\pmb{\beta_0}$ is the given external regression coefficient, ideally obtained from another similar inventory whose sample points are independent of the current inventory's sample points. Note that in this case the mean residual will not necessarily be zero.
To calculate the variance one uses the decomposition
\begin{equation}\label{external2}
\VAR_{1,2}(\hat{Y}_{reg})=\VAR_1\EX_{2\mid 1}(\hat{Y}_{reg})+\EX_1\VAR_{2 \mid 1}(\hat{Y}_{reg})
\end{equation} to obtain
\begin{equation}\label{external3}
\VAR(\hat{Y}_{reg})=\frac{1}{n_1}\VAR(Y(x))+(1-\frac{n_2}{n_1})\frac{1}{n_2}\VAR(R_0(x))
 \end{equation}
 which can be unbiasedly estimated with
\begin{equation}\label{varexternalsimple}
\hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s_2}}(Y(x)-\bar{Y}_2)^2+
(1-\frac{n_2}{n_1})\frac{1}{n_2}\frac{1}{n_2-1}\sum_{x\in{s_2}}(R_0(x)-\bar{R}_{0,2})^2
\end{equation}
where $\bar{Y}_2=\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)$ and $\bar{R}_{0,2}=\frac{1}{n_2}\sum_{x\in{s_2}}R_0(x)$.\\
The estimation for any small area $G\subset F$ is straightforward, indeed one simply restricts the samples of $n_1$ and $n_2$ points in $F$ to the $n_{1,G}$ and $n_{2,G}$ points in $G$ and apply the above formulae to obtain an unbiased estimate of the conditional variance (i.e. given $n_{1,G}$ and $n_{2,G}$, which are realizations random variables because in our set-up only $n_1$ and $n_2$ are fixed).
\subsection{Internal models}
In most applications the model has to be fitted with the data provided by the current inventory. In this case, the model is said to be \textbf{internal}. In very large samples one can treat an internal model as external and apply again the formulae given above, which obviously neglects the error in the regression coefficients. This is essentially the framework presented in (\cite{mandallaz}, chapter 5 and section 6.3). We shall show in the present paper how one can take the design-based variance of the regression coefficients into account, albeit still in large samples, and incorporate the mean residual directly in the model.\\
 The model-dependent estimator for the small area $G$ is called the
\textbf{synthetic estimator} and is given by
\begin{eqnarray}\label{synthetic1}
\hat{Y}_{G,synth}&=&\frac{1}{\lambda(G)}\int_G\hat{Y}_{s_2}(x)dx \\ \nonumber
&=& \frac{1}{\lambda(G)}\int_G \pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}dx=\bar{\pmb{Z}}_G^t\hat{\pmb{\beta}}_{s_2}
\end{eqnarray}
where $\bar{\pmb{Z}}_G=\frac{1}{\lambda(G)}\int_G\pmb{Z}(x)dx$ is the true mean of the auxiliary vector over the small area $G$, which is available only if the first phase is exhaustive. $\hat{Y}_{G,synth}$ is unbiased under the model, but not optimal as it does not the take the model-dependent spatial correlation of the $Y(x)$ into account. Let us emphasize the fact that the model, i.e. $\hat{\pmb{\beta}}_{s_2}$, is fitted with the full data set and not only with $\{Y(x), \pmb{Z}(x)\mid x\in G\}$. \\
\textbf{In this paper we shall investigate the properties of $\hat{Y}_{G,synth}$ in the design-based inference framework}. \\
First, let us note that $\hat{Y}_{G,synth}$ is a design-based consistent sample copy of
 $$\frac{1}{\lambda(G)}\int_G \hat{Y}(x)dx=\frac{1}{\lambda(G)}\int_G(Y(x)-R(x))dx=\bar{Y}_G-\frac{1}{\lambda(G)}\int_G R(x)dx$$
 Consequently, the synthetic estimator $\hat{Y}_{G,synth}$ has a design-based asymptotic bias equal to $-\frac{1}{\lambda(G)}\int_GR(x)$, which is not zero unless $G=F$ (we have zero mean residual over the entire domain, see [\ref{linearmodel2}]) or, which is unlikely, zero mean residual over the small area of interest. Using [\ref{synthetic1}] and [\ref{estvarmatrix}] \textbf{the estimated design-based variance of the synthetic estimator} is
\begin{equation}\label{synthetic2}
\hat{\VAR}(\hat{Y}_{G,synth})=\bar{\pmb{Z}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G
\end{equation}
We define the g-weights as
\begin{equation}\label{gweights1}
g_G(x)=\bar{\pmb{Z}}_G^t\pmb{A}^{-1}_{s_2}\pmb{Z}(x)
\end{equation}
It is easily checked that one can rewrite the point estimate and its estimated variance as
\begin{eqnarray}\label{gweights2}
\hat{Y}_{G,synth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}g_G(x)Y(x) \nonumber\\
\hat{\VAR}(\hat{Y}_{G,synth})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_G(x)\hat{R}^2(x)
\end{eqnarray}
where the $\hat{R}(x)=Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}$ are the empirical residuals.
In the above the special case $G=F$ is possible. The g-weights enjoy  several attractive statistical properties (see \cite{sarndal} for the aspects in general sampling theory and \cite{mandallaz} for their Monte-Carlo counterparts in forest inventory).\\
To compensate for the bias due to the non vanishing mean residual over $G$ one considers the \textbf{small-area estimator} (\cite{mandallaz} p.120)
\begin{equation}\label{smallareaest}
\hat{Y}_{G,small}=\hat{Y}_{G,synth}+
\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)
\end{equation}
\noindent where $s_{2,G}=s_2\cap G$ and $n_{2,G}=\sum_{x\in{s_2}}I_G(x)$ is the number of
points of $s_2$ falling within $G$. It can be shown (\cite{mandallaz}) that $\hat{Y}_{G,small}$ is asymptotically design-unbiased with estimated design-based variance given by
\begin{equation}\label{smallareadesignvariance1}
\hat{\VAR}(\hat{Y}_{G,small})=
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{equation}
\noindent where
$$\bar{\hat{R}}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)$$
is the estimated mean residual over a small area. The above variance estimate neglects the variance of $\hat{\pmb{\beta}}_{s_2}$ and is therefore valid only if $n_2$ is very large and $n_2>> n_{2,G}$. To have better insight we use the expansion [\ref{Taylor}] to obtain
\begin{equation}\label{smallexp1}
\hat{Y}_{G,small}-\bar{Y}_G=
\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x)\Big)
+ \frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R(x)-\bar{R}_G
\end{equation}
which leads to the variance
\begin{equation}\label{smallvar1}
\EX(\hat{Y}_{G,small}-\bar{Y}_G)^2=
\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\Big(\frac{1}{n_2}\EX R^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}^{-1}\bar{\pmb{Z}}_G + \VAR \big(\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R(x)\big )+ C
\end{equation}
\noindent where the cross-product term $C$ is given by
$$2\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\EX\big( (\frac{1}{n_2}\sum_{x\in{s_2}}R(x)\pmb{Z}(x))
(\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R(x)-\bar{R}_G)\big)$$
Both terms of the product tend to zero at rate $(n_2^{-\frac{1}{2}})$, which unfortunately is of the same order as the first two terms. However, using the fact that the $R(x)$, $\pmb{Z}(x)$ are independent of $R(y)$, $\pmb{Z}(y)$ for $x\ne y$ we obtain after tedious but simple calculations
$$C=\frac{1}{n_2}\bar{\pmb{Z}}^t_G\pmb{A}^{-1}\big(\EX_{x\in{G}}R^2(x)\pmb{Z}(x)-\bar{R}_G\EX_{x\in{G}}R(x)\pmb{Z}(x)\Big)$$
which we can reasonably assume to be negligible. The above arguments suggest therefore the following estimate of the design-based variance of the small-area estimator with exhaustive first phase
\begin{eqnarray}\label{smallareadesignvariance2}
\hat{\VAR}(\hat{Y}_{G,small})&=&
\bar{\pmb{Z}}^t_G\pmb{A}_{s_2}^{-1}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}} \hat{R}^2(x)\pmb{Z}(x)\pmb{Z}(x)^t\Big)\pmb{A}_{s_2}^{-1}\bar{\pmb{Z}}_G \\ \nonumber &+&
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
Comparing with [\ref{varexternalsimple}] (after restricting the samples to $G$) we see that \textbf{treating an internal model as an external model} (i.e. ignoring the variability of $\hat{\pmb{\beta}}_{s_2}$) \textbf{ will underestimate the variance of the small area estimate}. The first term in [\ref{smallareadesignvariance2}] reflects the uncertainty in the regression coefficients.\\
If the first-phase is non-exhaustive, i.e. $n_1 \ne \infty$, then one can replace the true mean $\bar{\pmb{Z}}_G$ by its estimate in the large sample
$$\hat{\bar{\pmb{Z}}}_{1,G}=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\pmb{Z}(x)$$ where $s_{1,G}$ is the set $s_1\cap G$ of the $n_{1,G}=\sum_{x\in{s_1}}I_g(x)$ points of the large sample falling into the small area $G$. This gives the \textbf{pseudo-synthetic estimator}
\begin{equation}\label{pseudosynth1}
\hat{Y}_{G,psynth}=\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}
=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\hat{Y}(x)
\end{equation}
which is clearly asymptotically equivalent to $\hat{Y}_{G,synth}$ as $n_1 \to \infty$ and its design-based expected value tends to  $\bar{\pmb{Z}}^t_G\pmb{\beta}$. To calculate the asymptotic variance we use the decomposition (actually the first order Taylor expansion)
$$\Delta=\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}-\bar{\pmb{Z}}_G^t\pmb{\beta}=
\hat{\bar{\pmb{Z}}}_{1,G}^t(\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})+ (\hat{\bar{\pmb{Z}}}_{1,G}^t-\bar{\pmb{Z}}_G)^t\pmb{\beta}$$
Asymptotically we get
$$\EX \Delta^2=\bar{\pmb{Z}}_G ^t\pmb{\Sigma}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G+
\pmb{\beta}^t\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\pmb{\beta}+
2\EX\Big((\hat{\pmb{\beta}}_{s_2}-\pmb{\beta})^t\hat{\bar{\pmb{Z}}}_{1,G}(\hat{\bar{\pmb{Z}}}_{1,G}
-\bar{\pmb{Z}}_G)^t\pmb{\beta}\Big)$$
\noindent where $\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}$ is the covariance matrix of
$\hat{\bar{\pmb{Z}}}_{1,G}  $. The first two terms are of order $n_2^{-1}$ and $n_1^{-1}$ respectively. For the third term we note that
$\EX (\hat{\bar{\pmb{Z}}}_{1,G}(\hat{\bar{\pmb{Z}}}_{1,G}-\bar{\pmb{Z}}_G)^t)$ is equal to the covariance matrix $\pmb{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}$ and therefore of order $n_1^{-1}$ and that
$\hat{\pmb{\beta}}_{s_2}-\pmb{\beta}$ is of order $n_2^{-\frac{1}{2}}$. The last term is therefore of smaller order than the first two which leads to the following asymptotic design-based estimate of variance
\begin{equation}\label{estvarpseudosynth1}
\hat{\VAR}(\hat{Y}_{G,psynth}): =
\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_{1,G}
+ \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\hat{\pmb{\beta}}_{s_2}
\end{equation}
The variance-covariance matrix of the auxiliary vector $\hat{\bar{\pmb{Z}}}_G$ is estimated by
\begin{equation}\label{estvarcovaux}
\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}=
\frac{1}{n_{1,G}(n_{1,G}-1)}\sum_{x\in{s_{1,G}}}
(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{1,G})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{1,G})^t
\end{equation}
Usually $\hat{Y}_{G,psynt}$ will have a small variance but at the cost of a potential bias.
We can rewrite [\ref{pseudosynth1}] and [\ref{estvarpseudosynth1}] with the g-weights
\begin{eqnarray}\label{gweightpseudo}
g_{G,1}(x)&=&\hat{\bar{\pmb{Z}}}^t_{1,G}\pmb{A}^{-1}_{s_2}\pmb{Z}(x) \nonumber\\
\hat{Y}_{G,psynth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}g_{G,1}(x)Y(x)
\end{eqnarray}
and after some algebra we get
\begin{eqnarray}\label{gweightpseudo2}
\hat{\VAR}(\hat{Y}_{G,psynth})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{G,1}(x)\hat{R}^2(x) + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\hat{\pmb{\beta}}_{s_2}  \nonumber \\
&=& \frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{G,1}(x)\hat{R}^2(x)+ \frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}(\hat{Y}(x)-\bar{\hat{Y}}_{1,G})^2
\end{eqnarray}
where $\bar{\hat{Y}}_{1,G}=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}\hat{Y}(x)$. The second term in the last equation is the variance of the predictions over $G$.\\
The \textbf{pseudo small-area estimator}
\begin{equation}\label{pseudosmall}
\hat{Y}_{G,psmall}=\hat{Y}_{G,psynth}+\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\hat{R}(x)
\end{equation}
is asymptotically design-unbiased and intuitively its variance can be expected to be well approximated by
\begin{equation}\label{pseudosmallvar}
\hat{\VAR}(\hat{Y}_{G,psmall})=
\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_{1,G}
+ \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1,G}}\hat{\pmb{\beta}}_{s_2}
+\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{equation}
A tedious formal proof can be given by using [\ref{external2}], [\ref{smallvar1}] and
[\ref{estvarpseudosynth1}].\\
Using the same arguments as in [\ref{gweightpseudo2}] we also have
\begin{eqnarray}\label{pseudosmallvar2}
\hat{\VAR}(\hat{Y}_{G,psmall})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{G,1}(x)\hat{R}^2(x)
+ \frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}(\hat{Y}(x)-\bar{\hat{Y}}_{1,G})^2 \nonumber \\
&+& \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
This should be compared with the external version [\ref{varexternalsimple}]
\begin{eqnarray}\label{varexternalsimpleG}
\hat{\VAR}(\hat{Y}_{G,psmall})&=&\frac{1}{n_{1,G}}\frac{1}{n_{2,G}-1}
\sum_{x\in{s_{2,G}}}(Y(x)-\bar{Y}_{2,G})^2 \nonumber \\
&+& (1-\frac{n_{2,G}}{n_{1,G}})
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{R}(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
For very large $n_{1,G}$ it is clear that the external version will underestimate the variance
as it neglects the first term in [\ref{pseudosmallvar2}], which albeit is also small for large $n_2$. \\
The special case $G=F$ deserves special attention: because of the zero mean residual we have $\hat{Y}_{F,psmall}=\hat{Y}_{F,psynt}=\hat{Y}_{reg}$ and [\ref{estvarpseudosynth1},\ref{pseudosmallvar2}] lead to the estimated variance
\begin{equation}\label{varyregsynt1}
\hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n^2_2}\sum_{x\in{s_2}}g^2_{F,1}(x)\hat{R}^2(x)
+ \frac{1}{n_1(n_1-1)}
\sum_{x\in{s_1}}(\hat{Y}(x)-\bar{\hat{Y}}_1)^2
\end{equation}
with $\bar{\hat{Y}}_1=\frac{1}{n_1}\sum_{x\in{s_1}}\hat{Y}(x)$. The external version
[\ref{varexternalsimpleG}] gives
\begin{equation}\label{varexternalsimpleG2}
\hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s_2}}(Y(x)-\bar{Y}_2)^2+
(1-\frac{n_{2}}{n_{1}})
\frac{1}{n_{2}}\frac{1}{n_{2}-1}\sum_{x\in{s_{2}}}\hat{R}^2(x)
 \end{equation}
 Writing $Y(x)=\hat{Y}(x)+\hat{R}(x)$ an using [\ref{sampleortho}] we can rewrite [\ref{varexternalsimpleG2}] as
 \begin{equation}\label{varexternalsimpleG3}
 \hat{\VAR}(\hat{Y}_{reg})=\frac{1}{n_1}\frac{1}{n_2-1}\sum_{x\in{s_2}}(\hat{Y}(x)-\bar{\hat{Y}}_2)^2+
\frac{1}{n_{2}}\frac{1}{n_{2}-1}\sum_{x\in{s_{2}}}\hat{R}^2(x)
\end{equation}
For $G=F$ the $g^2_{F,s_1}(x)$ are asymptotically equal to $1$ (see \cite{mandallaz} p. 113 and the properties of the g-weights discussed below) so that both versions [\ref{varyregsynt1}] and [\ref{varexternalsimpleG3}] are asymptotically equivalent. However, version [\ref{varyregsynt1}] estimates the variance of the predictions in the large sample, which is better, and it rests upon the g-weights for the residual part, which is known to have better conditional properties (see \cite{mandallaz} p. 84 and section \ref{poststrat} for the important special case of stratification).\\
In the next section we present a simple reformulation of the problem that allows one to transform small-area estimators into synthetic estimators, which offers a great mathematical advantage.
\subsection{Alternative estimators in extended model}
 The main difficulty stems from the fact that
$\int_{G}R(x)dx \ne 0$. If we now extend the auxiliary information vector  $\pmb{Z}(x)$ to $\pmb{\mathcal{Z}}^t(x)=(\pmb{Z}^t(x),I_G(x))\in{\mathcal{R}}^{(p+1)}$, the corresponding model reads
\begin{equation}\label{extendemodel1}
Y(x)=\pmb{\mathcal{Z}}^t(x)\pmb{\theta} +\mathcal{R}(x)
 \end{equation}
 which leads to the normal equation for the extended parameter vector $\pmb{\theta}\in{\mathcal{R}}^{(p+1)}$
$$\Big(\int_F\pmb{\mathcal{Z}}(x)\pmb{\mathcal{Z}}^t(x)dx\Big)\pmb{\theta}=:
\pmb{\mathcal{A}}\pmb{\theta}=\int_{F}Y(x)\pmb{\mathcal{Z}}(x)dx$$ and the orthogonality relationship
$$\int_{F}\mathcal{R}(x)\pmb{\mathcal{Z}}(x)dx=\pmb{0}$$
 Since $I_F(x)\equiv 1$ is the intercept term (or linear combination of the components of $\pmb{Z}(x)$) and $\pmb{\mathcal{Z}}(x)$ contains $I_G(x)$ \textbf{we have the two zero mean residual properties}
$$\int_F \mathcal{R}(x)dx=\int_G \mathcal{R}(x)dx=0$$
Hence, by including the $0,1$ indicator variable of the small area $G$ into the model, we enforce zero mean residual over $F$ and $G$. Note also that $G$ must be a proper subset of $F$, otherwise $\pmb{\mathcal{A}}$ and $\pmb{\mathcal{A}}_{s_2}$ are singular. In practice near-singularity could cause numerical problems, so that the small area $G$ must indeed be small with respect to $F$. Simple calculations yield the following block structure for $\pmb{\mathcal{A}}_{s_2}=\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{\mathcal{Z}}(x)\pmb{\mathcal{Z}}^t(x)$
\begin{equation}\label{tildeAs2}
 \pmb{\mathcal{A}}_{s_2}=
\left[ \begin {array}{ll}
\pmb{A}_{s_2}& \hat{p}_{2,G}\hat{\bar{\pmb{Z}}}_{2,G} \\
 \hat{p}_{2,G}\hat{\bar{\pmb{Z}}}_{2,G}^t & \hat{p}_{2,G} \\ \end {array} \right]
\end{equation}
where we have set $\hat{p}_{2,G}=\frac{n_{2,G}}{n_2}$, $\hat{\bar{\pmb{Z}}}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\pmb{Z}(x)$. Using formulae for the inversion of partitioned matrices (see e.g. \cite{searle} p. 27 and \cite{tian} for useful generalizations) one obtains
\begin{equation}\label{inversetildeAs2}
 \pmb{\mathcal{A}}^{-1}_{s_2}=
\left[ \begin {array}{ll}
\pmb{A}^{-1}_{s_2}& \pmb{0} \\
 \pmb{0} & \pmb{0} \\ \end {array} \right]
 + \frac{1}{\gamma}\left[ \begin {array}{ll}
\hat{p}_{2,G}^2\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}\hat{\bar{\pmb{Z}}}^t_{2,G}\pmb{A}^{-1}_{s_2}
& - \hat{p}_{2,G}\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}\\
 -\hat{p}_{2,G}\hat{\bar{\pmb{Z}}}^t_{2,G}\pmb{A}^{-1}_{s_2} & 1 \\ \end {array} \right]
\end{equation}
with $\gamma=\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}^t_{2,G}\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}$.\\
We need
$$\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{\mathcal{Z}}(x)=
(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}^t(x),\hat{p}_{2,G}\hat{\bar{Y}}_G)^t
=((\pmb{A}^{-1}_{s_2}\hat{\pmb{\beta}}_{s_2})^t,\hat{p}_{2,G}\hat{\bar{Y}}_G)^t$$
where $\hat{\bar{Y}}_G=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}Y(x)$. This leads after some algebra to the following relationship between the regressions coefficients
\begin{equation}\label{relationship}
 \hat{\pmb{\theta}}_{s_2}=
\left[ \begin {array}{l}
\hat{\pmb{\beta}}_{s_2} \\
 0 \\ \end {array} \right]
 + \frac{1}{\gamma}\left[ \begin {array}{l}
-\hat{p}_{2,G}^2(\hat{\bar{Y}}_G-\hat{\bar{\pmb{Z}}}^t_{2,G}\hat{\pmb{\beta}}_{s_2})\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}
\\
 \hat{p}_{2,G}(\hat{\bar{Y}}_G-\hat{\bar{\pmb{Z}}}^t_{2,G}\hat{\pmb{\beta}}_{s_2})\end {array} \right]
\end{equation}
Note that the term
$$\hat{\bar{Y}}_G-\hat{\bar{\pmb{Z}}}^t_{2,G}\hat{\pmb{\beta}}_{s_2}=\frac{1}{n_{2,G}}
\sum_{x\in{s_{2,G}}}(Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2})$$
is precisely the mean residual over the small area. Hence, the last component of $\hat{\pmb{\theta}}_{s_2}$ is essentially the residual term. We see that the original regression coefficient $\hat{\pmb{\beta}}_{s_2}$ is corrected in the extended model by the residual term and that the impact of this correction tends to zero as the small area gets smaller with respect to $F$, a very intuitive result indeed. One obtains a very similar but not identical result by least squares minimization under the constraint of zero mean residual over the small area (see \cite{searle}, pp 112-113).\\
In perfect analogy with [\ref{estvarmatrix}] the estimated covariance matrix is given by
\begin{equation}\label{estvarmatrixext}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}=\pmb{\mathcal{A}}_{s_2}^{-1}
\Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{\mathcal{R}}^2(x)\pmb{\mathcal{Z}}(x)
\pmb{\mathcal{Z}}(x)^t\Big)\pmb{\mathcal{A}}_{s_2}^{-1}
\end{equation}
where we have set $\hat{\mathcal{R}}(x)=Y(x)-\pmb{\mathcal{Z}}^t(x)\hat{\pmb{\theta}}_{s_2}$.
If the first phase is exhaustive we calculate the synthetic estimator in the extended model
\begin{equation}\label{extsynthethic1}
\hat{\tilde{Y}}_{G,synth}=
\frac{1}{\lambda(G)}\int_G\pmb{\mathcal{Z}}^t(x)\hat{\pmb{\theta}}_{s_2}dx
=\bar{\pmb{\mathcal{Z}}}^t_{G}\hat{\pmb{\theta}}_{s_2}
\end{equation}
With $\bar{\pmb{\mathcal{Z}}}^t_{G}=(\bar{\pmb{Z}}^t_{G},1)$ and some algebra one finally obtains
\begin{equation}\label{extsynthethic2}
\hat{\tilde{Y}}_{G,synth}=\bar{\pmb{Z}}_{G}^t\hat{\pmb{\beta}}_{s_2}+
\frac{\alpha}{n_{2,G}}\sum_{x\in{s_{2,G}}}\big(Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}\big)
\end{equation}
where we have set
$$\alpha=\frac{\hat{p}_{2,G}-\hat{p}^2_{2,G}\bar{\pmb{Z}}_{G}^t\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}
{\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}_{2,G}^t\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}$$
Clearly $\hat{\tilde{Y}}_{G,synth}$ and
$\hat{{Y}}_{G,small}$ are asymptotically equivalent because $\alpha$ tends to $1$ in large samples. Note that $\alpha=1$ if the sample is exactly balanced, i.e. if $\hat{\bar{\pmb{Z}}}_{2,G}= \bar{\pmb{Z}}_{G}$.\\
By using [\ref{synthetic2}] and replacing $\pmb{Z}(x)$ with $\pmb{\mathcal{Z}}(x)$ we obtain at once the asymptotic variance
\begin{equation}\label{extsynthetic1var}
\hat{\VAR}(\hat{\tilde{Y}}_{G,synth})=
\bar{\pmb{\mathcal{Z}}}^t_{G}\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}\bar{\pmb{\mathcal{Z}}}_{G}
\end{equation}
One can rewrite $\hat{\tilde{Y}}_{G,synth}$ in terms of g-weights in the extended model as in [\ref{gweights1}] and [\ref{gweights2}] with
$$\tilde{g}_{G}(x)=\bar{\pmb{\mathcal{Z}}}_G^t\pmb{\mathcal{A}}^{-1}_{s_2}\pmb{\mathcal{Z}}(x)$$
 \begin{eqnarray}\label{gweightsynthextended}
 \hat{\tilde{Y}}_{G,synth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G}(x)Y(x) \nonumber \\
 \hat{\VAR}(\hat{\tilde{Y}}_{G,synth})&=&\frac{1}{n^2_2}\sum_{x\in{s_2}}\tilde{g}^2_{G}(x)\hat{\mathcal{R}}^2(x)
 \end{eqnarray}
If the first phase is not exhaustive we estimate the true mean of the extended auxiliary variables
\begin{equation}
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}=\frac{1}{n_{1,G}}\sum_{x\in{s_{1,G}}}
\pmb{\mathcal{Z}}(x)
\end{equation}
to get the pseudo-synthetic estimate in the extended model
\begin{equation}\label{pseudosyntheticextended1}
\hat{\tilde{Y}}_{G,psynth}=\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^t\hat{\pmb{\theta}}_{s_2}
\end{equation}
As in [\ref{extsynthethic2}] we have
\begin{equation}\label{extsynthethic3}
\hat{\tilde{Y}}_{G,psynth}=\hat{\bar{\pmb{Z}}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}+
\frac{\alpha_1}{n_{2,G}}\sum_{x\in{s_{2,G}}}\big(Y(x)-\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}\big)
\end{equation}
where we have set
$\alpha_1=\frac{\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}_{1,G}^t
\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}
{\hat{p}_{2,G}-\hat{p}^2_{2,G}\hat{\bar{\pmb{Z}}}_{2,G}^t
\pmb{A}^{-1}_{s_2}\hat{\bar{\pmb{Z}}}_{2,G}}$.\\
By [\ref{estvarpseudosynth1}] we get immediately the following consistent estimate of the design-based variance
\begin{equation}\label{estvarpsynth1extended}
\hat{\VAR}(\hat{\tilde{Y}}_{G,psynth})=
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}
+ \hat{\pmb{\theta}}^t_{s_2}\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}}\hat{\pmb{\theta}}_{s_2}
\end{equation}
The variance-covariance matrix of $\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}$ can be estimated as usual by
\begin{equation}\label{estvarcovaux}
\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}}=
\frac{1}{n_{1,G} (n_{1,G}-1)}\sum_{x\in{s_{1,G}}}
(\pmb{\mathcal{Z}}(x)-\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G})(\pmb{\mathcal{Z}}(x)-
\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G})^t
\end{equation}
Again, one can rewrite the above expression with the g-weights
$\tilde{g}_{G,1}(x)=\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}^t
\pmb{\mathcal{A}}^{-1}_{s_2}\pmb{\mathcal{Z}}(x)$
namely
\begin{eqnarray}\label{gweightextended}
\hat{\tilde{Y}}_{G,psynth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,1}(x)Y(x)\\
\hat{\VAR}(\hat{\tilde{Y}}_{G,psynth})&=&\frac{1}{n^2_2}
\sum_{x\in{s_2}}\tilde{g}^2_{G,1}(x)\hat{\mathcal{R}}^2(x)+
\hat{\pmb{\theta}}^t_{s_2}\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}}\hat{\pmb{\theta}}_{s_2}
\end{eqnarray}
\textbf{Properties of the g-weights:}\label{gweights}
 \begin{enumerate}
 \item
 The g-weights enjoy the calibration properties
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{G,1}(x)\pmb{Z}(x)=
 \hat{\bar{\pmb{Z}}}_{1,G}$  and  $\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,1}(x)\pmb{\mathcal{Z}}(x)=
 \hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}$. The proof is immediate by transposing the equalities and by the very definition of the g-weights.
 \item
  The fact that one can assume the g-weights depend only on the point $x$ and not on the whole sample $s_2$ when calculating variances is fully justified by the Taylor expansion leading to the robust design-based covariances.
 \item
 By considering formally the trivial constant local density $Y(x)\equiv 1$ and solving the normal equations one sees that for any $G$ (i.e. also for $G=F$) one has
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{G,1}(x)=
  \frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{G,1}(x)=1$, i.e. the g-weights have means equal to 1.
 \item
  When $G=F$ the estimator $\hat{Y}_{reg}$ and the sample mean $\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)$ both converge towards the true value for an arbitrary $Y(x)$ and therefore one gets $\lim_{n_2\to\infty}g_{F,1}(x)=1>0$. This is not true for a proper subset $G\subset F$. In this case, $\hat{\tilde{Y}}_{G,synth}$ and the sample mean $\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}Y(x)$ converge towards the true value for an arbitrary $Y(x)$, which implies that $\tilde{g}_{G,1}(x)$ will tend to $0$ for $x\not\in{G}$ (negative values are possible) and to $\frac{n_2}{n_{2,G}} $ for $x\in{G}$.
 \end{enumerate}
In simple random sampling one can derive non-parametric versions of the above procedures, i.e. kernel-based regression estimators or $k$ nearest neighbors estimators (\textbf{knn}). However, they are more difficult to implement than the regression estimators described above, primarily because on must use re-sampling (bootstrap) techniques to obtain reliable variance estimators because analytical results are beyond reach and the classical external variance formula severely underestimate the true variance as shown by simulations. Also, the performances of the non-parametric estimators are not better than the regression estimators (for global and small-area estimation) if the coefficient of determination $R^2$ is sufficient large, say above $0.7$, which is the case for timber volume assessment using modern remote sensing data as auxiliary information (e.g. canopy height parameters obtain via e.g. LiDAR). Details can be found in \cite{mandallazreport4}, \cite{massey2} and the PhD thesis \cite{masseyphd}.
In the next section we generalize the previous results on regression estimators to cluster sampling, widely used in national inventories. The main ideas remain the same but the formulae are slightly more cumbersome due to the random cluster size. The non-parametric estimators are not yet available for cluster sampling.
\newpage
\section{Generalization to cluster sampling}
We follow the description of cluster sampling as defined in \cite{mandallaz} (especially section 5.5). A cluster is identified by its origin $x$, uniformly distributed in $\tilde{F}\supset F$. The geometry of the cluster is given by $M$ vectors $e_1,\ldots e_M$ defining the random cluster $x_l=x+e_l$. $M(x)=\sum_{l=1}^MI_{F}(x_l)$ is the random number of points of the cluster falling into the forest area $F$. We define the local density at the cluster level by $Y_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)Y(x_l)}{M(x)}$, likewise we set $\pmb{Z}_c(x)=\frac{\sum_{l=1}^MI_{F}(x_l)\pmb{Z}(x_l)}{M(x)}$. The set $\tilde{F}$ above can be mathematically defined as the smallest set $\{x\in{\mathcal{R}}^2 \mid M(x) \ne 0 \}$. In the first phase we have $n_1$ clusters identified by $x\in{s_1}$ and in the second phase $n_2$ clusters with $x\in{s_2}$, obtained by simple random sampling from $s_1$.\\
We shall use the model-based approach, in which the regression coefficient $\pmb{\beta}_c$ at the cluster level minimizes
$$\EX_{x\in{\tilde{F}}}M(x)(Y_c(x)-\pmb{\beta}^{t}\pmb{Z}_c(x))^2$$
In the pure design-based approach the weights will be $M^2(x)$ but this leads to non-zero mean residual (though close zero in practice), and the definitions of the regression estimator and of the normal equation are slightly different (see \cite{mandallaz}, section 5.5 for details). The choice of $M(x)$ rather than $M^2(x)$ as weights is suggested by the model-dependent approach. When $Y_c(x)$ is the mean of the
$M(x)$ observations, its variance can be expected to be inversely
proportional to $M(x)$.  This procedure  leads to the normal equation
$$\Big(\EX_{x\in{\tilde{F}}}M(x)\pmb{Z_c}(x)\pmb{Z_c}(x)^{t}\Big)\pmb{\beta}_c=\EX_{x\in{\tilde{F}}}M(x)Y_c(x)\pmb{Z_c}(x)$$
and to $\EX_{x\in{\tilde{F}}}M(x)R_c(x)=0$. An asymptotically design-unbiased estimate $\hat{\pmb{\beta}}_{c,s_2}$  for $\pmb{\beta}_c$ can be obtained  by taking a sample copy of the above equation, i.e.
 \begin{eqnarray}\label{estcluster1}
 \hat{\pmb{\beta}}_{c,s_2}&=&
 \Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{Z}_c(x)\pmb{Z}_c^t(x)\Big)^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \nonumber\\
 &:=&\pmb{A}^{-1}_{c,s_2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)Y_c(x)\pmb{Z}_c(x)\Big)
 \end{eqnarray}
 The empirical residuals at the cluster level are
 $$\hat{R}_{c}(x)=Y_c(x)-\pmb{Z}^t_c(x)\hat{\pmb{\beta}}_{c,s_2}$$
 which satisfy the orthogonality relation
 $$\sum_{x\in{s_2}}M(x)\hat{R}_c(x)\pmb{Z}_c(x)=0$$
 and in particular the zero mean residual property
$$\frac{\sum_{x\in{s_2}}M(x)\hat{R}_c(x)}{\sum_{x\in{s_2}}M(x)}=0$$
Using mutatis mutandis exactly the same arguments as in simple random sampling we get the asymptotic robust design-based estimated variance-covariance matrix
\begin{equation}\label{estvarcluster1}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,s_2}}=\pmb{A}^{-1}_{c,s_2}\Big(\frac{1}{n^2_2}\sum_{x\in{s_2}}M^2(x)\hat{R}^2_c(x)
\pmb{Z}_c(x)\pmb{Z}^t_c(x)\Big)\pmb{A}^{-1}_{c,s_2}
\end{equation}
The small-area estimation set up is slightly more complicated because the points of a cluster with $M(x) \ge 0, \;x\in{\tilde{F}}$ can be spread over more than one small area. The notation used in the 2012 version of this technical report was not sufficiently explicit in this respect. This does not affect the regression coefficients, always based on the entire sample in $F$ (the borrowing strength philosophy), but the formulation "after restricting the samples to the small area of interest" is now more explicit in the notation. We define the following quantities:
\begin{eqnarray} \label{extranotation}
M_G(x) &=& \sum_{l=1}^L  I_G(x+e_l) \nonumber \\
Y_{c,G}(x)&=&\frac{\sum_{l=1}^L I_G(x+e_l)Y(x+e_l)}{M_G(x)} \nonumber \\
\hat{\bar{Y}}_{c,2,G} &=& \frac{\sum_{x\in{s_{2,G}}}M_G(x)Y_{c,G}(x)}{\sum_{x\in{s_{2,G}}}M_G(x)}\nonumber \\
\pmb{Z}_{c,G}(x)&=& \frac{\sum_{l=1}^L I_G(x+e_l)\pmb{Z}(x+e_l)}{M_G(x)} \nonumber \\
\hat{Y}_{c,G}(x)&=&\pmb{Z}^t_{c,G}(x)\hat{\pmb{\beta}}_{c,s_2} \nonumber \\
\hat{R}_{c,G}&=& Y_{c,G}(x)-\hat{Y}_{c,G}(x) \nonumber \\
\hat{\bar{\pmb{Z}}}_{c,1,G}&=&\frac{\sum_{x\in{s_{1,G}}}M_G(x)\pmb{Z}_{c,G}(x)}{\sum_{x\in{s_{1,G}}}M_G(x)}
\end{eqnarray}

The estimated variance of the one-phase estimator is given by (\cite{mandallaz}, section 4.3).
\begin{equation}\label{onephaseclustervariance}
\hat{\VAR}(\hat{\bar{Y}}_{c,2,G})=\frac{1}{n_{2,G}(n_{2,G}-1))}
\sum_{x\in{s_{2,G}}}\big(\frac{M_G(x)}{\bar{M}_{2,G}}\big)^2(Y_{c,G}(x)-\hat{\bar{Y}}_{c,2,G})^2
\end{equation}
\noindent where $\bar{M}_{2,G}=\frac{\sum_{x\in{s_{2,G}}}M_G(x)}{n_{2,G}}$.\\
The estimate of the covariance matrix of the auxiliary variables is given by
\begin{equation}\label{estcovauxiliarycluster}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{Z}}}_{c,1,G}}=\frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}\big(\frac{M_G(x)}{\bar{M}_{1,G}}\big)^2(\pmb{Z}_{c,G}(x)-\hat{\bar{\pmb{Z}}}_{c,1,G})
(\pmb{Z}_{c,G}(x)-\hat{\bar{\pmb{Z}}}_{c,1,G})^t
\end{equation}
\noindent where $\bar{M}_{1,G}=\frac{\sum_{x\in{s_{1,G}}}M_G(x)}{n_{1,G}}$.\\
The \textbf{pseudo-synthetic estimate} is then
\begin{eqnarray}\label{psynthclusterest}
\hat{Y}_{c,G,psynth}&=&\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\beta}}_{c,s_2}\nonumber \\
&=& \frac{1}{n_2}\sum_{x\in{s_2}}g_{c,1,G}(x)Y_c(x)
\end{eqnarray}
with the g-weights $g_{c,1,G}(x)=\hat{\bar{\pmb{Z}}}_{c,1,G}^t\pmb{A}^{-1}_{c,s_2}M(x)\pmb{Z}_c(x)$. The estimated variance is as in [\ref{estvarpseudosynth1}]

\begin{equation}\label{estvarpseudosynth1cluster}
\hat{\VAR}(\hat{Y}_{c,G,psynth})=
\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,s_2}}\hat{\bar{\pmb{Z}}}_{c,1,G}
+ \hat{\pmb{\beta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{c,1,G}}\hat{\pmb{\beta}}_{c,s_2}
\end{equation}
The pseudo-synthetic estimate is generally design-biased. Adjusting for the residuals we get the  \textbf{pseudo small-area estimator}
\begin{equation}\label{psmallclusterest}
\hat{Y}_{c,G,psmall}=\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\beta}}_{c,s_2}
+ \frac{\sum_{x\in{s_{2,G}}}M_G(x)\hat{R}_{c,G}(x)}{\sum_{x\in{s_{2,G}}}M_G(x)}
\end{equation}
It is asymptotically design-unbiased and, by analogy with simple random sample (see [\ref{pseudosmallvar}]) its variance can be expected to be well approximated by

\begin{eqnarray}\label{estvarpsmallcluster}
\hat{\VAR}(\hat{Y}_{c,G,psmall})&=&
\hat{\bar{\pmb{Z}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{c,s_2}}\hat{\bar{\pmb{Z}}}_{c,1,G}
+ \hat{\pmb{\beta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{c,1,G}}\hat{\pmb{\beta}}_{c,s_2}
\nonumber \\ &+& \frac{1}{n_{2,G}(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}\big(\frac{M_G(x)}{\bar{M}_{2,G}}\big)^2
(\hat{R}_{c,G}(x)-\bar{\hat{R}}_{2,G})^2
\end{eqnarray}
where $\bar{M}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}M_G(x)$ and
$\bar{\hat{R}}_{2,G}=\frac{\sum_{x\in{s_{2,G}}}M_G(x)\hat{R}_{c,G}(x)}{\sum_{x\in{s_{2,G}}}M_G(x)}$.
One can rewrite [\ref{estvarpsmallcluster}] with g-weights and predictions as in [\ref{pseudosmallvar2}].
\par

As in simple two-phase sampling we can transform the above estimator into a synthetic estimator by considering the extended model $\pmb{\mathcal{Z}}_c^t(x)=(\pmb{Z}_c^t(x),I_{c,G}(x))\in{\mathcal{R}}^{(p+1)}$ with
$I_{c,G}(x)=\frac{\sum_{l=1}^M I_G(x_l)}{M(x)}=\frac{M_G(x)}{M(x)}$. We shall assume that $M_G(x)\equiv M(x)$ for all $x\in{\tilde{G}}=\{x \mid \sum_{l=1}^M I_G(x_l)>0\}$. If we have a partition of the forest in small areas $G_k, k=1,2,\ldots L$, then $\tilde{F} \subset \cup_{k=1}^L \tilde{G}_k$ but it is unlikely in general that the $\tilde{G}_k$ are disjoined. We can reasonably hope that in extensive forest inventory the surface area of points where $M_G(x)< M(x)$ is negligible. The theoretical normal equation
reads
\begin{equation}
\Big(\int_{\tilde{F}}M(x)\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}_c^t(x)dx\Big)\pmb{\theta}_c=:
\pmb{\mathcal{A}}_c\pmb{\theta}_c=\int_{\tilde{F}}M(x)Y_c(x)\pmb{\mathcal{Z}}_c(x)dx
\end{equation}
 which satisfy by construction the two zero mean residuals properties
$\int_{\tilde{F}} M(x)\mathcal{R}_c(x)dx=\int_{\tilde{G}} M(x)\mathcal{R}_c(x)dx=0$.
The second equality will only hold approximately if $I_{c,G}(x)<1$ for some $x$ in $\tilde{G}$.\\
With
$\pmb{\mathcal{A}}_{c,s_2}
=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}_c^t(x)$
we obtain the estimate of the regression coefficients at the cluster level
\begin{equation}\label{regcoeffclusterext}
\hat{\pmb{\theta}}_{c,s_2}=\pmb{\mathcal{A}}^{-1}_{c,s_2}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}M(x)
\pmb{\mathcal{Z}}_c(x)Y(x)\Big)
\end{equation}
with estimated design-based covariance matrix
\begin{equation}\label{regcoeffclusterextestvar}
\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,s_2}}=
\pmb{\mathcal{A}}^{-1}_{c,s_2}\Big(\frac{1}{n^2_2}\sum_{\in{s_2}}M^2(x)\hat{\mathcal{R}}_c^2(x)
\pmb{\mathcal{Z}}_c(x)\pmb{\mathcal{Z}}^t_c(x)\Big)\pmb{\mathcal{A}}^{-1}_{c,s_2}
\end{equation}
where the $\hat{\mathcal{R}}_c(x)=Y_c(x)-\pmb{\mathcal{Z}}^t_c(x)\hat{\pmb{\theta}}_{c,s_2}$ are the empirical residuals at the cluster level with respect to the extended model. We define the pseudo-synthetic estimator in the extended model according to
\begin{equation}\label{estimatorclusterextended}
\hat{\tilde{Y}}_{c,G,psynth}=\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^t\hat{\pmb{\theta}}_{c,s_2}
\end{equation}
where
$\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}=\frac{\sum_{x\in{s_{1,G}}}M_G(x)\pmb{\mathcal{Z}}_{c,G}(x)}
{\sum_{x\in{s_{1,G}}}M_G(x)}$
is the mean of the extended auxiliary vector over the small area.\\
As in [\ref{estvarpseudosynth1cluster}] the estimated variance is given by
\begin{equation}\label{estvarpsynth1clusterextended}
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,psynth})=
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,s_2}}
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}
+ \hat{\pmb{\theta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}}\hat{\pmb{\theta}}_{c,s_2}
\end{equation}
\noindent where
\begin{equation}\label{estcovauxiliaryclusterextended}
\hat{\pmb{\Sigma}}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}}=\frac{1}{n_{1,G}(n_{1,G}-1)}
\sum_{x\in{s_{1,G}}}\big(\frac{M_G(x)}{\bar{M}_{1,G}}\big)^2(\pmb{\mathcal{Z}}_{c,G}(x)-
\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G})
(\pmb{\mathcal{Z}}_{c,G}(x)-\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G})^t
\end{equation}
\textbf{Remark}:\\
Even if the condition $M_G(x)\equiv M(x)$ is fulfilled in $s_{2,G}$ we cannot expect $\hat{\tilde{Y}}_{c,G,psynth}$ to be asymptotically unbiased (although the sample copy of the normal equation converges towards the theoretical normal equation this does not guarantee zero mean residuals over $G$ as we have seen). However the bias can be expected to be negligible.\\[0.5cm]
Obviously a decomposition similar to [\ref{extsynthethic3}] will hold so that $\hat{\tilde{Y}}_{c,psynth}$ and $\hat{Y}_{c,G,psmall}$ in [\ref{psmallclusterest},\ref{estimatorclusterextended}]
will be close to each other in large samples.\\
Defining the g-weights at the cluster level as
\begin{equation}
\tilde{g}_{c,1,G}(x)=\hat{\bar{\pmb{\mathcal{Z}}}}^t_{c,1,G}\pmb{\mathcal{A}}^{-1}_{c,s_2}M(x)
\pmb{\mathcal{Z}}_c(x)
\end{equation}
we obtain as usual
\begin{equation}\label{gweightclusterpointestextended}
\hat{\tilde{Y}}_{c,G,psynth}=\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{c,1,G}(x)Y_c(x)
\end{equation}
and
\begin{equation}\label{gweightclustervarestextended}
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,psynth})=\frac{1}{n_2^2}\sum_{x\in{s_2}}
\tilde{g}^2_{c,1,G}(x)\hat{\mathcal{R}}^2(x)+
\hat{\pmb{\theta}}_{c,s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{\mathcal{Z}}}}_{c,1,G}}\hat{\pmb{\theta}}_{c,s_2}
\end{equation}
The synthetic estimator in the extended model corresponds formally to $n_1=\infty$, i.e.
\begin{equation}
\hat{\tilde{Y}}_{c,G,synth}=\bar{\pmb{\mathcal{Z}}}_G^t\hat{\pmb{\theta}}_{c,s_2}
 \end{equation}
 with estimated variance
 \begin{equation}
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,synth})=
\bar{\pmb{\mathcal{Z}}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{c,s_2}}\bar{\pmb{\mathcal{Z}}}_{G}
\end{equation}
with obvious modification for the g-weights
\begin{eqnarray}
\tilde{g}_{c,G}(x)&=&\bar{\pmb{\mathcal{Z}}}_G^t\pmb{\mathcal{A}}^{-1}_{c,s_2}M(x)
\pmb{\mathcal{Z}}_c(x)\nonumber \\
\hat{\tilde{Y}}_{c,G,synth}&=&\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{c,G}(x)Y_c(x)\nonumber \\
\hat{\VAR}(\hat{\tilde{Y}}_{c,G,synth})&=&\frac{1}{n_2^2}\sum_{x\in{s_2}}
\tilde{g}^2_{c,G}(x)\hat{\mathcal{R}}^2(x)
\end{eqnarray}
\noindent
\textbf{Properties of the g-weights}:
\begin{enumerate}
\item
 We have
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{c,1,G}(x)\pmb{Z}_c(x)=
 \hat{\bar{\pmb{Z}}}_{c,1,G}$  and  $\frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{c,1,G}(x)\pmb{\mathcal{Z}}_c(x)=
 \hat{\bar{\pmb{\mathcal{Z}}}}_{1,G}$.
 \item
 By considering $Y_c(x)\equiv 1$ one gets
 $\frac{1}{n_2}\sum_{x\in{s_2}}g_{c,1,G}(x)=
  \frac{1}{n_2}\sum_{x\in{s_2}}\tilde{g}_{c,1,G}(x)=1$, i.e. the g-weights have means equal to 1.
 \item
  When $G=F$ the estimator $\hat{Y}_{c,F,psynth}$ and the sample mean $\frac{\sum_{x\in{s_2}}M(x)Y(x)}{\sum_{x\in{s_2}}M(x)}$ both converge towards the true value over $F$. Hence, for large $n_2$, $g_{c,1,F}(x)\approx \frac{M(x)}{\bar{M}_2}$ with $\bar{M}_2=\frac{1}{n_2}\sum_{x\in{s_2}}M(x)$. Likewise for $\hat{\tilde{Y}}_{G,psynth}$ and the sample mean over $G$, $\hat{\bar{Y}}_{c,2,G}=\frac{\sum_{x\in{s_{2,G}}}M_G(x) Y_{c,G}(x)}{\sum_{x\in{s_{2,G}}}M_G(x)}$. Hence, for large $n_2$ we get $\tilde{g}_{c,1,G}(x)\approx 0$ for $x\not\in{\tilde{G}}$ (negative values are possible) and to $\tilde{g}_{c,1,G}(x)\approx \frac{M_G(x)}{\bar{M}_{2,G}}$ for $x\in{\tilde{G}}$, where $\bar{M}_{2,G}=\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}M_G(x)$.
 \end{enumerate}
\textbf{Remarks:}\\
With simple random sampling the construction of design-unbiased small-area estimators as synthetic or pseudo-synthetic estimators in the extended model containing the indicator variable of the small area of interest is mathematically more convenient than the classical approach. The mathematical approximation of the variances is also more satisfactory than simply treating the internal model as an external one and it can be formulated within the g-weight technique, which is known to offer several theoretical advantages. The same essentially holds in cluster sampling, with a slight advantage maybe for $\hat{Y}_{c,G,psmall}$,with the external variance, in terms of simplicity and validity, since one cannot guarantee in general zero mean residuals over $G$ for the pseudo-synthetic estimator $\hat{\tilde{Y}}_{c,G,psynth}$ in the extended model. It is certainly good practice to calculate the different small-area estimators and their estimated variances, particularly for cluster sampling. Large differences could reveal unsuspected patterns in the data (or eventually even programming errors).\\
It is mathematically clear that one can generalize all the previous results to the simultaneous estimation of $q \ge 2$ small areas by extending the model with $q$ indicator variables (combined extended model). One can conjecture that the combined model will be less efficient, for any given small area, than the individual estimation and, on the other hand, that it will smooth out the residual pattern.\\
To calculate confidence intervals we recommend to use the Student's $T$ distribution on $ n_2$ d.f. for $F$ and $n_{2,G}-1$ d.f. for any small area $G$. If $n_{2,G}$ is to small one can use the pseudo-synthetic estimator $\hat{Y}_{c,G,psynth}$, which will yield short confidence intervals, at the cost of a potential bias. In this case it might instructive to embed $G$ in a larger domain $G^*$ with $n_{2,G^*}$ large enough and to calculate the pseudo-synthetic estimate for $G$ in the model extended with the indicator variable of $G^*$.
\newpage
\section{Generalization to two-stage sampling}
In many applications costs to measure the response variable $Y_i$ are high. For instance,
a good determination of the volume may require that one records
$DBH$, as well as the diameter at $7m$ above ground and total height
in order to utilize a three-way volume function. However, one could rely
on a coarser, but cheaper, approximation of the volume based only on
$DBH$. Nonetheless, it may be most sensible to assess those three
parameters only on a sub-sample of trees. We now briefly formalize this
simple idea, which is used in the Swiss National Forest Inventory. The reader is referred to (\cite{mandallaz}, section 4.4, 4.5, 5.4 and 9.5) for details. For each point $x\in{s_2}$ trees are drawn with
probabilities $\pi_i$. The set of selected trees is denoted by
$s_{2}(x)$. From each of the selected trees $i\in{s_{2}(x)}$ one
gets an approximation $Y_i^*$ of the exact value $Y_i$. From the
finite set $s_{2}(x)$ one draws a sub-sample
$s_{3}(x)\subset{s_{2}(x)}$ of trees by Poisson sampling. For each tree $i\in{s_{3}(x)}$
one then measures the exact variable $Y_i$. Let us now define the
second stage indicator variable
\begin{equation}
 J_i(x)=\begin{cases}&1 \text{ if $i\in s_{3}(x)$}\\
                      &0 \text{ if $i\not\in s_{3}(x)$}
         \end{cases}
\end{equation}

 \par To construct a good point estimate, we must have
 \textbf{the residual} $R_i=Y_i-Y_i^*$ which is known only for trees
 $i\in{s_{3}(x)}$. The \textbf{generalized local density} $Y^*(x)$ is defined
 according to    \index{Generalized local density}
 \begin{eqnarray}\label{gdens}
 Y^*(x)&=&\frac{1}{\LF}\left( \sum_{i=1}^N \frac{I_{i}(x)Y_i^*}{\pi_i} +
 \sum_{i=1}^N \frac{I_{i}(x)J_{i}(x)R_i}{\pi_{i}p_i}\right)\nonumber \\
 &=&\frac{1}{\LF}\left( \sum_{i\in{s_{2}(x)}} \frac{Y_i^*}{\pi_i}
 +\sum_{i\in{s_{3}(x)}} \frac{R_i}{\pi_{i}p_i}\right)
 \end{eqnarray}
 where the $p_i$ are the conditional inclusion probabilities for the the second stage sampling, i.e. $p_i=\PR(J_i(x)=1 \mid I_i(x)=1)$.
 It follows from general principles presented in (\cite{mandallaz}, sections 4.4 and 4.5) that one can use all the previous results by replacing everywhere the exact local densities $Y(x)$, or $Y(x_l)$ in cluster sampling, by the corresponding generalized local densities $Y^*(x)$ or $Y^*(x_l)$. The second-stage variance is automatically taken into account. More mathematical details are given in \cite{mandallazreport5}.
 \newpage
\section{Examples}
\subsection{Post-stratification}\label{poststrat}
We consider the important special case of post-stratification, which illustrates the main issues. We consider a forested area $F$ partitioned in $L$ strata $F_k$, i.e. $F=\cup_{k=1}^L F_k$ and a small area $G\subset F$, we set $G_k=G \cap F_k$. Note some $G_k$ might be the empty set. The auxiliary vector is defined by the indicator variables of the $L$ strata, i.e.
$$\pmb{Z}^t(x)=(I_{F_1}(x),I_{F_2}(x),\ldots I_{F_L}(x))$$
 where $I_{F_k}(x)=1$ if $x\in{F}_k$ otherwise $I_{F_k}(x)=0$. Note that condition [\ref{linearmodel2}] is fulfilled. Straightforward calculations lead to the $(L,L)$ diagonal matrix $\pmb{A}_{s_2}=\frac{1}{n_2}diag(n_{2,k})$ where $n_{2,k}=\sum_{x\in{s_2}}I_{F_k}(x)$. This leads to the obvious regression estimate
 $$\hat{\pmb{\beta}}_{s_2}=(\hat{\beta}_1,\hat{\beta}_2,\ldots \hat{\beta}_L)^t$$
 with the empirical strata means $\hat{\beta}_k=\frac{1}{n_{2,k}}\sum_{x\in{s_2}\cap F_k}Y(x)=\hat{\bar{Y}}_k$.
 After some elementary algebra the estimated variance-covariance matrix is found to be the diagonal $(L,L)$ matrix
 $$\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}=diag(\frac{s_k^2}{n_{2,k}})$$
 where
 $s_k^2=\frac{1}{n_{2,k}}\sum_{x\in{F_k}}\hat{R}^2(x)$ with $\hat{R}(x)=Y(x)-\hat{\beta}_k$ for $x\in{F}_k$.\\
 One obtains for the empirical mean of the auxiliary vector over the small area
 $$\hat{\bar{\pmb{Z}}}_{1,G}=(\hat{p}_{1,G},\hat{p}_{2,G},\ldots, \hat{p}_{L,G})^t=\hat{\pmb{p}}_{1,G}$$
 where
 $$\hat{p}_{k,G}=\frac{\sum_{x\in{s_1}}I_{G_k}(x)}{\sum_{x\in{s_1}}I_G(x)}:=\frac{n_{1k,G}}{n_{1,G}}$$
 \noindent are the proportions of the strata surfaces areas within the small area as estimated from the large sample. Conditionally on $n_{1,G}$ the $n_{1k,G}$ follow the multinomial distribution with cell probabilities given by the vector
 $\pmb{p}^t_{G}=(\frac{\lambda(G_1)}{\lambda(G)},\frac{\lambda(G_2)}{\lambda(G)},\ldots \frac{\lambda(G_L)}{\lambda(G)})$. In this case the estimated variance-covariance matrix is known to be given by
 \begin{equation}\label{covmatrixcellfreq}
 \hat{\pmb{\Sigma}}_{\hat{\pmb{p}}_{1,G}}=
\frac{1}{n_{1,G}}\left[ \begin {array}{cccc}
\hat{p}_{1,G}(1-\hat{p}_{1,G})& \hat{p}_{1,G}\hat{p}_{2,G} &\ldots& \hat{p}_{1,G}\hat{p}_{1,L}\\
\hat{p}_{1,G}\hat{p}_{2,G}& \hat{p}_{2,G}(1-\hat{p}_{2,G}) &\ldots& \hat{p}_{2,G}\hat{p}_{1,L}\\
\ldots & \ldots & \ldots & \ldots \\
\hat{p}_{1,G}\hat{p}_{1,L} & \hat{p}_{2,G}\hat{p}_{1,L}& \ldots & \hat{p}_{1,L}(1-\hat{p}_{1,L}) \end {array} \right]
\end{equation}
Note that the same is obtained by using [\ref{estvarcovaux}] after replacing $n_{1,G}-1$ by $n_{1,G}$. Simple algebra leads then to the pseudo-synthetic estimate
\begin{equation}\label{postsynth1}
\hat{Y}_{G,psynth}=\hat{\pmb{p}}_{1,G}^t\hat{\pmb{\beta}}_{s_2}=\sum_{k=1}^L\hat{p}_{k,G}\hat{\beta}_k
\end{equation}
with estimated asymptotic design-based variance
\begin{equation}\label{postsynth1}
\hat{\VAR}(\hat{Y}_{G,psynth})=\sum_{k=1}^L\hat{p}^2_{k,G}\frac{s_k^2}{n_{2,k}} +
\frac{1}{n_{1,G}}\sum_{k=1}^L \hat{p}_{k,G}\big(\hat{\beta}_k-\hat{Y}_{G,psynt}\big)^2
\end{equation}
When $n_1=\infty$ and $G=F$ this is precisely the exact conditional variance estimate, i.e. given the $n_k$. The g-weights are $g_{s_2}(x)=p_k\frac{n_2}{n_{2,k}}$ for $x\in{F}_k$, where $p_k=\frac{\lambda(F_k)}{\lambda(F)}$. Thus, for $n_1 < \infty$ the overall variance will depend on the variances within strata and on the variance between strata, which is given by the second term. Note also that for $G=F$ in [\ref{postsynth1}] the strata weights are estimated from the large sample whereas this is not the case for the external model approach [\ref{varexternalsimple}], which illustrates perfectly the better conditional properties of the g-weights technique (see \cite{mandallaz}, p.84).\\
\noindent\textbf{Remarks}\\
\noindent If we assume the $\lambda(F_k)$ and therefore $\pmb{A}$ to be known, the estimator $$\hat{\pmb{\beta}}_0=\pmb{A}^{-1}\Big(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)\Big)$$ is easily found to be $(\frac{n_{21}}{n_2p_1}\hat{\bar{Y}}_1, \frac{n_{22}}{n_2p_2}\hat{\bar{Y}}_2,\ldots \frac{n_{2L}}{n_2p_L}\hat{\bar{Y}}_L)^t$ and yields $\hat{Y}_{synth}=\frac{1}{n_2}\sum_{\in{s_2}}Y(x)$, which is unbiased but useless. If we use $\pmb{A}_{s_2} $ to estimate $\pmb{\beta}$ we get as shown above $\hat{\beta}_k=\hat{\bar{Y}}_k$, which is very intuitive, and if we use $\pmb{A}^{-1}$ instead of $\pmb{A}^{-1}_{s_2}$ in the Taylor approximation for the variance, we obtain $\hat{\VAR}(\hat{Y}_{synth})=\sum_{k=1}^L\frac{n_{2k}s_k^2}{n^2_2}$ instead of $\sum_{k=1}^L p_k^2 \frac{s_k^2}{n_{2k}}$, the later is of course much better from a conditional point of view (even if both estimates are asymptotically equivalent). This examples illustrates why it is better to work with $\pmb{A}^{-1}_{s_2}$ throughout.
\\
It can be easily checked that the original small-area estimator is given by
$$\hat{Y}_{G,psmall}=\sum_{k=1}^L \hat{p}_{k,G}\hat{\bar{Y}}_k+\sum_{k=1}^L\frac{n_{2k,G}}{n_{2,G}}
(\hat{\bar{Y}}_{k,G}-\hat{\bar{Y}}_k)$$
where $n_{2k,G}=\sum_{x\in{s_2}}I_{G_k}(x)$ and $\hat{\bar{Y}}_{k,G}=\frac{1}{n_{2k,G}}\sum_{x\in{s_2 \cap G_k}}Y(x)$. Thus, the residual term will have an impact if the strata means within the small area differ from the strata means within the entire domain, which is intuitively clear.
\\ The formulae for the variances are very cumbersome and not really informative, likewise for
$\hat{\tilde{Y}}_{G,psynth}$ in the extended model.
\newpage
\subsection{Case study}
We reanalyze the case study described with in Chapter 9 of \cite{mandallaz}. The inventoried area covered $218ha$.  The auxiliary information is based on 16 stands defined by the following qualitative variables obtained from the manual interpretation of aerial photographs:
\begin{enumerate}
\item
\textbf{Developmental stage} \newline This entails four categories
``pole stage=3," ``young timber tree=4," ``middle age timber
tree=5," and ``old timber tree=6." These were assigned according to
the dominant diameter.
\item
\textbf{Degree of mixture} \newline This variable was simplified to
the categories of ``predominantly conifers=1" and ``predominantly
broadleaves=2."
\item \textbf{Crown closure} \newline
This variable was based on canopy density, defined as the proportion
of the entire ground surface within the stand that was covered by
the tree crowns. It was simplified to the categories of ``dense=1"
and ``close=2."
\end{enumerate}
These factors produced $4 \times 2 \times 2=16$ possible stands, all
of which were found on the study site. \\
The inventory utilized systematic cluster sampling. The cluster
comprises five points: central point, two points each established
$30\;m$ east or west of the central point; two other points each
established $40\;m$ either north or south of the central point.\\
The first phase sets the
central cluster point on a $120\;m$ W-E by $75\;m$ N-S rectangular
grid (note that the clusters partially overlapped in the N-S
direction). The second, terrestrial phase, place the central point on a 1:4
sub-grid of the first phase, i.e. on a $240\;m$ W-E by $150\;m$ N-S
systematic rectangular grid. The terrestrial inventory was purely one-stage
with simple circular plots of $300m^2$ horizontal surface area, and
an inventory threshold set at 12cm DBH.\\
We use the following linear model with the
vector $\pmb{Z}(x)$:

\begin{itemize}
\item
$Z_1(x) \equiv 1$ intercept term
\item
$Z_2(x)=1$ if $x$ lies in Development Stage 3 and $Z_2(x)=0$
otherwise
\newline $Z_3(x)=1$ if $x$ lies in Development Stage 4 and $Z_3(x)=0$
otherwise
\newline $Z_4(x)=1$ if $x$ lies in Development Stage 5 and $Z_4(x)=0$
otherwise
\newline $Z_2(x)=Z_3(x)=Z_4(x)=-1$ if $x$ lies in Development Stage
6
\item
$Z_5(x)=1$ if $x$ lies in a coniferous stand and $Z_5(x)=-1$
otherwise
\item
$Z_6(x)=1$ if $x$ lies in a dense stand and $Z_6(x)=-1$ otherwise
\end{itemize}
Hence, we have an additive ANOVA model with 7 parameters, as compared with 16 parameters for the full stratification model. The coefficient of determination $R^2$ (in the classical model-dependent approach, i.e. without taking the cluster structure into account) was satisfactory for the stem density ($0.53$) but not for the basal area $(0.19)$. This case study was performed in 1989 and is the only data available for cluster sampling in Switzerland (the reason for using cluster sampling was to provide good estimates of the variograms for the geostatistical Kriging procedures, see \cite{mandallaz} and \cite{dmhabil} for details). It should be emphasized that with modern remote sensing techniques (e.g. LiDAR or digital aerial photographs) one can obtain much better $R^2$'s, e.g.  around $0.70$ or more for timber volume (mean canopy height being the most important explanatory variable).\\
We shall consider 5 small areas:
\begin{itemize}
\item
Small area $G_1$ ($\approx 17ha$) was used for a full census. The condition that a cluster hitting the small area has all its points in $F$ within the small area is occasionally  violated (i.e. $I_{c,G}(x)=\frac{\sum_{l=1}^M I_G(x_l)}{M(x)} <1$ for some $x$), so that the extended model for $G_1$ is only approximately correct. The mean residual over the small area is not exactly zero. The true values for basal area and stem densities are known.
\item
Small area $G_2$ ($\approx 33ha$) is the most eastern part of the forest.
\item
Small area $G_3$ ($\approx 46ha$) is the most southern part of the forest.
\item
Small area $G_4$ ($\approx 55ha$) is the central part north of $G_3$.
\item
Small area $G_5$ ($\approx 84ha$) is the most western part north of of $G_3$.
\end{itemize}
We have $F=G_2\cup G_3\cup G_4\cup G_5$ and $I_{c,G_k}(x) \equiv 1$ for $k=2,3,4,5$.
Fig. \ref{zbergterrestrial} displays the terrestrial plots according to the domains $G_2-G_5$. Stand map of $F$ and detailed maps of $G_1$ are given in \cite{mandallaz} Chapter 8.
\begin{figure}[h]
\begin{center}
\caption{\label{zbergterrestrial}\textbf{Location of terrestrial plots in $G_2-G_5$}}
\end{center}
\begin{center}
\includegraphics[scale=0.50]{domain2.jpg}
\end{center}
\end{figure}

Tables \ref{result1} and \ref{result3} display the results for the basal area and the stem density for the small areas $G_1$, $G_2$, $G_3$, $G_4$ and $G_5$.\\
The external variances for $\hat{Y}_{c,G,psmall}$ are given by the equations
\begin{equation*}
\hat{Y}_{c,G,psmall}=\frac{\sum_{x\in{s_{1,G}}}M_G(x)\hat{Y}_{c,G}(x)}{\sum_{x\in{s_{1,G}}}M_G(x)}
+\frac{\sum_{x\in{s_{2,G}}}M_G(x)\hat{R}_{c,G}(x)}{\sum_{x\in{s_{2,G}}}M_G(x)}
\end{equation*}
and
\begin{eqnarray*}
\widehat{\VAR}_{ext}(\widehat{Y}_{c,G,psmall})&=&\left(1-\frac{n_{2,G}}{n_{1,G}}\right)
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}
\sum_{x\in{s_{2,G}}}\left(\frac{M_G(x)}{\bar{M}_{2,G}}\right)^{2}(\hat{R}_{c,G}(x)-\hat{\bar{R}}_{c,2,G})^2\\
&+&
\frac{1}{n_{1,G}}\frac{1}{(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}\left(\frac{M_G(x)}{\bar{M}_{2,G}}
\right)^{2}(Y_{c,G}(x)-\hat{Y}_{c,2,G})^2
\end{eqnarray*}
The resulting standard errors are given in $(-)$.\\
Tables \ref{result1} (basal area) and \ref{result3} (density of stems) summarize the results for the sample mean $\hat{\bar{Y}}_G$, the pseudo-synthetic estimator $\hat{Y}_{c,G,psynth}$, the pseudo small-area estimator $\hat{Y}_{c,G,psmall}$ and the pseudo-synthetic estimator in the extended model  $\hat{\tilde{Y}}_{c,G,psynth}$ for $F$ and the small areas $G_1$, $G_2$, $G_3$, $G_4$, $G_5$. \\
The extended model for the $\hat{\tilde{Y}}_{c,G_k,psynth}$ contains only the indicator variable of the corresponding small area $G_k$. For this reason the corresponding estimates for the entire domain $F$ depend on the selected $G_k$ and they are given separately in Tables \ref{result2} and \ref{result4}.\\
 We also consider the joint estimation of $F$ and the small areas $G_2$, $G_3$, $G_4$ and $G_5$, which form a partition of $F$. The corresponding model $\pmb{\mathcal{Z}}(x)$ contains the $4$ indicator variables $I_{G_k}(x)$ ($k=2,3,4,5$), the previous components $Z_l(x), l=2,3,4,5,6$ but no longer the intercept term $Z_1(x) \equiv 1$ (otherwise $\pmb{\mathcal{A}}_{s_2}$ would be singular because $Z_1(x)$ is a linear combination of the $I_{G_k}(x)$). The results for this estimator, denoted by $ \hat{\tilde{Y}}_{c,G,cpsynth}$, are displayed in Table  \ref{result5}.\\
 All the calculations were performed with the linear algebra procedure \textbf{proc IML} of the statistical software package SAS.

\section{Discussion and conclusions}
 The results for the basal area are disappointing, except maybe for global estimation, in the sense that the regression estimators, with the exception of the pseudo-synthetic estimator $\hat{Y}_{c,G,psynth}$, frequently have a larger error than the sample mean. This is of course due to the very low coefficient of determination $R^2=0.19$ and we shall therefore not pursue the discussion for basal area, simply noting that all point estimates were close to each other, with the exception of the small area $G_1$ with the full census, for which the pseudo-synthetic estimator was closer to the true values (also for the density of stems). As confirmed by simulations this was due to the fact that the plots within this small area were in the lower tail of the distribution for both basal area and the density of stem. \\
As expected for the stem density (with an acceptable $R^2=0.53$) the two-phase estimators perform much better than the sample mean. For the classical small-area estimator $\hat{Y}_{c,G,psmall}$ the standard errors based on the external model assumption were smaller than their counterparts based on the g-weights (i.e. on the design-based covariance matrix of the regression coefficients) for $F,G_3,G_4,G_5$ and slightly larger for $G_1,G_2$, but the differences were small, a reassuring result. The point estimates of  $\hat{Y}_{c,G,psmall}$ were very close to the point estimates of the pseudo-synthetic estimator $\hat{\tilde{Y}}_{c,G,psynth}$ (in the extended model with the single indicator variable of the small area of interest), which has slightly smaller g-weights standard errors, except for $F$ and $G_1$. From a practical point of view $\hat{Y}_{c,G,psmall}$ and $\hat{\tilde{Y}}_{c,G,psynth}$ are essentially equivalent. The estimators $\hat{\tilde{Y}}_{c,G,psynth}$, $\hat{Y}_{c,G,psmall}$ and the combined estimator $\hat{\tilde{Y}}_{c,G,cpsynth}$ (model without intercept but extended with the 4 indicator variables of $G_2,G_3,G_4,G_5$) were also practically equivalent for $F,G_2,G_3,G_4,G_5$, the combined estimator having slightly smaller g-weights standard errors.\\
The pseudo-synthetic estimator $\hat{Y}_{c,G,psynth}$ yields point estimates close to those of
$\hat{Y}_{c,G,psmall}$, $\hat{\tilde{Y}}_{c,G,psynth}$, $\hat{\tilde{Y}}_{c,G,cpsynth}$ (recall that
$\hat{Y}_{c,F,psmall}=\hat{Y}_{c,F,psynth}$) for $F, G_2,G_3,G_4,G_5$ but, of course, with much smaller g-weights based standard errors, at a cost of a potential bias. For $G_1$ the point estimate $\hat{Y}_{c,G_1,psynth}$ was very close to the true value (for the density of stem and the basal area), precisely because the clusters hitting $G_1$ were, as afore mentioned, not "representative" for $G_1$. This can happen!\\
From a mathematical point of view the g-weights technique in the models extended by the indicator variables of the small areas is without any doubts the most elegant approach in simple random sampling: it bypasses the residuals terms and allows for a straightforward calculation of the asymptotic variance that takes into account the errors of the regression coefficients. The same is likely to hold in large sample also with cluster sampling. The results from the case study show that the differences between the asymptotically unbiased estimators are very small from a practical point of the view, and that a potentially biased estimator can by chance be closer to the truth.

\newpage

\bibliography{biblio1}
\newpage
\begin{table}[h]
\begin{center}
\caption{ \label{result1}\textbf{Results for basal area}}
\end{center}
\begin{center}
 \begin{tabular}{l|c|c|c|c|c|}\hline
  \textbf{Domain}     & $n_1:n_2$  & $\hat{\bar{Y}}_G$ & $\hat{Y}_{c,G,psynth}$  & $\hat{Y}_{c,G,psmall}$ & $ \hat{\tilde{Y}}_{c,G,psynth}$ \\[1.0ex]
  & $n_1\bar{M}_1:n_2\bar{M}_2$   & &   & & \\[1.0ex]
  $ $ & $ $ & $ $ &  $ $ & $ $ &  \\ \hline\hline
 $F$     & $ 298:73$     & $31.90$   &  $31.34$     &  $31.34$       & see Table \ref{result2}    \\ [1.0ex]
         & $1203:298$    & $(1.08)$  &  $[0.94]$    &  $[0.94]$      & $ $     \\ [1.0ex]
         &               &           &  $(0.89)$    &  $(0.89)$      & $ $     \\ \hline\hline
 $G_1$   & $29:8$        &  $24.54$  & $30.28$      &  $23.99$       & $25.55$            \\ [1.0ex]
 true=29.60  & $92:19$   &  $(3.59)$ & $[1.37]$      &  $[3.96]$      & $[3.76]$          \\ [1.0ex]
         & $ $           &           &              &  $(3.68)$      &  $ $             \\ \hline \hline
 $G_2$ & $49:9$         & $30.69$     &  $28.27$    &  $29.32 $      & $29.31$     \\ [1.0ex]
       & $185:41$       & $(2.05)$    & $[1.47]$    &  $[2.55]$      & $[2.33]$     \\ [1.0ex]
       & $ $            &   $ $   &   $ $           &  $(2.08)$      &  $ $         \\ \hline \hline
 $G_3$ & $73:18$   &  $32.32$         & $31.62$     &  $31.46$       & $31.46$      \\ [1.0ex]
       &$250:66$   &  $(2.05)$        & $[1.54]$    &  $[2.37]$      & $[2.18] $    \\[1.0ex]
       &           &   $ $   &                      &  $(1.87)$      & $ $               \\ \hline \hline
 $G_4$ & $81:17$   &  $27.63$         & $29.72$     &  $27.78$       & $27.77$          \\ [1.0ex]
       & $306:69$  &  $(2.15)$        & $[1.15] $   &  $[2.27]$      & $[1.98]$        \\ [1.0ex]
       & $ $       &  $ $    &  $ $                 &  $(2.00)$      & $ $            \\ \hline \hline
 $G_5$ &  $125:29$ &  $34.49$         &  $33.50$    & $34.33$        & $34.33$           \\ [1.0ex]
       & $462:122$ &  $(1.78)$        &$[1.00]$     & $[1.55]$       & $[1.30]$           \\ [1.0ex]
       &           &  $ $             & $$          & $(1.35)$       & $ $    \\    \hline \hline

\end{tabular}
\end{center}
\textbf{Design-based standard errors:} $[-]$ (g-weights based) and $(-)$ (based on external or sample variance)\\
For $G_1$ the mean residual of $ \hat{\tilde{Y}}_{c,G,psynth}$ was $-1.59$ and $0$, as expected, for all other domains.
\end{table}

\begin{table}[h]
\begin{center}
\caption{ \label{result2}\textbf{Results basal area for $F$ with extended models}}
\end{center}
\begin{center}
 \begin{tabular}{|c|c|c|c|c|}\hline
  $\hat{\tilde{Y}}^{(1)}_{c,F,psynth}$  & $\hat{\tilde{Y}}^{(2)}_{c,F,psynth}$& $\hat{\tilde{Y}}^{(3)}_{c,F,psynth}$  & $\hat{\tilde{Y}}^{(4)}_{c,F,psynth}$ &  $\hat{\tilde{Y}}^{(5)}_{c,F,psynth}$ \\[1.0ex] \hline \hline
  $31.30$    & $31.36$   & $31.35$   & $31.30$  & $31.33$ \\[1.0ex]
  $[0.92] $  & $[0.93]$  & $[0.94]$  & $[0.92]$ & $[0.94]$ \\[1.0ex] \hline \hline
\end{tabular}
\end{center}
$\hat{\tilde{Y}}^{(k)}_{c,F,psynth}$ is the pseudo-synthetic estimate for $F$ in the model extended with the indicator variable of domain $G_k$, $k=1,2,3,4,5$. The g-weights based standard errors are given in $[-]$.
\end{table}

\newpage


\begin{table}[h]
\begin{center}
\caption{ \label{result3}\textbf{Results for stem density}}
\end{center}
\begin{center}
 \begin{tabular}{l|c|c|c|c|c|}\hline
  \textbf{Domain}     & $n_1:n_2$  & $\hat{\bar{Y}}_G$ & $\hat{Y}_{c,G,psynth}$  & $\hat{Y}_{c,G,psmall}$ & $ \hat{\tilde{Y}}_{c,G,psynth}$ \\[1.0ex]
  & $n_1\bar{M}_1:n_2\bar{M}_2$   & &   & & \\[1.0ex]
  $ $ & $ $  & $ $ & $ $   & $ $  &  \\ \hline\hline
 $F$  &    $ 298:73$ & $321.03$  &  $325.79$   &  $325.79 $       & see Table \ref{result4}     \\ [1.0ex]
      & $1203:298$   & $(18.44)$ & $[12.80 ]$  &  $[12.80 ]$      & $ $     \\ [1.0ex]
      &              &           & $(12.09 )$  &  $(12.09)$      & $ $     \\ \hline\hline
 $G_1$ & $29:8$      & $245.61$  & $279.54 $   &  $257.34$       & $258.20$            \\ [1.0ex]
 true=$280.23$ & $92:19$ & $(65.51)$ & $[27.26]$   &  $[48.26]$       & $[49.56]$          \\ [1.0ex]
       & $ $         &           &             &  $(48.29 )$      &  $ $         \\ \hline \hline
 $G_2$ & $49:9$      & $393.50$  & $400.49 $   &  $406.47 $       & $406.41$     \\ [1.0ex]
       & $185:41$    & $(79.69)$ &$ [30.95 ]$  &  $[42.96 ]$      & $[39.36 ]$     \\ [1.0ex]
       & $ $       &   $ $       &   $ $       &  $(43.49 )$      &   $ $         \\ \hline \hline
 $G_3$ & $73:18$   &  $275.25$   & $279.75 $   &  $282.46 $       & $282.40$      \\ [1.0ex]
       &$250:66$   &  $(20.98)$  & $[17.16 ]$  &  $[22.68 ]$      & $[20.77]$    \\[1.0ex]
       &           &   $ $       &             &  $(16.56)$       & $ $       \\ \hline \hline
 $G_4$ & $81:17$   &  $284.06$   &  $307.85$   &  $274.79$        & $274.53 $          \\ [1.0ex]
       & $306:69$  &  $(39.51)$  &  $[19.69]$  &  $[26.64]$       & $[23.55]$        \\ [1.0ex]
       & $ $       &  $ $        &  $ $        &  $(24.12)$       & $ $            \\ \hline \hline
 $G_5$ &  $125:29$ &  $342.35$   &  $332.66$   &  $347.89$        & $347.91 $           \\ [1.0ex]
       & $462:122$ &  $(24.50)$  &  $[15.33]$  &  $[21.26]$       & $[18.65]$           \\ [1.0ex]
       &           &  $ $     & $$             &  $(17.49)$       & $ $       \\    \hline \hline

\end{tabular}
\end{center}
\textbf{Design-based standard errors:} $[-]$ (g-weights based) and $(-)$ (based on external or sample variance)\\
For $G_1$ the mean residual of $ \hat{\tilde{Y}}_{c,G,psynth}$ was $-0.99$ and $0$, as expected, for all other domains.
\end{table}


\begin{table}[h]
\begin{center}
\caption{ \label{result4}\textbf{Results for stem density for $F$ with extended models}}
\end{center}
\begin{center}
 \begin{tabular}{|c|c|c|c|c|}\hline
  $\hat{\tilde{Y}}^{(1)}_{c,F,psynth}$  & $\hat{\tilde{Y}}^{(2)}_{c,F,psynth}$& $\hat{\tilde{Y}}^{(3)}_{c,F,psynth}$  & $\hat{\tilde{Y}}^{(4)}_{c,F,psynth}$ &  $\hat{\tilde{Y}}^{(5)}_{c,F,psynth}$ \\[1.0ex] \hline \hline
  $325.62$    & $325.88$  & $325.72$   & $325.15$   & $325.56$ \\[1.0ex]
  $[12.81]$   & $[12.84]$ & $[12.85]$  & $[12.67]$  & $[12.62]$ \\[1.0ex] \hline \hline
\end{tabular}
\end{center}
$\hat{\tilde{Y}}^{(k)}_{c,F,psynth}$ is the pseudo-synthetic estimate for $F$ in the model extended with the indicator variable of domain $G_k$, $k=1,2,3,4,5$. The g-weights based standard errors are given in $[-]$.
\end{table}

\begin{table}[h]
\begin{center}
\caption{ \label{result5}\textbf{Two-phase combined estimates}}
\end{center}
\begin{center}
 \begin{tabular}{|l|c|c|c|}
   \textbf{Domain} & \textbf{sample sizes} & \textbf{basal area}  & \textbf{stem density}\\[1.0ex]
 $ $   & $n_1:n_2$    &  & \ \\[1.0ex]
 $ $   & $n_1\bar{M}_1:n_2\bar{M}_2$  & $ \hat{\tilde{Y}}_{c,G,cpsynth}$ &$\hat{\tilde{Y}}_{c,G,cpsynth}$ \\ \hline
 $F$            & $ 298:73$   &  $31.32 $    &  $325.17$       \\ [1.0ex]
                & $1203:298$  &  $[0.93]$    &  $[12.62]$       \\ \hline \hline
 $G_2$          &  $49:9$     &  $29.39 $    &  $407.84$        \\ [1.0ex]
                &  $185:41$   &  $[2.23]$    &  $[39.04]$       \\ \hline \hline
 $G_3$          &  $73:18$     &  $31.57 $    &  $284.17$         \\ [1.0ex]
                & $250:66$    &  $[2.09]$    &  $[18.09]$       \\ \hline \hline
 $G_4$          &  $81:17$     &  $27.77 $    &  $274.59$        \\ [1.0ex]
                & $306:69$    &  $[1.99]$   &  $[23.49]$        \\ \hline \hline
 $G_5$          &  $125:29$    &  $34.31$    &  $347.76$          \\ [1.0ex]
                & $462:122$   &  $[1.24]$   &  $[16.61]$         \\ \hline \hline
\end{tabular}
\end{center}
The g-weights based standard errors are given in $[-]$. As expected on mathematical grounds all extended models yielded empirical means of the residuals over the entire domain and over one or many small areas which were equal to zero ($<10^{-12})$.
\end{table}






\end{document}




















\begin{table}[h]
\begin{center}
\caption{ \label{newresult1}\textbf{Two-phase estimates for basal area}}
\end{center}
\begin{center}
 \begin{tabular}{|l|c|c|c|c|}\hline
                      & \textbf{sample sizes} & $\hat{Y}_{c,G,psynth}$   &$ \hat{Y}_{c,G,psmall}$ & $\hat{\tilde{Y}}_{c,G,psynth}$\\[1.0ex]
                & $n_1:n_2$  &   &  &  \\[1.0ex]
 \textbf{Domain}& $n_1\bar{M}_1:n_2\bar{M}_2$ &   &  &  \\ \hline \hline
 $F$            &  $ 298:73$ &  $31.34$    &  $31.34$    & $31.30[0.92]$   \\ [1.0ex]
                & $1203:298$ &  [0.94]     &  $[0.94]$   & $31.35[0.93]$ \\ [1.0ex]
                &            &             &  $(0.91)$   & $31.35[0.94]$  \\ \hline \hline
 $G_1$          &  $29:8$    &  30.28    &  $23.99$              & $25.55$     \\ [1.0ex]
  true=$29.60$  &  $92:19$   &  [1.34]   &  $[3.90]$             & $[3.79]$   \\ [1.0ex]
                &            &           &  $(3.68)$             &             \\ \hline\hline
 $G_2$          &  $49:9$    &  28.27    &   $29.32$             & $29.31$     \\ [1.0ex]
                &  $185:41$  &  [1.40]   &  $[2.52]$             & $[2.23]$  \\ [1.0ex]
                &            &           &  $(2.08)$             &            \\ \hline
 $G_{21}\subset G_2$ &$17:3$ & $25.55 $  &   $29.52 $            & $ 29.61$     \\ [1.0ex]
                 &  $39:15$   &  $[2.16]$     &  $[ 4.13]$        & $[2.95]$  \\ [1.0ex]
                &            &               &  $(3.53)$         &            \\ \hline \hline
  $G_3$         &  $73:18$   &  31.62    &  $31.46$              & $31.46$  \\ [1.0ex]
                & $250:66$   &  [1.47]   &   $[2.33]$            & $[2.16] $ \\[1.0ex]
                &             &            &  $(1.87)$            & \\ \hline \hline
\end{tabular}
\end{center}
\textbf{Standard errors:} $[-]$ (Taylor, g-weights) and $(-)$ (external model)
\end{table}
\noindent \textbf{Remark:}\\
The mean residual for small area $G_1$ was $-1.59$ for the extended model instead of $0$ because $I_{c,G}(x) \not\equiv 1$.

\newpage
\begin{table}[h]
\begin{center}
\caption{ \label{newresult2}\textbf{Two-phase estimates for stem density}}
\end{center}
\begin{center}
 \begin{tabular}{|l|c|c|c|c|}\hline
        & \textbf{sample sizes} & $\hat{Y}_{c,G,psynth}$  & $\hat{Y}_{c,G,psmall}$ & $ \hat{\tilde{Y}}_{c,G,psynt}$ \\[1.0ex]
        & $n_1:n_2$   & &   &  \\[1.0ex]
    \textbf{Domain} & $n_1\bar{M}_1:n_2\bar{M}_2$ &   &  &  \\ \hline\hline
 $F$            & $ 298:73$  &  $325.79$    &  $325.79$       & $325.62[12.81]$     \\ [1.0ex]
                & $1203:298$ &  $[12.80]$   &  $[12.80]$      & $325.88[12.84]$     \\ [1.0ex]
                &            &              &  $(12.39)$      & $325.72[12.85]$ \\ \hline\hline
 $G_1$          &  $29:8$    &  $279.54$    &  $257.34$              & $258.20$     \\ [1.0ex]
 true=$280.23$  &  $92:19$   &  $[22.65]$   &  $[45.81]$             & $[54.07]$   \\ [1.0ex]
                &            &              &  $(48.29)$             &    \\ \hline\hline
 $G_2$          &  $49:9$    &  $400.49$    &  $406.47$              & $406.41 $     \\ [1.0ex]
                & $185:41$   &  $[23.36]$   &  $[41.83]$             & $[36.22 ]$  \\ [1.0ex]
                &            &              &  $(43.49)$             &            \\ \hline
  $G_{21}\subset G_2$ &$17:3$    &  $578.90$    &  $589.51$           & $589.74 $ \\ [1.0ex]
                    & $39:15$   &  $[35.48]$   &  $[85.68]$          & $[67.16]$  \\ [1.0ex]
                &            &                 &  $(94.31)$          &            \\ \hline\hline
 $G_3$          &  $73:18$   &  $279.75$    &  $282.46$              & $282.40$  \\ [1.0ex]
                & $250:66$   &  $[15.41]$   &  $[21.38]$             & $[20.14 ] $ \\[1.0ex]
                &            &              &  $(16.56)$             & \\ \hline\hline
\end{tabular}
\end{center}
\textbf{Standard errors:} $[-]$ (Taylor, g-weights) and $(-)$ (external model)
\end{table}
\noindent \textbf{Remark:}\\
\noindent The mean residual for small area $G_1$ was $-1.00$ for the extended model instead of $0$ because $I_{c,G}(x) \not\equiv 1$.

\newpage
\begin{table}[h]
\begin{center}
\caption{ \label{newresult3}\textbf{Two-phase combined estimates}}
\end{center}
\begin{center}
 \begin{tabular}{|l|c|c|c|}
   \textbf{Domain} & \textbf{sample sizes} & \textbf{basal area}  & \textbf{stem density}\\[1.0ex]
 $ $   & $n_1:n_2$    &  & \ \\[1.0ex]
 $ $   & $n_1\bar{M}_1:n_2\bar{M}_2$  & $ \hat{\tilde{Y}}_{c,G,cpsynth}$ &$\hat{\tilde{Y}}_{c,G,cpsynth}$ \\ \hline
 $F$            & $ 298:73$   &  $31.32 $    &  $325.17$       \\ [1.0ex]
                & $1203:298$  &  $[0.93]$    &  $[12.62]$       \\ \hline \hline
 $G_2$          &  $49:9$     &  $29.39 $    &  $407.84$        \\ [1.0ex]
                &  $185:41$   &  $[2.23]$    &  $[39.04]$       \\ \hline \hline
 $G_3$          &  $73:18$     &  $31.57 $    &  $284.17$         \\ [1.0ex]
                & $250:66$    &  $[2.09]$    &  $[18.09]$       \\ \hline \hline
 $G_4$          &  $81:17$     &  $27.77 $    &  $274.59$        \\ [1.0ex]
                & $306:69$    &  $[1.99]$   &  $[23.49]$        \\ \hline \hline
 $G_5$          &  $125:29$    &  $34.31$    &  $347.76$          \\ [1.0ex]
                & $462:122$   &  $[1.24]$   &  $[16.61]$         \\ \hline \hline
\end{tabular}
\end{center}
\textbf{Standard errors:} $[-]$ (Taylor, g-weights).
\end{table}
\noindent \textbf{Remarks}:\\
The classical estimates $\hat{Y}_{c,G,psmall}$ for $G_4$ were: $27.78(2.00)$ for basal area and $274.79(24.12)$ for stem density. For $G_5$  the corresponding results were $34.33(1.35)$ and $347.89(17.49)$.\\
As expected on mathematical grounds all extended models yielded empirical means of the residuals over the entire domain and over one or many small areas which were equal to zero ($<10^{-12})$.

The standard error for $\hat{Y}_{c,G,small}$ are given within $(-)$ when considering the internal model as an external one, i.e. by using the formulae (see \cite{mandallaz} p. 87):
\begin{equation*}
\hat{Y}_{c,G,psmall}=\frac{\sum_{x\in{s_{1,G}}}M(x)\hat{Y}_{c}(x)}{\sum_{x\in{s_{1,G}}}M(x)}
+\frac{\sum_{x\in{s_{2,G}}}M(x)\hat{R}_{c}(x)}{\sum_{x\in{s_{2,G}}}M(x)}
\end{equation*}
\begin{eqnarray*}
\widehat{\VAR}(\widehat{Y}_{c,G,psmall})&=&\left(1-\frac{n_{2,G}}{n_{1,G}}\right)
\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}
\sum_{x\in{s_{2,G}}}\left(\frac{M(x)}{\bar{M}_{2,G}}\right)^{2}(\hat{R}_c(x)-\hat{\bar{R}}_{2,G})^2\\
&+&
\frac{1}{n_{1,G}}\frac{1}{(n_{2,G}-1)}\sum_{x\in{s_{2,G}}}\left(\frac{M(x)}{\bar{M}_{2,G}}
\right)^{2}(Y_c(x)-\hat{Y}_{2,G})^2
\end{eqnarray*}
The standard errors given in $[-]$ refer to the various Taylor expansions given in [\ref{estvarpseudosynth1cluster}], [\ref{estvarpsmallcluster}], [\ref{estvarpsynth1clusterextended}] or their equivalent g-weights versions.\\

The extended model for the
$\hat{\tilde{Y}}_{c,G_k,psynt}$ contain only the indicator variable of the corresponding small area $G_k$. For this reason the corresponding estimates for the entire domain $F$ are given for $G_1$ $G_2$ and $G_3$ separately (in this order).\\




\end{document}
