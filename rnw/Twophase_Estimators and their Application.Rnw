% !Rnw root = JStatSoft_forestinventory_master.Rnw

\section{Two-phase Estimators and their Application}
\label{sec:twophase_and_appl}


% ---------------------------------------------------------------------------- %
\subsection{Global Estimators}
\label{sec:twophase_globest}


% NOTES:
% - The package also allows for calculating estimates solely based on the terrestrial sample $s_2$ (so called \textit{onephase estimation})
%   by the \code{onephase} function in \pkg{forestinventory}. This function can also be used to access the gain in efficiency of the mutliphase estimation procedures. 
%
% - we only use the non-cluster sampling here to illustrate:
% a) reg-coef calculation, point estimates and two variances, important components
% b) we introduce the boundary adjustments: explain why, give mathematical formula, mention that this is optional and can be left out, in which case the simple means of Z are used
% c) exhaustive and non-exhaustive


% Here, we give the \textbf{mathematical background} of:
% \begin{itemize}
%   \item the classical two-phase estimator, including:
%     \item the external and g-weight variance. Explain the differences and pros for using the g-weight version
%     \item the auxiliary components (Z) and covvar(Z) as well as their use in the point and variance estimator
%   \item the boundary weight adjustment (mathematically: weighted means of Z) --> our extension to the already published stuff
%   \item ...
% \end{itemize}
% 
% 

% 
% Here, we give the \textbf{application} examples:
% \begin{itemize}
%   \item example for non-exhaustive case: use grison dataset
%     \item with boundary adjustments, just mention that this is optional and can be left out, in which case the simple means of Z are used
%   \item example for exhaustive case: use cluster set
% \end{itemize}

\subsubsection{Mathematical Background}

The vector of regression coefficients of the OLS regression model is found by using the following solution to the sample-based normal equation:

\begin{equation}\label{normequ_simple}
  \hat{\pmb{\beta}}_{s_2}=\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x) \Big)^{-1} \Big(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)\Big)
\end{equation}

The individual predictions can then be calculated as $\hat{Y}(x)=\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}$ and the empirical model residuals, which are only available at all sample locations $x \in s_2$, are calculated as $\hat{R}(x)=Y(x)-\hat{Y}(x)$. In the \proglang{R} package \pkg{forestinventory}, only \textit{internal models} can be used to calculate estimations. This means that the vector of regression coefficients are always derived from the current inventory data that are passed to the \code{twophase()} and \code{threephase()} function.  While virtually all inventorists fit their models using the current inventory data, \citet{massey2015a} also consider scenarios using external models where the sample used to train the model is assumed to be taken from an independent source. However, such functions have yet to be implemented in the package at this time.\par

The package provides the calculation of \textbf{point estimates} under exhaustive ($ex$) and non-exhaustive ($nex$) use of the auxiliary information, which means to respectively apply $\hat{\pmb{\beta}}_{s_2}$ to $\bar{\pmb{Z}}$, i.e. the \textit{exact} spatial mean of $\pmb{Z}(x)$, or to $\hat{\bar{\pmb{Z}}}$, i.e. an \textit{estimate} of the spatial mean of $\pmb{Z}(x)$:

\begin{subequations}\label{eq:pointest_2p_reg}
\begin{align}
  \hat{Y}_{reg2p,ex} & =\bar{\pmb{Z}}^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_ex}\\
  \hat{Y}_{reg2p,nex} & =\hat{\bar{\pmb{Z}}}^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_nex}
\end{align}
\end{subequations}

Note that for \textit{internal} linear models the mean of the residuals $\frac{1}{n_2}\sum_{x\in{s_2}}\hat{R}(x)$ is zero by construction which is why it does not appear in the point estimate.

The package \pkg{forestinventory} implements two kinds of variances for each of these point estimates: the \textit{g-weight} formulation, also referred to as \textit{design-based variance}, (see \ref{eq:gw_var_2p_reg_ex} and \ref{eq:gw_var_2p_reg_nex}) given in \citet{mandallaz2016}, and the \textit{external variance} formulation (see \ref{eq:varexternal_2p_reg_ex} and \ref{eq:varexternal_2p_reg_nex}) that assumes a true external regression model and thus neglects the uncertainty in the regression coefficients.

The \textit{g-weight} formulation is
\begin{subequations}\label{eq:gw_var_2p_reg}
\begin{align}
  \hat{\var}(\hat{Y}_{reg2p,ex}) & :=\bar{\pmb{Z}}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}} \label{eq:gw_var_2p_reg_ex}\\
  \hat{\var}(\hat{Y}_{reg2p,nex}) & :=
  \hat{\bar{\pmb{Z}}}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}
  + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1}}\hat{\pmb{\beta}}_{s_2} \label{eq:gw_var_2p_reg_nex}
\end{align}
\end{subequations}

where the g-weight variance-covariance matrix of $\beta_{s_2}$ is calculated as

\begin{equation}\label{eq:estvarmatrix}
  \hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}:=\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x) \Big)^{-1}
  \Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}^2(x)\pmb{Z}(x)\pmb{Z}^t(x)\Big)
  \Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x) \Big)^{-1}
\end{equation}

and the uncertainty caused by using the $s_1$ sample to estimate $\bar{\pmb{Z}}$ by $\hat{\bar{\pmb{Z}}}$ is accounted for by the variance-covariance matrix of the auxiliary vector $\hat{\bar{\pmb{Z}}}$
\begin{equation}\label{estvarcovaux}
\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}}=
\frac{1}{n_{1}(n_{1}-1)}\sum_{x\in{s_{1}}}
(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}})^t
\end{equation}

The \textit{external variance} formulation for linear regression models is

\begin{subequations}\label{eq:varexternal_2p_reg}
\begin{align}
  \hat{\var}_{ext}(\hat{Y}_{reg2p,ex}) & = \frac{1}{n_2}\hat{\var}(R(x))=\frac{1}{n_2}\frac{1}{n_{2}-1}\sum_{x \in {s_2}}\Big(\hat{R}(x)-\bar{\hat{R}}\Big)^2 \label{eq:varexternal_2p_reg_ex} \\
  \hat{\var}_{ext}(\hat{Y}_{reg2p,nex}) & = \frac{1}{n_1}\hat{\var}(\hat{Y}(x)) + \frac{1}{n_2}\hat{\var}(R(x)) \nonumber \\ & =
 \frac{1}{n_1}\frac{1}{n_1-1}\sum_{x\in{s_1}}(\hat{Y}(x)-\bar{\hat{Y}}_{s_1})^2 + \frac{1}{n_2}\frac{1}{n_2-1}\sum_{x\in{s_2}}(\hat{R}(x)-\bar{\hat{R}})^2 \label{eq:varexternal_2p_reg_nex}
\end{align}
\end{subequations}

Note that when applied to internal linear regression models, the external variance is asymptotically unbiased and usually slightly smaller than the g-weight variance, where the uncertainty of the regression coefficients is accounted for by the variance-covariance matrix (\ref{eq:estvarmatrix}).  The external variances are provided in the package \pkg{forestinventory} in case the user wants to compare linear models to another model type where no g-weight formation is possible, as is the case with non-parametric models like kNN.

\subsubsection{Calculation of Explanatory Variables}

We will now draw our attention to the calculation of the explanatory variables from the auxiliary data for both the non-exhaustive and exhaustive case. In both cases, a set $\pmb{Z}$ of $p$ explanatory variables $x_1,...,x_p$ is calculated within a defined spatial extent (so called \textit{support}) around each sample location $x$. Figure \ref{fig:exh_nexh_and_boundweights_b} shows how the derivation of $\pmb{Z}(x)$ in the \textbf{non-exhaustive} case often looks like in practice, i.e. a regular terrestrial grid $s_2$ is given by a terrestrial inventory, densified to a larger sample $s_1$, and then supports are used to calculate the set of explanatory variables at the sample locations. This is in perfect aggreement with the just presented theory of the non-exhaustive estimator, except for using regular grids rather than randomly placed sample points. In this case, the \pkg{forestinventory} package calculates the emprirical mean of the $\pmb{Z}(x)$ values given in the input data frame, i.e. $\hat{\bar{\pmb{Z}}}=\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$. The \textbf{exhaustive} case however leaves some space for discussion concerning the calculation of $\pmb{Z}(x)$: This is because in the infinite population approach, the calculation $\pmb{Z}(x)$ again only refers to the sample point $x$ and has no spatial association. Likewise the mean of the local density (i.e. the total), the only way to determine the exact mean of $\pmb{Z}(x)$ would be to calculate the integral $\bar{\pmb{Z}}=\frac{1}{\lambda(F)}\int_{x \in F} \pmb{Z}(x)$, which in practice means to calculate $\pmb{Z}(x)$ at an infinite number of sample points $x$ ($n_1=\infty$). Since this is obviously impossible, one can only try to get a reasonable approximation of $\bar{\pmb{Z}}$.\par
One possibility to approximate the exact mean $\bar{\pmb{Z}}$ is shown in Figure \ref{fig:exh_nexh_and_boundweights_a}, which illustrates the implemented solution to the exhaustive use of the auxiliary information described in \citet{mandallaz2013b}. In order for all of the wall-to-wall auxiliary information to be exploited, the spatial arrangement of the supports were tesselated to form a perfect partition over the inventory area as is the case with the dark green polygons (squares tiles in this case). Is has to be noted that this would perfectly enable for calculating the exact mean $\bar{\pmb{Z}}$ in the \textit{finite} population approach, i.e. deriving $\pmb{Z}(x)$ for the entire finite population of supports and consequenlty calculating the exact mean $\bar{\pmb{Z}}$. While this is not equal to the exact mean $\bar{\pmb{Z}}$ in the infinite population approach, $n_1$ is still expected to be reasonably large to assume a good approximation of $\bar{\pmb{Z}}$ as long as the size of the supports are not unreasonably large. Whereas this implementation provides a convenient flexibility to switch between extimators of both the finite and infinite approach using the \textit{same} data set, it can also impose practical drawbacks. One is that a perfect tesselation by the supports strongly depends on the distance between the sample locations of $s_1$ in combination with the spatial extent of the support. Since in practice the support size will rather be chosen in order to achieve a best possible explanatory power of the regression model, thus minimizing the residual variation, a perfect tesselation might not always be feasible. In the infinite population frame however, the supports are also allowed to overlap if this seems necessary to acquire a sufficiently large sample $n_1$ of $\pmb{Z}(x)$ to get a reasonable approximation of $\bar{\pmb{Z}}$. This is again because $\pmb{Z}(x)$ only refers to the sample point and has no spatial association. With this respect, the infinite population approach can thus provide more flexibility than the finite approach. One could even consider the use of different support sizes per explanatory variable as long as they are consistently used at all sample locations.\par
Note that there are few cases where the exact mean can also be precisely calculated in the infinite population framework. The first case is when the explanatory variables are provided by polygon layers (e.g. map of development stages). In this case, one can calculate the exact mean as the area-weighted average of each categorical variable. The second case is when the exact mean can be calculated in one step, e.g. taking the mean of all height pixels of a raster canopy height model will almost perfectly equal the mean calculated by the use of an infinite number of supports \citep{mandallaz2013b}.


\begin{figure}[htb]
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
    %\resizebox{1.12\hsize}{!}{\includegraphics{fig/exh_and_boundweights(2).PNG}}%
    \resizebox{0.85\hsize}{!}{\includegraphics{fig/boundaryweight_graphic(left).eps}}%
		\caption{} \label{fig:exh_nexh_and_boundweights_a}
		\end{subfigure}
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
   %\resizebox{0.77\hsize}{!}{\includegraphics{fig/nexh_and_boundweights.PNG}}%
   \resizebox{0.85\hsize}{!}{\includegraphics{fig/boundaryweight_graphic(right).eps}}%
		\caption{} \label{fig:exh_nexh_and_boundweights_b}
	\end{subfigure}
\caption{Concept of exhaustive, see \textbf{(a)}, and non-exhaustive, see \textbf{(b)}, calculation of explanatory variables including boundary adjustment at the support level. Auxiliary data are in both cases available over the entire inventory area marked by the large rectangle. A vector of explanatory variables $\pmb{Z}(x)$ is calculated within the supports (small squares)  at each sample location $x$ (points) that falls into the forest area (green underlying polygon). w: weights used for calculating the weighted mean of each explanatory variable. Dotted circles indicate terrestrial sample plots.}
\label{fig:exh_nexh_bw}
\end{figure}



%% NOTES:
% We will now quickly draw our attention to the calculation of the explanatory variables from the auxiliary data for both the non-exhaustive and exhaustive case. In both cases, a set $\pmb{Z}$ of $p$ explanatory variables $x_1,...,x_p$ is calculated within a defined spatial extent (so called \textit{support}) around each sample location $x$. Figure 4(b) shows how the derivation of Z(x) in the non-exhaustive case often looks like in practice, i.e. a regular terrestrial grid $s_2$ is given by a terrestrial inventory, densified to a larger sample $s_1$, and then supports are used to calculate the set of explanatory variables at the sample location x. This is in perfect aggreement with the just presented theory of the non-exhaustive estimators, except for using regular grids rather than randomly placed sample points, i.e. the forestinventory package will calculate the emprirical mean of the Z(x) values given in the input dataframe, i.e. $\hat{\bar{\pmb{Z}}}=\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$.
% The exhaustive case however leaves some space of discussion concerning the calculation of $Z(x)$:
%
%
% - first, mention what Z(x) means in practice, and how we calculate a variable stored in Z(x), i.e. use supports (maybe mention the choice of support size,
%   and that it can have an influence on model accuracy). 
% - Figure 4(b) shows how the derivation of the Z(x) in the non-exhaustive case often looks like in practice, i.e. a regular grid is given by a 
%   terrestrial inventory, densified to an s1-sample, and then supports are used to calculate the set of explanatory variables at sample location x.
%   This is in perfect aggreement with the just presented theory of the non-exhaustive estimators (except for using regular grids rather than randomly
%   placed sample points) and the forestinventory package will just use the emprirical mean of the Z(x) values given in the input dataframe, i.e. 
%   $\hat{\bar{\pmb{Z}}}=\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$.
% - The exhaustive case however leaves some space for the implementation:
%   This is because likewise the local density $Y(x)$ gathered in the second phase $s_2$, also the calculation of Z(x) only referrs to 
%   the sample point x, and has no spatial association (see section 2.1). Likewise the mean of the local density, the only way to determine the exact mean 
%   of $\bar{Z}$ would be to calculate the integral, i.e. in practice calculate Z(x) at an infinite number ($n_1=\infty$) of sample points $x$. In practice,
%   this is obviously impossible.
% - Figure 4(a) shows the implemented solution of an "exhaustive use" of the auxiliary information as described in Mandallaz, Breschan, Hill (2013). Is has to be noted
%   that this would perfectly enable for calculating the exact mean $\bar{Z}$ in the \textit{finite} population approach (i.e. we calculate Z(x) for the entire 
%   population of supports, and can thus calculate the exact mean $\bar{Z}$). However, this is not equal to the exact mean $\bar{Z}$ in the infinite population
%   approach for the mentioned reasons.
% - What we can do in practice is to get a reasonable approximation of $\bar{Z}$. 
%
%
% - mention that in the strict inf.pop.appr. frame, the supports are also allowed to overlap if this seems necessary to acquire a sufficiently large sample
%   n1 of Z(x) to get a reasonable approximation of $\bar{Z}$. This is possible because likewise the local density $Y(x)$ gathered in the second phase $s_2$,
%   also the calculation of Z(x) only referrs to the sample point, and has no spatial association.
%
% - mention that the support size thereby does not have to match the plotsize of the terrestrial inventory. In the strict inf.pop.appr. frame, the supports are also allowed to overlap if this seems necessary to acquire a sufficiently large sample n1 of Z(x) to get a reasonable approximation of $\bar{Z}$. This is possible because likewise the local density $Y(x)$ gathered in the second phase $s_2$, also the calculation of Z(x) only referrs to the sample point, and has no spatial association. In fact, the support size will in practice be chosen in order to achieve a best possible explanatory power of the regression model, thus minimzing the residual variation. mention that a change of support size between sample points for a given expl. variable is not allowed (because.... Daniel ?)
%
%


%% ------------ in progress -------------------- %%



An extension to the so-far published estimators by Mandallaz is the consideration of a \textit{\textbf{boundary adjustment}} according to forest/non-forest decision on the support level, which can be optionally applied in the \code{twophase()} and \code{threephase()} function of the package. In forest inventories, the sample is often restricted to those sample locations located within the forest area. In case a consistent forest definition can be applied to both the $s_2$ and $s_1$ sample (e.g. by a ploygon forest mask layer), it might be desired to restrict the calculation of the explanatory variables to the forest area within the given support (Figure \ref{fig:exh_nexh_bw}). This method was suggested in \citet{mandallaz2013b} and led to an improvement of the regression model accuracy. To insure an unbiased calculation of either $\hat{\bar{\pmb{Z}}}(x)$ or $\bar{\pmb{Z}}(x)$, the respective means have then to be calculated as the \textit{weighted} mean according to \ref{eq:wmeanZ}. The weight $w(x)$ is thereby equal to the percentage of forested area within the support of sample location $x$.

\begin{equation}\label{eq:wmeanZ}
  \hat{\bar{\pmb{Z}}}=\frac{\sum_{x\in{s_1}}w(x)\pmb{Z}(x)}{\sum_{x\in{s_1}}w(x)}
\end{equation}


% --------------------------- %
\subsubsection{Application}

To demonstrate the use of the global two-phase estimators, we will use the \code{grisons} data set that comes with installing the package from the CRAN repository. The dataset contains data from a two-phase forest inventory under simple (i.e. non-cluster) sampling design conducted in 2007 that was used in \citet{mandallaz2013b} as a case study. The $s_1$ same is comprised of 306 sample locations arranged on a systematic grid containing auxiliary information in the form of LiDAR canopy height metrics (\code{mean}, \code{stddev}, \code{max}, \code{q75}). Auxiliary information for all 306 plots is provided in the form of LiDAR canopy height metrics (\code{mean}, \code{stddev}, \code{max}, \code{q75}). For a systematic subsample of 67 ($s_2$ sample), terrestrial information of the timber volume per hectare (\code{tvol}) on the sample plot level is provided from a terrestrial survey. We can load \pkg{forestinventory} and examine the \code{grisons} data set in the \proglang{R} environment as follows:

<<echo=FALSE>>=
options(width=10000) 
@

\begin{small}
<<eval=FALSE>>=
library(forestinventory)
data("grisons", package = "forestinventory")
head(grisons)
@
\end{small}

\begin{small}
<<eval=TRUE, echo=FALSE>>=
library(forestinventory)
set.seed(666);print(head(grisons[sample(1:306,10),c(1,3,4,5,6,7,8,9)], 10),row.names=c(1:10))
@
\end{small}

Estimates can be made using the \code{onephase()}, \code{twophase()} or \code{threephase()} functions. The data frame inputted to these functions must have the structure where each row corresponds to a unique sample location and the columns specify the attributes associated to that respective sample location. Attributes that are missing, e.g. because they are associated with sample locations that were not selected in the subsample for the subsequent phase, should be designated as \code{NA} and the phase membership is encoded as numeric.

For global two-phase estimation, we have to specify

\begin{itemize}
  \itemsep0em
  \item the regression model (\code{formula}) as specified in the \code{lm}()-function \citep{R}
  \item the inputted \code{data.frame} containing the inventory information (\code{data})
  \item the \code{list}-object \code{phase_id} containing: the \code{phase.col} argument identifying the name of the column specifying membership to $s_1$ or $s_2$, and the \code{terrgrid.id} argument specifying which numeric value indicates $s_2$ membership in that column
  \item the name of the column containing the weights $w(x)$ of the boundary adjustments (optional)
\end{itemize}

The \textbf{non-exhaustive estimator with boundary weight adjustment} can thus be applied as follows:

\begin{small}
<<>>=
reg2p_nex <- twophase(formula=tvol ~ mean + stddev + max + q75,
                   data=grisons,
                   phase_id=list(phase.col = "phase_id_2p", terrgrid.id = 2),
                   boundary_weights = "boundary_weights")
@
\end{small}

The \code{twophase()} function creates an \code{S3} object of class \code{"twophase"} with subclass \code{"global"}. A readable \textbf{summary of the estimation results} can be obtained by passing this object to the \code{summary()} function, which automatically interprets what type of estimator was used and returns pertinent information such as the regression model formula, the point estimate (\code{estimate}), the g-weight and external variance (\code{g_variance} and \code{ext_variance}) as well as the sample sizes and the model R$^2$:

\begin{small}
<<>>=
summary(reg2p_nex)
@
\end{small}

For practical use, one should normally always prefer the g-weight variance over the external variance. This is because when we use internal models, the regression coefficients actually dependent on our sample. In contrast to the external variance, the g-weight variance accounts for this sampling variability which results in more reliable point and variance estimates and also enjoys better statistical calibration properties (g-weights). The external and g-weight variances are asymptotically equivalent but the external variance is really only included here in case the user wants to compare to another estimator where no g-weight variance exists.

The \textbf{exhaustive estimator} can be applied by additionally passing a vector containing the \textit{exact} means of the explanatory variables, i.e. $\bar{\pmb{Z}}$, to the optional argument \code{exhaustive}. This vector must be calculated beforehand in such a way that any desired boundary adjustment has already been applied. Note that the vector input to \code{exhaustive} must be in the same order that the \code{lm()}-function processes a \code{formula} object including the intercept term whose exact mean will always be 1. This step is particularly recommended if categorical variables are present because the \code{lm()}-function, which is internally used to set up the design-matrix, automatically creates dummy variables with one of the categories used as a reference. The correct order for our \code{grisons} example can easily be extracted using:

\begin{small}
<<eval=FALSE>>=
colnames(lm(formula = tvol ~ mean + stddev + max + q75, data = grisons, x = TRUE)$x)
@
\end{small}

The \textbf{exhaustive estimator} can be applied after defining the vector of exact means $\bar{\pmb{Z}}$ taken from \citet{mandallaz2013b}, denoted as \code{true.means.Z}:

\begin{small}
<<>>=
true.means.Z <- c(1, 11.39, 8.84, 32.68, 18.03)

reg2p_ex <- twophase(formula = tvol ~ mean + stddev + max + q75,
                    data = grisons,
                    phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                    exhaustive = true.means.Z)
@
\end{small}

An alternative way to look at the estimation results without using the \code{summary()} is to query \code{reg2p_ex} directly:
\begin{small}
<<>>=
reg2p_ex$estimation
@
\end{small}
Note that both variances of the exhaustive estimation are smaller than those of the non-exhaustive estimation. This is essentially because we eliminated one component of uncertainty by substituting the estimated means of the explanatory variables $\hat{\bar{\pmb{Z}}}$ by their exact means $\bar{\pmb{Z}}$.


%--------------------------------------------------------------------------------------------------%
% ################################################################################################ %
%--------------------------------------------------------------------------------------------------%

\subsection{Small Area Estimators}
\label{sec:twophase_sae}

\subsubsection{Mathematical Background}

The package \pkg{forestinventory} provides three broad types of small area estimators each of which has an exhaustive and non-exhaustive form. We will use a different nomenclature for the non-exhaustive case in small area estimation since much of the existing literature shows preference for the label \textit{pseudo} to indicate that the mean of the explanatory variables within the small area was based on a finite sample. The main idea for all these small area estimators is to calculate the regression coefficient vector $\hat{\pmb{\beta}}_{s_2}$ and its variance-covariance matrix $\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}$ on the entire $s_2$ sample according to equations \ref{normequ_simple} and \ref{eq:estvarmatrix}, and subsequently use that to make predictions for sample locations restricted to small area $G$.\par

%----------------------------------------------- %
% \textbf{Small and Pseudo Small Area Estimator}\par

We first introduce the \textbf{small area estimator} (\textit{small}), which uses exhaustively computed explanatory variables, and its non-exhaustive version, the \textbf{\textit{pseudo}} \textbf{small area estimator} (\textit{psmall}). 

\begin{subequations}\label{eq:pest_2p_small_psmall}
\begin{align}
  \hat{Y}_{G,small,2p} & =\bar{\pmb{Z}}_G^t\hat{\pmb{\beta}}_{s_2} + \frac{1}{n_{2,G}}\hat{R}(x)  \label{eq:pointest_2p_small} \\
  \hat{Y}_{G,psmall,2p} & =\hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\beta}}_{s_2} + \frac{1}{n_{2,G}}\hat{R}(x) \label{eq:pepsmall}
\end{align}
\end{subequations}

In the equations for the point estimates (\ref{eq:pointest_2p_small} and \ref{eq:pepsmall}), we see that the globally derived regression coefficients are applied to the exhaustively or non-exhaustively calculated means of the explanatory variables ($\bar{\pmb{Z}}_G$, $\hat{\bar{\pmb{Z}}}_G$) which are now only based on the first-phase sample $s_{1,G}$ located within small area $G$. A potential bias of the regression model predictions in the small area $G$, due to fitting the regression model with data outside of $G$, is then corrected by adding the mean of the model residuals in $G$ to the first term. This is called the \textit{bias} or \textit{residual correction} term.

The package provides the \textit{g-weight} variance for \textit{small} and \textit{psmall} respectively (Equations \ref{eq:var_2p_reg_small}, \ref{eq:var_2p_reg_psmall}) as well as the  \textit{external variance} (Equation \ref{eq:varext_2p_reg_small}, \ref{eq:varext_2p_reg_psmall}). Again note that all components are restricted to those available at the sample locations in the small area ($s_{1,G}$ and $s_{2,G}$), with exception of the regression coefficient components $\hat{\pmb{\beta}}_{s_2}$ and $\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}$.

\begin{subequations}\label{eq:var_2p_small_psmall}
\begin{align}
  \hat{\var}(\hat{Y}_{G,small,2p}) & := \bar{\pmb{Z}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G
    + \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2  \label{eq:var_2p_reg_small} \\
  \hat{\var}(\hat{Y}_{G,psmall,2p}) & := \hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_G
  + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_G}\hat{\pmb{\beta}}_{s_2}
  + \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2  \label{eq:var_2p_reg_psmall}
\end{align}
\end{subequations}

\begin{subequations}\label{eq:varext_2p_small_psmall}
\begin{align}
  \hat{\var}_{ext}(\hat{Y}_{G,small,2p}) & := \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2  \label{eq:varext_2p_reg_small} \\
  \hat{\var}_{ext}(\hat{Y}_{G,psmall,2p}) & := \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(Y(x)-\bar{Y}_{2,G}\Big)^2 \nonumber \\
 &+ \Big(1-\frac{n_{2,G}}{n_{1,G}}\Big)\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2 \label{eq:varext_2p_reg_psmall}
\end{align}
\end{subequations}

If boundary adjustment is applied, the simple mean of the explanatory variable over the small area $\hat{\bar{\pmb{Z}}}_G=\frac{1}{n_{1,G}}\sum_{x \in s_{1,G}}\pmb{Z}(x)$ is replaced by its weighted version $\hat{\bar{\pmb{Z}}}_G=\frac{\sum_{x\in{s_{1,G}}}w(x)\pmb{Z}(x)}{\sum_{x\in{s_{1,G}}}w(x)}$, and likewise for exhaustively used auxiliary information.



%------------------------------------------------------------- %
% \textbf{Synthetic and Pseudo Synthetic Estimator}\par

The \textbf{synthetic estimator} (\textit{synth}) and \textbf{pseudo synthetic estimator} \textit{psynth} are commonly applied when no terrestrial sample is available within the small area $G$ (i.e. $n_{2,G}=0$). In this case, the point estimate (equation \ref{eq:pointest_2p_reg_synth} and \ref{eq:pointest_2p_reg_psynth}) are based \textit{only} on the predictions generated by applying the globally derived regression model to the auxiliary vector $\bar{\pmb{Z}}_G$ or $\hat{\bar{\pmb{Z}}}_G$. However, the bias correction using the observed residuals is not applied as was the case in the small and pseudo small area estimator (Equations \ref{eq:pointest_2p_small} and \ref{eq:pepsmall}). Thus, the (pseudo) synthetic estimator is unbiased \textbf{only} if the \textit{zero mean property} of the residuals $\int_F R(x)dx=0$ also holds for the residuals in the small area $G$, i.e. $\int_G R(x)dx=0$. However, this assumption is unlikely to be fullfilled in practice resulting in a potentially unobservable design-based bias equal to $-\frac{1}{\lambda(G)}\int_G R(x)$. Also note that the residual variation can no longer be considered in the g-weight variance (equation \ref{eq:var_2p_reg_synth} and \ref{eq:var_2p_reg_psynth}). Therefore, the synthetic estimators will usually have a smaller variance than estimators incorporating the regression model uncertainties, but at the cost of a potential bias. Due to the absence of available residuals in $G$, there is also no external variance form for the synthetic and pseudo synthetic estimator.

\begin{subequations}\label{eq:pest_2p_synth_psynth}
\begin{align}
  \hat{Y}_{G,synth,2p} & =\bar{\pmb{Z}}_G^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_synth} \\
  \hat{Y}_{G,psynth,2p} & =\hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_psynth} \\
  \hat{\var}(\hat{Y}_{G,synth,2p}) & = \bar{\pmb{Z}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G \label{eq:var_2p_reg_synth} \\
  \hat{\var}(\hat{Y}_{G,psynth,2p}) & = \hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_G
  + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_G}\hat{\pmb{\beta}}_{s_2}  \label{eq:var_2p_reg_psynth}
\end{align}
\end{subequations}

where the variance-covariance matrix of the auxiliary vector $\hat{\bar{\pmb{Z}}}_G$ is estimated by
\begin{equation}\label{estvarcovaux_G}
\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{G}}=
\frac{1}{n_{G}(n_{G}-1)}\sum_{x\in{s_{1,G}}}
(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{G})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{G})^t
\end{equation}

%------------------------------------------------------------- %
% \textbf{Extended Synthetic and Extended Pseudo Synthetic Estimator}\par

The synthetic estimators, \textit{synth} and \textit{psynth}, have attractively compact formulas but come with the downside of potential bias in their point estimates as well as unreliably small variances. The \textit{small} and \textit{psmall} estimators overcome this issue by using a bias correction term, i.e. $\frac{1}{n_{2,G}}\sum_{x \in s_{2,G}}\hat{R}(x)$. The motivation behind the \textbf{\textit{extended}} \textbf{synthetic} and \textbf{\textit{extended}} \textbf{pseudo synthetic estimator} (\textit{extsynth} and \textit{extpsynth}) is to use the same mathematically elegant formulas of the (pseudo) synthetic estimators while at the same time ensuring that the prediction model residuals over the entire area $F$ and the small area $G$ are by definition both zero at the same time, i.e. $\int_F R(x)dx=\int_G R(x)dx=0$. This is accomplished by extending the vector of auxiliary information $\pmb{Z}(x)$ by a binary categorical indicator variable $I_G(x)$ which takes the value 1 if the sample location $x$ lies inside the target small area $G$ and is otherwise set to 0. Recalling that linear models fit using OLS have zero mean residuals property by construction, this leads to unbiased point estimates and is also true if categorical variables are included in the model. The new \textit{extended} auxiliary vector thus becomes $\pmb{\mathbb{Z}}^t(x)=(\pmb{Z}^t(x),I_G(x))$ and can be used to replace its non-extended counterpart $\pmb{Z}^t(x)$ whereever it is used in Equations \ref{eq:pest_2p_synth_psynth} and \ref{estvarcovaux_G}. Note that the package functions internally extent the data set by the indicator variable if the \textit{extsynth} or \textit{extpsynth} estimator is called.

Not every equation needs to be re-written here, but to give an example of the notational change, the regression coefficient under extended model approach becomes

\begin{equation}\label{ext_normequ_simple}
  \hat{\pmb{\theta}}_{s_2}=\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{\mathbb{Z}}(x)\pmb{\mathbb{Z}}^t(x) \Big)^{-1} \Big(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{\mathbb{Z}}(x)\Big)
\end{equation}

The point estimates and their g-weight variances can then be re-written as

\begin{subequations}\label{eq:pest_2p_extsynth_extpsynth}
\begin{align}
\hat{Y}_{G,extsynth,2p} & = \bar{\pmb{\mathbb{Z}}}^t_{G}\hat{\pmb{\theta}}_{s_2} \label{eq:pointest_2p_extsynth} \\
\hat{Y}_{G,extpsynth,2p} & =\hat{\bar{\pmb{\mathbb{Z}}}}_{G}^t\hat{\pmb{\theta}}_{s_2} \label{eq:pointest_2p_extsynth} \\
\hat{\var}(\hat{Y}_{G,extsynth,2p}) & = \bar{\pmb{\mathbb{Z}}}^t_{G}\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}\bar{\pmb{\mathbb{Z}}}_{G} \label{eq:var_2p_extsynth} \\
\hat{\var}(\hat{Y}_{G,extpsynth,2p})& =
\hat{\bar{\pmb{\mathbb{Z}}}}_{G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}
\hat{\bar{\pmb{\mathbb{Z}}}}_{G}
+ \hat{\pmb{\theta}}^t_{s_2}\hat{\Sigma}_{\hat{\bar{\pmb{\mathbb{Z}}}}_{1,G}}\hat{\pmb{\theta}}_{s_2} \label{eq:var_2p_extpsynth}
\end{align}
\end{subequations}

While the formulas look similar to the synthetic estimators, note that a decomposition of $\hat{\pmb{\theta}}_{s_2}$ reveals that the residual correction term is now included in the regression coefficient $\hat{\pmb{\theta}}_{s_2}$ \citep{mandallaz2016} and thus the estimates are asymptotically design-unbiased.

The package also provides the external variance for both the \textit{extended} synthetic and \textit{extended} pseudo synthetic estimator. Note that both neither the extended model approach nor external variance estimates are possible in the absence of terrestrial samples und thus model residuals in $G$, which is precisely when one must rely on the (pseudo) synthetic estimates. The external variance forms of \textit{extsynth} and \textit{extpsynth} are

\begin{subequations}\label{eq:ext_varexternal_2p_extsynth}
\begin{align}
  \hat{\var}_{ext}(\hat{Y}_{G,extsynth,2p}) & = \frac{1}{n_{2,G}} \frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{\mathbb{R}}(x)-\bar{\hat{\mathbb{R}}})^2 \label{eq:ext_varexternal_2p_extsynth} \\
  \hat{\var}_{ext}(\hat{Y}_{G,extpsynth,2p}) & =
 \frac{1}{n_{1,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(Y(x)-\bar{Y}_{2,G})^2 \nonumber \\
 &+ (1-\frac{n_{2,G}}{n_{1,G}})\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{\mathbb{R}}(x)-\bar{\hat{\mathbb{R}}})^2 \label{eq:ext_varexternal_2p_extpsynth}
\end{align}
\end{subequations}

To summarize, the (pseudo) synthetic estimator can be applied whether terrestrial inventory sample is found in the small area or not, but has a deceptively small g-weight variance due to its potential bias.  When terrestrial sample is observed in the small area, we can produce (asymptotically) design-unbiased estimates and variances using either \textit{small} or \textit{psmall} which remove this bias directly with a mean residual term, or more elegantly with \textit{extsynth} or \textit{extpsynth} which simply uses the same synthetic formulas while including an indicator variable for the small area in the model formula to remove the bias by construction in OLS.


%------------------------------------------------------------- %
\subsubsection{Application}

Small area estimates in the \pkg{forestinventory} package can be applied by specifying the optional argument \code{small_area}. The input data set has to include an additional column of class factor that describes the small area membership of the sample location represented by that row. The argument \code{small_area} requires a \code{list}-object that comprises

\begin{itemize}
  \itemsep0em
  \item the name of the column specifiying the small area of each observation (\code{sa.col})
  \item a vector specifying the small area(s) for which estimations are desired (\code{areas})
  \item the argument \code{unbiased} that controls which of the three available estimators is applied
\end{itemize}

In order to apply the \textbf{pseudo small area estimator} (\textit{psmall}) with boundary adjustment, we set \code{unbiased=TRUE} as well as the optional argument \code{psmall=TRUE}:
\begin{small}
<<>>=
psmall_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                      phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                      boundary_weights = "boundary_weights",
                      small_area = list(sa.col = "smallarea", areas = c("A", "B"),
                                        unbiased = TRUE),
                      psmall = TRUE)
summary(psmall_2p)
@
\end{small}

The small area functions all return an \code{S3} object of class \code{"twophase"} with subclass \code{"smallarea"}. In addition to global estimation, the \code{estimation} object now comprises the estimates and variances for all small areas (column \code{area}). We can view the sample sizes by looking into the object itself
\begin{small}
<<>>=
psmall_2p$samplesizes
@
\end{small}

The \textbf{extended pseudo synthetic estimator} (\textit{extpsynth}) can be applied by setting \code{unbiased=TRUE} and leaving the optional argument \code{psmall} to its default value of \code{FALSE}:
\begin{small}
<<>>=
extpsynth_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                         phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                         boundary_weights = "boundary_weights",
                         small_area = list(sa.col = "smallarea", areas = c("A", "B"),
                                           unbiased = TRUE))
extpsynth_2p$estimation
@
\end{small}

The \pkg{forestinventory} package automatically includes the indicator variable for the small area behind the scenes so there is no need to implement it yourself. Notice that the $R^2$s (\code{r.squared}) under the \textit{extpsynth} estimator are vary between small areas, while they are identical under the \textit{psmall} estimator. This is because under the \textit{extpsynth} estimator, the regression model is recalculated for each small area estimation after adding the indicator variable for the respective small area in the globally derived design matrices. In case of the  \textit{psmall} estimator, the regression model stays the same for each small area estimation. Although the results of both estimators should always be close to each other, we recommend applying both estimators and compare the results afterwards in order to reveal unsuspected patterns in the data, particularly in the case of cluster sampling (see section \ref{sec:speccas_and_scen}).\par

Setting the argument \code{unbiased=FALSE} applies the \textbf{pseudo synthetic estimator} to the selected small areas. Note that in the \code{grisons} data set, all small areas possess much more than the suggested minimum number of terrestrial observations (a rule of thumb is that $n_{2,G} \geq 6$) required to produce reliable design-unbiased estimates. Hence, choosing to use \textit{psynth} is probably not desireable and is just applied here for demonstration purposes. The function will in this case ignore the terrestrial information in the data set.


\begin{small}
<<>>=
psynth_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                      phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                      boundary_weights = "boundary_weights",
                      small_area = list(sa.col = "smallarea", areas = c("A", "B"),
                                        unbiased = FALSE))
psynth_2p$estimation
@
\end{small}

We see here that the \textit{psynth} variances are almost only half the variances of the \textit{psmall} and \textit{extended psynth} estimators. However, \textit{psmall} and \textit{extended psynth} are design unbiased and their variances reflect the fact that they account for the uncertainty of the regression model predictions. The g-weight variance of \textit{psynth} completely neglects a potential bias and as a result risks severely overstating the estimation precision.\par

The \textbf{exhaustive versions} of the small area estimators (equations \ref{eq:pointest_2p_small},\ref{eq:var_2p_reg_small},\ref{eq:varext_2p_reg_small},\ref{eq:pointest_2p_reg_synth},\ref{eq:var_2p_reg_synth}) are specified via the optional argument \code{exhaustive}. Its application requires that we know the exact means of all explanatory variables within the small area(s) of interest. In contrast to the \textit{global} estimators, the exact means have now to be delivered in the form of a \code{data.frame}, where each row corresponds to a small area, and each column specifies the exact mean of the respective explanatory variable. Note that likewise the case of global estimation, the order of the explanatory variables in the data frame has to match the order in which they appear in the design matrix defined by the \code{lm()}-function in \proglang{R}. In order to tell \proglang{R} which row describes which small area, the row names have to match the respective names of the small areas specified in the \code{areas} argument.

For the \code{grisons} data set, the exact means of the explanatory variables for the small areas used in \citet{mandallaz2013b} are thus defined by
\begin{small}
<<eval=FALSE>>=
colnames(lm(formula = tvol ~ mean + stddev + max + q75, data = grisons, x = TRUE)$x)
@
\end{small}
\begin{small}
<<>>=
true.means.Z.G <- data.frame(Intercept = rep(1, 4),
                         mean = c(12.85, 12.21, 9.33, 10.45),
                         stddev = c(9.31, 9.47, 7.90, 8.36),
                         max = c(34.92, 35.36, 28.81, 30.22),
                         q75 = c(19.77, 19.16, 15.40, 16.91))
rownames(true.means.Z.G) <- c("A", "B", "C", "D")
@
\end{small}
\begin{small}
<<>>=
true.means.Z.G
@
\end{small}

For example, the \textbf{extended synthetic estimator} (\textit{extsynth}) can then be applied by
\begin{small}
<<>>=
extsynth_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                        phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                        small_area = list(sa.col ="smallarea", areas = c("A", "B"),
                                          unbiased = TRUE),
                        exhaustive = true.means.Z.G)
extsynth_2p$estimation
@
\end{small}

Just as in the global case, we see that the variance has again been significantly decreased by substituting the \textit{exact} auxiliary means and both first phase sample sizes are now infinity. Note that the function extracts the required exact means for small area \code{"A"} and \code{"B"} from the complete set of exact means defined in \code{true.means.Z.G}.

\newpage