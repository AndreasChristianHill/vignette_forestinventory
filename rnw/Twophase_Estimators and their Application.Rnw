% !Rnw root = JStatSoft_forestinventory_master.Rnw

\section{Two-phase Estimators and their Application}
\label{sec:twophase_and_appl}


% ---------------------------------------------------------------------------- %
\subsection{Global Estimators}
\label{sec:twophase_globest}


% NOTES:
% - The package also allows for calculating estimates solely based on the terrestrial sample $s_2$ (so called \textit{onephase estimation})
%   by the \code{onephase} function in \pkg{forestinventory}. This function can also be used to access the gain in efficiency of the mutliphase estimation procedures. 
%
% - we only use the non-cluster sampling here to illustrate:
% a) reg-coef calculation, point estimates and two variances, important components
% b) we introduce the boundary adjustments: explain why, give mathematical formula, mention that this is optional and can be left out, in which case the simple means of Z are used
% c) exhaustive and non-exhaustive


% Here, we give the \textbf{mathematical background} of:
% \begin{itemize}
%   \item the classical two-phase estimator, including:
%     \item the external and g-weight variance. Explain the differences and pros for using the g-weight version
%     \item the auxiliary components (Z) and covvar(Z) as well as their use in the point and variance estimator
%   \item the boundary weight adjustment (mathematically: weighted means of Z) --> our extension to the already published stuff
%   \item ...
% \end{itemize}
% 
% 

% 
% Here, we give the \textbf{application} examples:
% \begin{itemize}
%   \item example for non-exhaustive case: use grison dataset
%     \item with boundary adjustments, just mention that this is optional and can be left out, in which case the simple means of Z are used
%   \item example for exhaustive case: use cluster set
% \end{itemize}

\subsubsection{Mathematical Background}

The vector of regression coefficients of the OLS regression model is found by using the following solution to the sample-based normal equation:

\begin{equation}\label{normequ_simple}
  \hat{\pmb{\beta}}_{s_2}=\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x) \Big)^{-1} \Big(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{Z}(x)\Big)
\end{equation}

The individual predictions can then be calculated as $\hat{Y}(x)=\pmb{Z}^t(x)\hat{\pmb{\beta}}_{s_2}$ and the empirical model residuals, which are only available at all sample locations $x \in s_2$, are calculated as $\hat{R}(x)=Y(x)-\hat{Y}(x)$. In the \proglang{R} package \pkg{forestinventory}, only \textit{internal models} can be used to calculate estimations. This means that the vector of regression coefficients are always derived from the current inventory data that are passed to the \code{twophase()} and \code{threephase()} function.  While virtually all inventorists fit their models using the current inventory data, \citet{massey2015a} also consider scenarios using external models where the sample used to train the model is assumed to be taken from an independent source. However, such functions have yet to be implemented in the package at this time.\par

The package provides the calculation of \textbf{point estimates} under exhaustive ($ex$) and non-exhaustive ($nex$) use of the auxiliary information, which means to respectively apply $\hat{\pmb{\beta}}_{s_2}$ to $\bar{\pmb{Z}}$, i.e. the \textit{exact} spatial mean of $\pmb{Z}(x)$, or to $\hat{\bar{\pmb{Z}}}$, i.e. an \textit{estimate} of the spatial mean of $\pmb{Z}(x)$:

\begin{subequations}\label{eq:pointest_2p_reg}
\begin{align}
  \hat{Y}_{reg2p,ex} & =\bar{\pmb{Z}}^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_ex}\\
  \hat{Y}_{reg2p,nex} & =\hat{\bar{\pmb{Z}}}^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_nex}
\end{align}
\end{subequations}

Note that for \textit{internal} linear models the mean of the residuals $\frac{1}{n_2}\sum_{x\in{s_2}}\hat{R}(x)$ is zero by construction which is why it does not appear in the point estimate.

The package \pkg{forestinventory} implements two kinds of variances for each of these point estimates: the \textit{g-weight} formulation, also referred to as \textit{design-based variance}, (see \ref{eq:gw_var_2p_reg_ex} and \ref{eq:gw_var_2p_reg_nex}) given in \citet{mandallaz2016}, and the \textit{external variance} formulation (see \ref{eq:varexternal_2p_reg_ex} and \ref{eq:varexternal_2p_reg_nex}) that assumes a true external regression model and thus neglects the uncertainty in the regression coefficients.

The \textit{g-weight} formulation is
\begin{subequations}\label{eq:gw_var_2p_reg}
\begin{align}
  \hat{\var}(\hat{Y}_{reg2p,ex}) & :=\bar{\pmb{Z}}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}} \label{eq:gw_var_2p_reg_ex}\\
  \hat{\var}(\hat{Y}_{reg2p,nex}) & :=
  \hat{\bar{\pmb{Z}}}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}
  + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{1}}\hat{\pmb{\beta}}_{s_2} \label{eq:gw_var_2p_reg_nex}
\end{align}
\end{subequations}

where the design-based variance-covariance matrix of $\beta_{s_2}$ is calculated as

\begin{equation}\label{eq:estvarmatrix}
  \hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}:=\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x) \Big)^{-1}
  \Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}^2(x)\pmb{Z}(x)\pmb{Z}^t(x)\Big)
  \Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{Z}(x)\pmb{Z}^t(x) \Big)^{-1}
\end{equation}

and the uncertainty caused by using the $s_1$ sample to estimate $\bar{\pmb{Z}}$ by $\hat{\bar{\pmb{Z}}}$ is accounted for by the variance-covariance matrix of the auxiliary vector $\hat{\bar{\pmb{Z}}}$
\begin{equation}\label{estvarcovaux}
\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}}=
\frac{1}{n_{1}(n_{1}-1)}\sum_{x\in{s_{1}}}
(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}})^t
\end{equation}

The \textit{external variance} formulation for linear regression models is

\begin{subequations}\label{eq:varexternal_2p_reg}
\begin{align}
  \hat{\var}_{ext}(\hat{Y}_{reg2p,ex}) & = \frac{1}{n_2}\hat{\var}(R(x))=\frac{1}{n_2}\frac{1}{n_{2}-1}\sum_{x \in {s_2}}\Big(\hat{R}(x)-\bar{\hat{R}}\Big)^2 \label{eq:varexternal_2p_reg_ex} \\
  \hat{\var}_{ext}(\hat{Y}_{reg2p,nex}) & = \frac{1}{n_1}\hat{\var}(\hat{Y}(x)) + \frac{1}{n_2}\hat{\var}(R(x)) \nonumber \\ & =
 \frac{1}{n_1}\frac{1}{n_1-1}\sum_{x\in{s_1}}(\hat{Y}(x)-\bar{\hat{Y}}_{s_1})^2 + \frac{1}{n_2}\frac{1}{n_2-1}\sum_{x\in{s_2}}(\hat{R}(x)-\bar{\hat{R}})^2 \label{eq:varexternal_2p_reg_nex}
\end{align}
\end{subequations}

Note that when applied to internal linear regression models, the external variance is asymptotically unbiased and usually slightly smaller than the g-weight variance, where the uncertainty of the regression coefficients is accounted for by the variance-covariance matrix (\ref{eq:estvarmatrix}).  The external variances are provided in the package \pkg{forestinventory} in case the user wants to compare linear models to another model type where no g-weight formation is possible, as is the case with non-parametric models like kNN.

%% ------------ in progress -------------------- %%

% \begin{figure}[htb]
% 	\begin{subfigure}[t]{0.5\textwidth}
% 		\centering
%     \resizebox{1.12\hsize}{!}{\includegraphics{fig/exh_and_boundweights(2).PNG}}%
% 		\caption{} \label{fig:exh_nexh_and_boundweights_a}
% 		\end{subfigure}
% 	\begin{subfigure}[t]{0.5\textwidth}
% 		\centering
%    \resizebox{0.77\hsize}{!}{\includegraphics{fig/nexh_and_boundweights.PNG}}%
% 		\caption{} \label{fig:exh_nexh_and_boundweights_b}
% 	\end{subfigure}
% \caption{Concept of exhaustive, see \textbf{(a)}, and non-exhaustive, see \textbf{(b)}, calculation of explanatory variables including boundary adjustment at the support level. Auxiliary data are in both cases available over the entire inventory area marked by the large rectangle. A set $\pmb{Z}$ of $p$ explanatory variables $x_{1},...,x_{p}$ is calculated within the small squares at each sample location $x$ (points) that falls into the forest area (green underlying polygon). w: weights used for calculating the weighted mean of each explanatory variable. Dotted circles indicate terrestrial sample plots.}
% \label{fig:exh_nexh_bw}
% \end{figure}


% At this point, an inquisitive reader should notice that the calculation of $\bar{\pmb{Z}}$ and $\hat{\bar{\pmb{Z}}}$ has not been explicitly defined yet.  
% 
% We 
% 
% For the non-exhaustive context, assuming there is no boundary adjustment necessary, we can simply use $\hat{\bar{\pmb{Z}}}=\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$ and then the mathematical theory is perfectly aligned with its implementation in the \pkg{forestinventory} package.  However, in the exhaustive context under the infinite population approach, $n_1=\infty$ technically, which means that mathematically $\bar{\pmb{Z}}=\frac{1}{\lambda{(F)}}\int_{x\in{F}}\pmb{Z}(x)$. This integral is practically impossible to calculate \textit{exactly} because $\pmb{Z}(x)$ would have to be calculated for an infinite number of $x$'s in $s_1$. What we want to do to get around this is to find a way to exhaustively use all available \textbf{auxiliary information} in such a way that $n_1$ is so large that we can safely assume that $\bar{\pmb{Z}}$ is known. Practically speaking, since very large $n_1$ translates into very large input datasets which may conflict with the memory limitations in \proglang{R}, the \pkg{forestinventory} package allows the user to calculate vector $\bar{\pmb{Z}}$ on their own and input it directly into the \code{twophase()} and \code{threephase()}. 
% 
% % In the \pkg{forestinventory} package, exhaustively available auxiliary information over the entire inventory area does not necessarily imply exhaustive calculation of the explanatory variables from that auxiliary information.
% 
% The key to calculating your own $\bar{\pmb{Z}}$ is to understand the difference between the explanatory variables contained in vector $\pmb{Z}(x)$ and spatial extent of the auxiliary information around the point $x$ (the so-called support) from which $\pmb{Z}(x)$ is derived. Figure \ref{fig:exh_nexh_bw} illustrates the practical difference between the exhaustive and non-exhaustive set-ups.  In order for all of the wall-to-wall auxiliary information to be exploited, the spatial arrangement of the supports should be tesselated to form a partition over the inventory area as is the case with the dark green polygons (squares tiles in this case) in Figure \ref{fig:exh_nexh_and_boundweights_a}.  As long as the size of the supports are not unreasonably large, $n_1$ is expected to be reasonably large to assume that the difference between $\hat{\bar{\pmb{Z}}}$ and $\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$ is neglible. This is in contrast to the non-exhaustive set-up in Figure \ref{fig:exh_nexh_and_boundweights_b} where the supports leave much of the available auxiliary information unused (represented by the light green tiles). In both cases, the vector of explanatory variables, $\pmb{Z}(x)$, can be calculated for each point $x$ using the support around that point. It should be noted that a perfect tesselation by the supports strongly depends on the distance between the sample locations of $s_1$ as well as the support sizes and might not always be feasible in practice, in which case the assumption of tesselation must be relaxed.\par

\subsubsection{Calculation of Explanatory Variables}

We will now draw our attention to the calculation of the explanatory variables from the auxiliary data for both the non-exhaustive and exhaustive case. In both cases, a set $\pmb{Z}$ of $p$ explanatory variables $x_1,...,x_p$ is calculated within a defined spatial extent (so called \textit{support}) around each sample location $x$. Figure \ref{fig:exh_nexh_and_boundweights_b} shows how the derivation of $\pmb{Z}(x)$ in the \textbf{non-exhaustive} case often looks like in practice, i.e. a regular terrestrial grid $s_2$ is given by a terrestrial inventory, densified to a larger sample $s_1$, and then supports are used to calculate the set of explanatory variables at the sample locations. This is in perfect aggreement with the just presented theory of the non-exhaustive estimator, except for using regular grids rather than randomly placed sample points. In this case, the \pkg{forestinventory} package calculates the emprirical mean of the $\pmb{Z}(x)$ values given in the input data frame, i.e. $\hat{\bar{\pmb{Z}}}=\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$. The \textbf{exhaustive} case however leaves some space of discussion concerning the calculation of $\pmb{Z}(x)$: This is because in the infinite population approach, the calculation $\pmb{Z}(x)$ again only referrs to the sample point $x$ and has no spatial association. Likewise the mean of the local density (i.e. the total), the only way to determine the exact mean of $\pmb{Z}(x)$ would be to calculate the integral $\bar{\pmb{Z}}=\frac{1}{\lambda(F)}\int_{x \in F} \pmb{Z}(x)$, which in practice means to calculate $\pmb{Z}(x)$ at an infinite number ($n_1=\infty$) of sample points $x$. Since this is obviously impossible, we can only try get a reasonable approximation of $\bar{\pmb{Z}}$.\par
One possibility to approximate the exact mean $\bar{\pmb{Z}}$ is shown in Figure \ref{fig:exh_nexh_and_boundweights_a}, which illustrates the implemented solution to the exhaustive use of the auxiliary information described in \citet{mandallaz2013b}. In order for all of the wall-to-wall auxiliary information to be exploited, the spatial arrangement of the supports were tesselated to form a perfect partition over the inventory area as is the case with the dark green polygons (squares tiles in this case). Is has to be noted that this would perfectly enable for calculating the exact mean $\bar{\pmb{Z}}$ in the \textit{finite} population approach, i.e. deriving $\pmb{Z}(x)$ for the entire finite population of supports and consequenlty calculating the exact mean $\bar{\pmb{Z}}$. While this is not equal to the exact mean $\bar{\pmb{Z}}$ in the infinite population approach, $n_1$ is still expected to be reasonably large to assume a good approximation of $\bar{\pmb{Z}}$ as long as the size of the supports are not unreasonably large. Whereas this implementation provides a convenient flexibility to switch between extimators of both the finite and infinite approach using the \textit{same} data set, it can also impose practical drawbacks. One is that a perfect tesselation by the supports strongly depends on the distance between the sample locations of $s_1$ in combination with the spatial extent of the support. Since in practice the support size will rather be chosen in order to achieve a best possible explanatory power of the regression model, thus minimizing the residual variation, a perfect tesselation might not always be feasible. In the infinite population frame however, the supports are also allowed to overlap if this seems necessary to acquire a sufficiently large sample $n_1$ of $\pmb{Z}(x)$ to get a reasonable approximation of $\bar{\pmb{Z}}$. This is again because $\pmb{Z}(x)$ only referrs to the sample point and has no spatial association. With this respect, the infinite population approach can thus provide more flexibility than the finite approach. One could even consider the use of different support sizes for an explanatory variable as long as they are consistently used at all sample locations. % or: as long as they are not changed between sample locations (change of support)

Note that there are few cases where the exact mean can also be precisely calculated in the infinite population framework. The first case is when the explanatory variables are provided by polygon layers (e.g. map of development stages). In this case, one can calculate the exact mean as the area-weighted average of each categorical variable. The second case is when the exact mean can be calculated in one step, e.g. taking the mean of all height pixels of a raster canopy height model will equal the mean calculated by the use of an infinite number of supports \citep{mandallaz2013b}.


\begin{figure}[htb]
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
    \resizebox{1.12\hsize}{!}{\includegraphics{fig/exh_and_boundweights(2).PNG}}%
		\caption{} \label{fig:exh_nexh_and_boundweights_a}
		\end{subfigure}
	\begin{subfigure}[t]{0.5\textwidth}
		\centering
   \resizebox{0.77\hsize}{!}{\includegraphics{fig/nexh_and_boundweights.PNG}}%
		\caption{} \label{fig:exh_nexh_and_boundweights_b}
	\end{subfigure}
\caption{Concept of exhaustive, see \textbf{(a)}, and non-exhaustive, see \textbf{(b)}, calculation of explanatory variables including boundary adjustment at the support level. Auxiliary data are in both cases available over the entire inventory area marked by the large rectangle. A set $\pmb{Z}$ of $p$ explanatory variables $x_{1},...,x_{p}$ is calculated within the small squares at each sample location $x$ (points) that falls into the forest area (green underlying polygon). w: weights used for calculating the weighted mean of each explanatory variable. Dotted circles indicate terrestrial sample plots.}
\label{fig:exh_nexh_bw}
\end{figure}



%% NOTES:
% We will now quickly draw our attention to the calculation of the explanatory variables from the auxiliary data for both the non-exhaustive and exhaustive case. In both cases, a set $\pmb{Z}$ of $p$ explanatory variables $x_1,...,x_p$ is calculated within a defined spatial extent (so called \textit{support}) around each sample location $x$. Figure 4(b) shows how the derivation of Z(x) in the non-exhaustive case often looks like in practice, i.e. a regular terrestrial grid $s_2$ is given by a terrestrial inventory, densified to a larger sample $s_1$, and then supports are used to calculate the set of explanatory variables at the sample location x. This is in perfect aggreement with the just presented theory of the non-exhaustive estimators, except for using regular grids rather than randomly placed sample points, i.e. the forestinventory package will calculate the emprirical mean of the Z(x) values given in the input dataframe, i.e. $\hat{\bar{\pmb{Z}}}=\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$.
% The exhaustive case however leaves some space of discussion concerning the calculation of $Z(x)$:
%
%
% - first, mention what Z(x) means in practice, and how we calculate a variable stored in Z(x), i.e. use supports (maybe mention the choice of support size,
%   and that it can have an influence on model accuracy). 
% - Figure 4(b) shows how the derivation of the Z(x) in the non-exhaustive case often looks like in practice, i.e. a regular grid is given by a 
%   terrestrial inventory, densified to an s1-sample, and then supports are used to calculate the set of explanatory variables at sample location x.
%   This is in perfect aggreement with the just presented theory of the non-exhaustive estimators (except for using regular grids rather than randomly
%   placed sample points) and the forestinventory package will just use the emprirical mean of the Z(x) values given in the input dataframe, i.e. 
%   $\hat{\bar{\pmb{Z}}}=\frac{1}{n_{1}}\sum_{x\in{s_{1}}}\pmb{Z}(x)$.
% - The exhaustive case however leaves some space for the implementation:
%   This is because likewise the local density $Y(x)$ gathered in the second phase $s_2$, also the calculation of Z(x) only referrs to 
%   the sample point x, and has no spatial association (see section 2.1). Likewise the mean of the local density, the only way to determine the exact mean 
%   of $\bar{Z}$ would be to calculate the integral, i.e. in practice calculate Z(x) at an infinite number ($n_1=\infty$) of sample points $x$. In practice,
%   this is obviously impossible.
% - Figure 4(a) shows the implemented solution of an "exhaustive use" of the auxiliary information as described in Mandallaz, Breschan, Hill (2013). Is has to be noted
%   that this would perfectly enable for calculating the exact mean $\bar{Z}$ in the \textit{finite} population approach (i.e. we calculate Z(x) for the entire 
%   population of supports, and can thus calculate the exact mean $\bar{Z}$). However, this is not equal to the exact mean $\bar{Z}$ in the infinite population
%   approach for the mentioned reasons.
% - What we can do in practice is to get a reasonable approximation of $\bar{Z}$. 
%
%
% - mention that in the strict inf.pop.appr. frame, the supports are also allowed to overlap if this seems necessary to acquire a sufficiently large sample
%   n1 of Z(x) to get a reasonable approximation of $\bar{Z}$. This is possible because likewise the local density $Y(x)$ gathered in the second phase $s_2$,
%   also the calculation of Z(x) only referrs to the sample point, and has no spatial association.
%
% - mention that the support size thereby does not have to match the plotsize of the terrestrial inventory. In the strict inf.pop.appr. frame, the supports are also allowed to overlap if this seems necessary to acquire a sufficiently large sample n1 of Z(x) to get a reasonable approximation of $\bar{Z}$. This is possible because likewise the local density $Y(x)$ gathered in the second phase $s_2$, also the calculation of Z(x) only referrs to the sample point, and has no spatial association. In fact, the support size will in practice be chosen in order to achieve a best possible explanatory power of the regression model, thus minimzing the residual variation. mention that a change of support size between sample points for a given expl. variable is not allowed (because.... Daniel ?)
%
%


%% ------------ in progress -------------------- %%



An extension to the so-far published estimators by Mandallaz is the consideration of a \textit{\textbf{boundary adjustment}} according to forest/non-forest decision on the support level, which can be optionally applied in the \code{twophase()} and \code{threephase()} function of the package. In forest inventories, the sample is often restricted to those sample locations located within the forest area. In case a consistent forest definition can be applied to both the $s_2$ and $s_1$ sample (e.g. by a ploygon forest mask layer), it might be desired to restrict the calculation of the explanatory variables to the forest area within the given support (Figure \ref{fig:exh_nexh_bw}). This method was suggested in \citet{mandallaz2013b} and led to an improvement of the regression model accuracy. To insure an unbiased calculation of either $\hat{\bar{\pmb{Z}}}(x)$ or $\bar{\pmb{Z}}(x)$, the respective means have then to be calculated as the \textit{weighted} mean according to \ref{eq:wmeanZ}. The weight $w(x)$ is thereby equal to the percentage of forested area within the support of sample location $x$.

\begin{equation}\label{eq:wmeanZ}
  \hat{\bar{\pmb{Z}}}=\frac{\sum_{x\in{s_1}}w(x)\pmb{Z}(x)}{\sum_{x\in{s_1}}w(x)}
\end{equation}


% --------------------------- %
\subsubsection{Application}

To demonstrate the use of the global two-phase estimators, we will use the \code{grisons} data set that comes with installing the package from the CRAN repository. The dataset contains data from a two-phase forest inventory under simple (i.e. non-cluster) sampling design conducted in 2007 that was used in \citet{mandallaz2013b} as a case study. The $s_1$ same is comprised of 306 sample locations arranged on a systematic grid containing auxiliary information in the form of LiDAR canopy height metrics (\code{mean}, \code{stddev}, \code{max}, \code{q75}). Auxiliary information for all 306 plots is provided in the form of LiDAR canopy height metrics (\code{mean}, \code{stddev}, \code{max}, \code{q75}). For a systematic subsample of 67 ($s_2$ sample), terrestrial information of the timber volume per hectare (\code{tvol}) on the sample plot level is provided from a terrestrial survey. We can load \pkg{forestinventory} and examine the \code{grisons} data set in the \proglang{R} environment as follows:

<<echo=FALSE>>=
options(width=10000) 
@

\begin{small}
<<eval=FALSE>>=
library(forestinventory)
data("grisons", package = "forestinventory")
head(grisons)
@
\end{small}

\begin{small}
<<eval=TRUE, echo=FALSE>>=
library(forestinventory)
set.seed(666);head(grisons[sample(1:306,10),c(1,3,4,5,6,7,8,9)], 10)
@
\end{small}

Estimates can be made using the \code{onephase()}, \code{twophase()} or \code{threephase()} functions. The data frame inputted to these functions must have the structure where each row corresponds to a unique sample location and the columns specify the attributes associated to that respective sample location. Attributes that are missing, e.g. because they are associated with sample locations that were not selected in the subsample for the subsequent phase, should be designated as \code{NA} and the phase membership is encoded as numeric.

For global two-phase estimation, we have to specify

\begin{itemize}
  \itemsep0em
  \item the regression model (\code{formula}) as specified in the \code{lm}()-function \citep{R}
  \item the inputted \code{data.frame} containing the inventory information (\code{data})
  \item the \code{list}-object \code{phase_id} containing: the \code{phase.col} argument identifying the name of the column specifying membership to $s_1$ or $s_2$, and the \code{terrgrid.id} argument specifying which numeric value indicates $s_2$ membership in that column
  \item the name of the column containing the weights $w(x)$ of the boundary adjustments (optional)
\end{itemize}

The \textbf{non-exhaustive estimator with boundary weight adjustment} can thus be applied as follows:

\begin{small}
<<>>=
reg2p_nex <- twophase(formula=tvol ~ mean + stddev + max + q75,
                   data=grisons,
                   phase_id=list(phase.col = "phase_id_2p", terrgrid.id = 2),
                   boundary_weights = "boundary_weights")
@
\end{small}

The \code{twophase()} function creates an \code{S3} object of class \code{"twophase"} with subclass \code{"global"}. A readable \textbf{summary of the estimation results} can be obtained by passing this object to the \code{summary()} function, which automatically interprets what type of estimator was used and returns pertinent information such as the regression model formula, the point estimate (\code{estimate}), the design-based and external variance (\code{g_variance} and \code{ext_variance}) as well as the sample sizes and the model R$^2$:

\begin{small}
<<>>=
summary(reg2p_nex)
@
\end{small}

For practical use, one should normally always prefer the design-based variance over the external variance. This is because when we use internal models, the regression coefficients actually dependent on our sample. In contrast to the external variance, the design-based variance accounts for this sampling variability which results in more reliable point and variance estimates and also enjoys better statistical properties (g-weights). The external and design-based variances are asymptotically equivalent but the external variance is really only included here in case the user wants to compare to another estimator where no design-based variance exists.

The \textbf{exhaustive estimator} can be applied by additionally passing a vector containing the \textit{exact} means of the explanatory variables, i.e. $\bar{\pmb{Z}}$, to the optional argument \code{exhaustive}. This vector must be calculated beforehand in such a way that any desired boundary adjustment has already been applied. Note that the vector input to \code{exhaustive} must be in the same order that the \code{lm()}-function processes a \code{formula} object including the intercept term whose exact mean will always be 1. This step is particularly recommended if categorical variables are present because the \code{lm()}-function, which is internally used to set up the design-matrix, automatically creates dummy variables with one of the categories used as a reference. The correct order for our \code{grisons} example can easily be extracted using:

\begin{small}
<<eval=FALSE>>=
colnames(lm(formula = tvol ~ mean + stddev + max + q75, data = grisons, x = TRUE)$x)
@
\end{small}

The \textbf{exhaustive estimator} can be applied after defining the vector of exact means $\bar{\pmb{Z}}$ taken from \citet{mandallaz2013b} and denoted as \code{true.means.Z}:

\begin{small}
<<>>=
true.means.Z <- c(1, 11.39, 8.84, 32.68, 18.03)

reg2p_ex <- twophase(formula = tvol ~ mean + stddev + max + q75,
                    data = grisons,
                    phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                    exhaustive = true.means.Z)
@
\end{small}

An alternative way to look at the estimation results without using the \code{summary()} is to query \code{reg2p_ex} directly:
\begin{small}
<<>>=
reg2p_ex$estimation
@
\end{small}
Note that both variances of the exhaustive estimation are smaller than those of the non-exhaustive estimation. This is essentially because we eliminated one component of uncertainty by substituting the estimated means of the explanatory variables $\hat{\bar{\pmb{Z}}}$ by their exact means $\bar{\pmb{Z}}$.


%--------------------------------------------------------------------------------------------------%
% ################################################################################################ %
%--------------------------------------------------------------------------------------------------%

\subsection{Small Area Estimators}
\label{sec:twophase_sae}

\subsubsection{Mathematical Background}

The package \pkg{forestinventory} provides 3 small area estimators. A general principle of all small area estimators of the package is to restrict the information to the small area $G$ whenever possible. An exception however is the retrieval of the regression coefficient vector $\hat{\pmb{\beta}}$ and its variance-covariance matrix $\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}$, which are always calculated using the entire sample $s_2$ according to equation \ref{normequ_simple} and \ref{eq:estvarmatrix}.\par

%----------------------------------------------- %
% \textbf{Small and Pseudo Small Area Estimator}\par

We first introduce the \textbf{small area estimator} (\textit{small}), which uses exhaustively computed explanatory variables, and its non-exhaustive version, the \textbf{\textit{pseudo}} \textbf{small area estimator} (\textit{psmall}). Their point estimates are given in equation \ref{eq:pointest_2p_small} and \ref{eq:pepsmall}. As obvious, the globally derived regression coefficients are first applied to the exhaustively or non-exhaustively calculated means of the explanatory variables ($\bar{\pmb{Z}}_G$, $\hat{\bar{\pmb{Z}}}_G$) available at the sample location $s_{1,G}$ in the small area. A potential bias of the regression model predictions in the small area $G$ is then corrected by adding the mean of the model residuals in $G$ to the first term. This is called the \textit{bias} or \textit{residual correction} term.

\begin{subequations}\label{eq:pest_2p_small_psmall}
\begin{align}
  \hat{Y}_{G,small,2p} & =\bar{\pmb{Z}}_G^t\hat{\pmb{\beta}}_{s_2} + \frac{1}{n_{2,G}}\hat{R}(x)  \label{eq:pointest_2p_small} \\
  \hat{Y}_{G,psmall,2p} & =\hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\beta}}_{s_2} + \frac{1}{n_{2,G}}\hat{R}(x) \label{eq:pepsmall}
\end{align}
\end{subequations}

The package again provides the \textit{design-based} (equation \ref{eq:var_2p_reg_small}, \ref{eq:var_2p_reg_psmall}) and \textit{external variance} (equation \ref{eq:varext_2p_reg_small}, \ref{eq:varext_2p_reg_psmall}) for the \textit{small} and the \textit{psmall} estimator. Again note that all components are restricted to those available at the sample locations $s_{1,G}$ and $s_{2,G}$ in the small area, with exception of the regression coefficient components $\hat{\pmb{\beta}}_{s_2}$ and $\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}$.

\begin{subequations}\label{eq:var_2p_small_psmall}
\begin{align}
  \hat{\var}(\hat{Y}_{G,small,2p}) & := \bar{\pmb{Z}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G
    + \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2  \label{eq:var_2p_reg_small} \\
  \hat{\var}(\hat{Y}_{G,psmall,2p}) & := \hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_G
  + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_G}\hat{\pmb{\beta}}_{s_2}
  + \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2  \label{eq:var_2p_reg_psmall}
\end{align}
\end{subequations}

\begin{subequations}\label{eq:varext_2p_small_psmall}
\begin{align}
  \hat{\var}_{ext}(\hat{Y}_{G,small,2p}) & := \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2  \label{eq:varext_2p_reg_small} \\
  \hat{\var}_{ext}(\hat{Y}_{G,psmall,2p}) & := \frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(Y(x)-\bar{Y}_{2,G}\Big)^2 \nonumber \\
 &+ \Big(1-\frac{n_{2,G}}{n_{1,G}}\Big)\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x \in s_{2,G}} \Big(\hat{R}(x)-\bar{\hat{R}}_{2,G}\Big)^2 \label{eq:varext_2p_reg_psmall}
\end{align}
\end{subequations}

The the variance-covariance matrix of the auxiliary vector $\hat{\bar{\pmb{Z}}}_G$ is estimated by
\begin{equation}\label{estvarcovaux_G}
\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_{G}}=
\frac{1}{n_{G}(n_{G}-1)}\sum_{x\in{s_{1,G}}}
(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{G})(\pmb{Z}(x)-\hat{\bar{\pmb{Z}}}_{G})^t
\end{equation}

If boundary adjustment is applied, the simple mean of the explanatory variable over the small area $\hat{\bar{\pmb{Z}}}_G=\frac{1}{n_{1,G}}\sum_{x \in s_{1,G}}\pmb{Z}(x)$ is replaced by its weighted version $\hat{\bar{\pmb{Z}}}_G=\frac{\sum_{x\in{s_{1,G}}}w(x)\pmb{Z}(x)}{\sum_{x\in{s_{1,G}}}w(x)}$, and likewise for exhaustively used auxiliary information.



%------------------------------------------------------------- %
% \textbf{Synthetic and Pseudo Synthetic Estimator}\par

The \textbf{synthetic estimator} (\textit{synth}) and \textbf{pseudo synthetic estimator} \textit{psynth} are applied if no terrestrial samples are available within the small area $G$ (i.e. $n_{2,G}=0$). In this case, the point estimate (equation \ref{eq:pointest_2p_reg_synth} and \ref{eq:pointest_2p_reg_psynth}) is based \textit{only} on the predictions generated by applying the globally derived regression model to the auxiliary vector $\bar{\pmb{Z}}_G$ or $\hat{\bar{\pmb{Z}}}_G$. However, the bias correction applied in the small and pseudo small area estimator (equation \ref{eq:pointest_2p_small} and \ref{eq:pepsmall}) is no longer possible. Thus, the (pseudo) synthetic estimator is unbiased \textbf{only} if the \textit{zero mean property} of the residuals $\int_F R(x)dx=0$ also holds for the residuals in the small area $G$, i.e. $\int_G R(x)dx=0$. If this assumption is not fullfilled, the synthetic point estimate $\hat{Y}_{G,synth,2p}$ and the pseudo synthetic point estimate $\hat{Y}_{G,psynth,2p}$ will have a design-based asymptotic bias equal to $-\frac{1}{\lambda(G)}\int_G R(x)$. Also note that the residual variation can no longer be considered in the design-based variance (equation \ref{eq:var_2p_reg_synth} and \ref{eq:var_2p_reg_psynth}). Therefore, the synthetic estimators will usually have a smaller variance than estimators incorporating the regression model uncertainties, but at the cost of a potential bias. Due to the absence of available residuals in $G$, there is also no external variance form for the synthetic and pseudo synthetic estimator.

\begin{subequations}\label{eq:pest_2p_synth_psynth}
\begin{align}
  \hat{Y}_{G,synth,2p} & =\bar{\pmb{Z}}_G^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_synth} \\
  \hat{Y}_{G,psynth,2p} & =\hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\beta}}_{s_2} \label{eq:pointest_2p_reg_psynth}
\end{align}
\end{subequations}

\begin{subequations}\label{eq:var_2p_synth_psynth}
\begin{align}
  \hat{\var}(\hat{Y}_{G,synth,2p}) & := \bar{\pmb{Z}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\bar{\pmb{Z}}_G \label{eq:var_2p_reg_synth} \\
  \hat{\var}(\hat{Y}_{G,psynth,2p}) & := \hat{\bar{\pmb{Z}}}_G^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\beta}}_{s_2}}\hat{\bar{\pmb{Z}}}_G
  + \hat{\pmb{\beta}}_{s_2}^t\hat{\Sigma}_{\hat{\bar{\pmb{Z}}}_G}\hat{\pmb{\beta}}_{s_2}  \label{eq:var_2p_reg_psynth}
\end{align}
\end{subequations}


%------------------------------------------------------------- %
% \textbf{Extended Synthetic and Extended Pseudo Synthetic Estimator}\par

In the \textit{small} and \textit{psmall} estimator, the crucial zero mean residual property for the small area $G$ is forced by the bias adjustment terms (equation \ref{eq:pointest_2p_small}, \ref{eq:pepsmall}).  However, the equations would considerably simplify if we could mathematically insure that the prediction model residuals over the entire area $F$ and the small area $G$ are by definition both zero at the same time, i.e. $\int_F R(x)dx=\int_G R(x)dx=0$. This idea is realized in the \textbf{\textit{extended}} \textbf{synthetic} and \textbf{\textit{extended}} \textbf{pseudo synthetic estimator} (\textit{extsynth} and \textit{extpsynth}) by extending the vector of auxiliary information $\pmb{Z}(x)$ by a binary categorical indicator variable $I_G(x)$ which takes the value 1 if the sample location $x$ lies inside the small area $G$ and is otherwise set to 0. The \textit{extended} auxiliary vector thus reads as $\pmb{\mathbb{Z}}^t(x)=(\pmb{Z}^t(x),I_G(x))$ and leads to a classical ANOVA regression model. Note that the intercept of the regression model is thus adjusted for all data points belonging to small area $G$ which insures that the mean of the residuals $\frac{1}{n_{2,G}}\sum_{x\in{s_{2,G}}}R(x)$ in $G$ are always zero by construction.\par


Under the extended auxiliary vector, the regression coefficient is calculated as

\begin{equation}\label{ext_normequ_simple}
  \hat{\pmb{\theta}}_{s_2}=\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{\mathbb{Z}}(x)\pmb{\mathbb{Z}}^t(x) \Big)^{-1} \Big(\frac{1}{n_2}\sum_{x\in{s_2}}Y(x)\pmb{\mathbb{Z}}(x)\Big)
\end{equation}

with its variance-covariance matrix

\begin{equation}\label{eq:ext_estvarmatrix}
  \hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}:=\Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{\mathbb{Z}}(x)\pmb{\mathbb{Z}}^t(x) \Big)^{-1}
  \Big(\frac{1}{n_2^2}\sum_{x\in{s_2}}\hat{R}^2(x)\pmb{\mathbb{Z}}(x)\pmb{Z}(x)^t\Big)
  \Big(\frac{1}{n_2}\sum_{x\in{s_2}}\pmb{\mathbb{Z}}(x)\pmb{\mathbb{Z}}^t(x) \Big)^{-1}
\end{equation}

The point estimate is defined as

\begin{subequations}\label{eq:pest_2p_extsynth_extpsynth}
\begin{align}
\hat{Y}_{G,extsynth,2p}= \bar{\pmb{\mathbb{Z}}}^t_{G}\hat{\pmb{\theta}}_{s_2} \label{eq:pointest_2p_extsynth} \\
\hat{Y}_{G,extpsynth,2p}=\hat{\bar{\pmb{\mathbb{Z}}}}_{G}^t\hat{\pmb{\theta}}_{s_2} \label{eq:pointest_2p_extsynth}
\end{align}
\end{subequations}


and the design-based variances are derived as

\begin{subequations}\label{eq:pest_2p_extsynth_extpsynth}
\begin{align}
\hat{\var}(\hat{Y}_{G,extsynth,2p})= \bar{\pmb{\mathbb{Z}}}^t_{G}\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}\bar{\pmb{\mathbb{Z}}}_{G} \label{eq:var_2p_extsynth} \\
\hat{\var}(\hat{Y}_{G,extpsynth,2p})=
\hat{\bar{\pmb{\mathbb{Z}}}}_{G}^t\hat{\pmb{\Sigma}}_{\hat{\pmb{\theta}}_{s_2}}
\hat{\bar{\pmb{\mathbb{Z}}}}_{G}
+ \hat{\pmb{\theta}}^t_{s_2}\hat{\Sigma}_{\hat{\bar{\pmb{\mathbb{Z}}}}_{1,G}}\hat{\pmb{\theta}}_{s_2} \label{eq:var_2p_extpsynth}
\end{align}
\end{subequations}

with the variance-covariance matrix of the auxiliary vector $\hat{\bar{\pmb{\mathbb{Z}}}}_G$ estimated by

\begin{equation}\label{ext_estvarcovaux_G}
\hat{\Sigma}_{\hat{\bar{\pmb{\mathbb{Z}}}}_{1,G}}=
\frac{1}{n_{1,G} (n_{1,G}-1)}\sum_{x\in{s_{1,G}}}
(\pmb{\mathbb{Z}}(x)-\hat{\bar{\pmb{\mathbb{Z}}}}_{G})(\pmb{\mathbb{Z}}(x)-
\hat{\bar{\pmb{\mathbb{Z}}}}_{G})^t
\end{equation}

The package also provides the external variance which is calculated as

\begin{subequations}\label{eq:ext_varexternal_2p_extsynth}
\begin{align}
  \hat{\var}_{ext}(\hat{Y}_{G,extsynth,2p}) & = \frac{1}{n_{2,G}} \frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{\mathbb{R}}(x)-\bar{\hat{\mathbb{R}}})^2 \label{eq:ext_varexternal_2p_extsynth} \\
  \hat{\var}_{ext}(\hat{Y}_{G,extpsynth,2p}) & =
 \frac{1}{n_{1,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(Y(x)-\bar{Y}_{2,G})^2 \nonumber \\
 &+ (1-\frac{n_{2,G}}{n_{1,G}})\frac{1}{n_{2,G}}\frac{1}{n_{2,G}-1}\sum_{x\in{s_{2,G}}}(\hat{\mathbb{R}}(x)-\bar{\hat{\mathbb{R}}})^2 \label{eq:ext_varexternal_2p_extpsynth}
\end{align}
\end{subequations}

The estimators are called \textit{synthetic} since no residual term is present such as in the \textit{small} and \textit{psmall} estimator (equation \ref{eq:pointest_2p_small} and \ref{eq:pepsmall}) and thus look similar to the synthetic estimators (equation \ref{eq:pointest_2p_reg_synth}, \ref{eq:pointest_2p_reg_synth}, \ref{eq:var_2p_reg_synth}, \ref{eq:var_2p_reg_psynth}). However, note that by a decomposition of $\hat{\pmb{\theta}}_{s_2}$ it can be shown that the residual correction term is indeed included in the regression coefficient $\hat{\pmb{\theta}}_{s_2}$ \citep{mandallaz2016} and thus the estimator is asymptotically design-unbiased.


%------------------------------------------------------------- %
\subsubsection{Application}

Small area estimations in the package \pkg{forestinventory} can be applied by additionally specifying the optional argument \code{small_area}. The input data set has therefore to be extended by an additional column that specifies for each each row (i.e. observation of a sample location) the respective small area in which it is located. The argument \code{small_area} requires a \code{list}-object that comprises

\begin{itemize}
  \itemsep0em
  \item the name of the column specifiying the small area of each observation (\code{sa.col})
  \item a vector specifying the small area(s) for which estimations are desired (\code{areas})
  \item the argument \code{unbiased} that controls which of the three available estimators is applied
\end{itemize}

In order to apply the \textbf{pseudo small area estimator} (\textit{psmall}) with boundary adjustment, we set \code{unbiased=TRUE} as well as the optional argument \code{psmall=TRUE}:
\begin{small}
<<>>=
psmall_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                      phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                      boundary_weights = "boundary_weights",
                      small_area = list(sa.col = "smallarea", areas = c("A", "B"),
                                        unbiased = TRUE),
                      psmall = TRUE)
summary(psmall_2p)
@
\end{small}

The small area functions all return an \code{S3} object of class \code{"twophase"} with subclass \code{"smallarea"}. In addition to global estimation, the \code{summary()}-output and the \code{estimation} object comprise the estimates and the sample sizes (\code{n1G} and \code{n2G}) for all calculated small areas (column \code{area}). Note that the sample size information is also particularly provided in the \code{samplesizes} object returned by the function call.
\begin{small}
<<>>=
psmall_2p$samplesizes
@
\end{small}

The \textbf{extended pseudo synthetic estimator} (\textit{extpsynth}) can be applied by setting \code{unbiased=TRUE} and leaving the optional argument \code{psmall} to its default value of \code{FALSE}:
\begin{small}
<<>>=
extpsynth_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                         phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                         boundary_weights = "boundary_weights",
                         small_area = list(sa.col = "smallarea", areas = c("A", "B"),
                                           unbiased = TRUE))
extpsynth_2p$estimation
@
\end{small}

Note that the point estimates and the design-based and external variances of both estimators are very similiar. However, the $R^2$s (\code{r.squared}) under the \textit{extpsynth} estimator are changing with the small area, while they are identical under the \textit{psmall} estimator. This is because under the \textit{extpsynth} estimator, the regression model is recalculated for each small area estimation after adding the indicator variable for the respective small area in the globally derived design matrices. In case of the  \textit{psmall} estimator, the regression model stays the same for each small area estimation. Although the results of both estimators should always be close to each other, we recommend applying both estimators and compare the results afterwards in order to reveal unsuspected patterns in the data, particularly in the case of cluster sampling (see section \ref{sec:speccas_and_scen}).\par

Setting the argument \code{unbiased=FALSE} applies the \textbf{pseudo synthetic estimator} to the selected small areas. Note that in the \code{grisons} data set, all small areas possess at least 2 terrestrial observations ($n_{2,G} \geq 2$) and hence design-unbiased estimations are possible. Applying the pseudo synthetic estimator equals treating those small areas as if $n_{2,G}=0$ by ignoring the terrestrial information.
\begin{small}
<<>>=
psynth_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                      phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                      boundary_weights = "boundary_weights",
                      small_area = list(sa.col = "smallarea", areas = c("A", "B"),
                                        unbiased = FALSE))
psynth_2p$estimation
@
\end{small}

In this case, the variances of the pseudo synthetic estimator is almost only half the variances of the \textit{psmall} and \textit{extended psynth} estimator. The reason for this is that the pseudo synthetic estimator neglects the uncertainty of the regression model predictions in terms of incorporating the residual variation (compare equation \ref{eq:var_2p_reg_psynth} to \ref{eq:var_2p_reg_psmall} and \ref{eq:var_2p_extpsynth}). Consequently, relying on the synthetic estimation would in this case lead to a severe overestimation of the estimation precision.\par

The \textbf{exhaustive versions} of the small area estimators (equations \ref{eq:pointest_2p_small},\ref{eq:var_2p_reg_small},\ref{eq:varext_2p_reg_small},\ref{eq:pointest_2p_reg_synth},\ref{eq:var_2p_reg_synth}) can again be applied by specifying the optional argument \code{exhaustive}. Its application requires that we know the exact means of all explanatory variables within the small area(s) of interest. In contrast to the \textit{global} estimators, the exact means have now to be delivered in the form of a \code{data.frame}, where each row corresponds to a small area, and each column specifies the exact mean of the respective explanatory variable. Note that likewise the case of global estimation, the order of the explanatory variables in the data frame has to match the order in which they appear in the design matrix defined by the \code{lm()}-function in \proglang{R}. In order to tell \proglang{R} which row describes which small area, the row names have to match the respective names of the small areas specified in the \code{areas} argument.

For the \code{grisons} data set, the exact means of the explanatory variables for the small areas used in \citet{mandallaz2013b} are thus defined by
\begin{small}
<<eval=FALSE>>=
colnames(lm(formula = tvol ~ mean + stddev + max + q75, data = grisons, x = TRUE)$x)
@
\end{small}
\begin{small}
<<>>=
true.means.Z.G <- data.frame(Intercept = rep(1, 4),
                         mean = c(12.85, 12.21, 9.33, 10.45),
                         stddev = c(9.31, 9.47, 7.90, 8.36),
                         max = c(34.92, 35.36, 28.81, 30.22),
                         q75 = c(19.77, 19.16, 15.40, 16.91))
rownames(true.means.Z.G) <- c("A", "B", "C", "D")
@
\end{small}
\begin{small}
<<>>=
true.means.Z.G
@
\end{small}

For example, the \textbf{extended synthetic estimator} (\textit{extsynth}) can then be applied by
\begin{small}
<<>>=
extsynth_2p <- twophase(formula = tvol ~ mean + stddev + max + q75, data = grisons,
                        phase_id = list(phase.col = "phase_id_2p", terrgrid.id = 2),
                        small_area = list(sa.col ="smallarea", areas = c("A", "B"),
                                          unbiased = TRUE),
                        exhaustive = true.means.Z.G)
extsynth_2p$estimation
@
\end{small}

Comparing this result with the non-exhaustive counterpart \textit{extpsynth}, we see that the estimation uncertainty defined by the variance has again be significantly increased by substituting the estimated by the exact auxiliary means. In the function output, exhaustive phases are indicated by a sample size of infinity (\code{Inf}). Also note that the function extracts the required exact means for small area \code{"A"} and \code{"B"} from the complete set of exact means defined in \code{true.means.Z.G}.

\newpage